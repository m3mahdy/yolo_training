{"cells":[{"cell_type":"markdown","id":"8b26f5d6","metadata":{"id":"8b26f5d6"},"source":["# YOLO Training\n","- Support for YOLOv8, YOLOv9, YOLOv10, YOLO11, YOLO12\n"]},{"cell_type":"markdown","id":"a7e53f76","metadata":{"id":"a7e53f76"},"source":["# 1. Set Directories"]},{"cell_type":"code","execution_count":1,"id":"c698714c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c698714c","executionInfo":{"status":"ok","timestamp":1764196532376,"user_tz":-180,"elapsed":45441,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"4a25f759-c7b7-437f-e6f9-fd88787e89ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Drive\n","âœ“ W&B API key loaded from Colab secrets\n"]}],"source":["# Base directories\n","# Detect environment: Colab or local\n","\n","import os\n","from pathlib import Path\n","\n","\n","IS_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n","\n","USE_WANDB = True  # Set to False to disable W&B logging\n","\n","\n","\n","if IS_COLAB:\n","    #Mount Google Drive if not already mounted\n","    from google.colab import drive\n","    drive.mount('/content/Drive', force_remount=True)\n","    # Running in Google Colab\n","    BASE_DIR = Path('/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo')\n","\n","    # Configure W&B API key\n","    if USE_WANDB:\n","        # In Colab, get API key from secrets\n","        from google.colab import userdata\n","        wandb_api_key = userdata.get('wandb_api_key')\n","        os.environ['WANDB_API_KEY'] = wandb_api_key\n","        print('âœ“ W&B API key loaded from Colab secrets')\n","\n","    DATASET_BASE_DIR = Path('/computer_vision_yolo')\n","\n","else:\n","    # Running locally\n","    BASE_DIR = Path.cwd().parent\n","    if USE_WANDB:\n","        print('âœ“ Running locally - W&B will use existing login or prompt')\n","\n","    DATASET_BASE_DIR = Path.cwd().parent\n"]},{"cell_type":"code","execution_count":4,"id":"c8064a27","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8064a27","executionInfo":{"status":"ok","timestamp":1764196661370,"user_tz":-180,"elapsed":79177,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"b6d5fcd7-7e1c-4142-cb81-f4e14c00a6e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'computer_vision_yolo'...\n","remote: Enumerating objects: 2867, done.\u001b[K\n","remote: Counting objects: 100% (12/12), done.\u001b[K\n","remote: Compressing objects: 100% (10/10), done.\u001b[K\n","remote: Total 2867 (delta 2), reused 6 (delta 2), pack-reused 2855 (from 3)\u001b[K\n","Receiving objects: 100% (2867/2867), 1.87 GiB | 26.29 MiB/s, done.\n","Resolving deltas: 100% (927/927), done.\n","Updating files: 100% (107/107), done.\n"]}],"source":[" ! cd /content/Drive/MyDrive/ksu_yolo_2025 && git clone https://github.com/m3mahdy/computer_vision_yolo"]},{"cell_type":"code","execution_count":5,"id":"16c67ad5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16c67ad5","executionInfo":{"status":"ok","timestamp":1764196684199,"user_tz":-180,"elapsed":8163,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"c57ff472-1f8f-437e-c382-1e1bb6c0825c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":[" ! cd {BASE_DIR} && pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":7,"id":"43f33666","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43f33666","executionInfo":{"status":"ok","timestamp":1764196874919,"user_tz":-180,"elapsed":108890,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"f3bda72c-f419-4b09-8685-9f335f2c7b6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory â€˜/computer_vision_yoloâ€™: File exists\n","\n","======================================================================\n","BDD100K DATASET DOWNLOAD & EXTRACT\n","======================================================================\n","\n","Available datasets:\n","\n","[1] Full Dataset Source Files\n","    Original BDD100K images and labels (requires processing)\n","    Size: ~5.28GB, ~180MB\n","\n","[2] Limited Dataset (YOLO Ready)\n","    Balanced limited dataset - 30-40% coverage (~25K train)\n","    Size: ~3.3GB\n","\n","[3] Tuning Dataset (YOLO Ready)\n","    Tuning dataset - 20% coverage (~14K train)\n","    Size: ~1.58GB\n","\n","[4] Tiny Dataset (YOLO Ready)\n","    Tiny dataset - ~500 train, ~1K total (fast testing)\n","    Size: ~100MB\n","\n","[5] Test Split Only (YOLO Ready)\n","    Test split only for validation (20K images)\n","    Size: ~1.1GB\n","\n","[0] Exit\n","======================================================================\n","\n","Select dataset (0-5): 2\n","\n","======================================================================\n","Processing: Limited Dataset (YOLO Ready)\n","======================================================================\n","\n","ğŸ“¥ Downloading from Google Drive...\n","   File ID: 1psk1Q9YUdV2e_xKzIV29hFfRjx4vGGmG\n","   Destination: bdd100k_yolo_limited.zip\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1psk1Q9YUdV2e_xKzIV29hFfRjx4vGGmG\n","From (redirected): https://drive.google.com/uc?id=1psk1Q9YUdV2e_xKzIV29hFfRjx4vGGmG&confirm=t&uuid=8a6f9ec1-3baa-4461-af7d-e1e2755308f4\n","To: /bdd100k_limited_datasets_zipped/bdd100k_yolo_limited.zip\n","100% 3.56G/3.56G [00:29<00:00, 121MB/s]\n","âœ“ Download complete: 3397.7 MB\n","\n","ğŸ“¦ Extracting bdd100k_yolo_limited.zip...\n","  Extracting: 100% 119922/119922 [00:35<00:00, 3413.95files/s] \n","âœ“ Extracted to: /computer_vision_yolo\n","\n","======================================================================\n","âœ… Limited Dataset (YOLO Ready) ready!\n","======================================================================\n","\n","\n","Download another dataset? (y/n): y\n","\n","======================================================================\n","BDD100K DATASET DOWNLOAD & EXTRACT\n","======================================================================\n","\n","Available datasets:\n","\n","[1] Full Dataset Source Files\n","    Original BDD100K images and labels (requires processing)\n","    Size: ~5.28GB, ~180MB\n","\n","[2] Limited Dataset (YOLO Ready)\n","    Balanced limited dataset - 30-40% coverage (~25K train)\n","    Size: ~3.3GB\n","\n","[3] Tuning Dataset (YOLO Ready)\n","    Tuning dataset - 20% coverage (~14K train)\n","    Size: ~1.58GB\n","\n","[4] Tiny Dataset (YOLO Ready)\n","    Tiny dataset - ~500 train, ~1K total (fast testing)\n","    Size: ~100MB\n","\n","[5] Test Split Only (YOLO Ready)\n","    Test split only for validation (20K images)\n","    Size: ~1.1GB\n","\n","[0] Exit\n","======================================================================\n","\n","Select dataset (0-5): 0\n","\n","Exiting...\n"]}],"source":["# download limited dataset\n","!mkdir {DATASET_BASE_DIR}\n","!cd {BASE_DIR}/dataset && cp 8_download_extract_other_datasets.py {DATASET_BASE_DIR} && cd {DATASET_BASE_DIR} && python 8_download_extract_other_datasets.py\n"]},{"cell_type":"markdown","id":"109394c4","metadata":{"id":"109394c4"},"source":["## 2. Import Required Libraries"]},{"cell_type":"code","execution_count":8,"id":"fc6f98f1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc6f98f1","executionInfo":{"status":"ok","timestamp":1764196896199,"user_tz":-180,"elapsed":7371,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"73b6ce01-1d91-4d94-c400-5005bfee2140"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","âœ“ Libraries imported successfully\n","âœ“ Device: cuda\n","  GPU: NVIDIA A100-SXM4-40GB\n","  CUDA Version: 12.6\n","  Available Memory: 42.47 GB\n"]}],"source":["# Install required libraries (uncomment if running in Colab)\n","# !pip install -q ultralytics wandb pyyaml\n","\n","import os\n","import sys\n","import gc\n","import yaml\n","import json\n","import torch\n","import shutil\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","from datetime import datetime\n","from tqdm import tqdm\n","import pickle\n","import platform\n","import psutil\n","\n","import wandb\n","\n","# YOLO imports\n","from ultralytics import YOLO\n","\n","# ReportLab imports for PDF generation\n","from reportlab.lib.pagesizes import A4\n","from reportlab.lib import colors as rl_colors\n","from reportlab.lib.units import inch\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.enums import TA_CENTER, TA_LEFT\n","from PIL import Image as PILImage\n","\n","warnings.filterwarnings('ignore')\n","\n","# Configure matplotlib for notebook display\n","%matplotlib inline\n","sns.set_style('whitegrid')\n","plt.rcParams['figure.figsize'] = (15, 10)\n","\n","# Check GPU availability\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'âœ“ Libraries imported successfully')\n","print(f'âœ“ Device: {device}')\n","if device == 'cuda':\n","    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n","    print(f'  CUDA Version: {torch.version.cuda}')\n","    print(f'  Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n"]},{"cell_type":"markdown","id":"2659c792","metadata":{"id":"2659c792"},"source":["## 3. Configuration"]},{"cell_type":"code","execution_count":14,"id":"d163f0be","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d163f0be","executionInfo":{"status":"ok","timestamp":1764198851118,"user_tz":-180,"elapsed":4278,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"651990be-f4c4-47fd-98f3-49b956e601f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Drive\n","âœ“ W&B API key loaded from Colab secrets\n","\n","âš™ï¸  CONFIGURATION MODE: Using Default YOLO Configuration\n","   No hyperparameter tuning will be applied\n","\n","ğŸ†• NEW TRAINING MODE: Creating new run \"yolov8m_train_20251126_231411\"\n","================================================================================\n","CONFIGURATION SUMMARY\n","================================================================================\n","Environment: Google Colab\n","Base Directory: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo\n","Model: yolov8m\n","Dataset: bdd100k_yolo_limited\n","Data YAML: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tmp/yolov8m/data.yaml\n","  Dataset path in YAML: /computer_vision_yolo/bdd100k_yolo_limited\n","Classes: 10\n","Class Names: {0: 'person', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motor', 7: 'bike', 8: 'traffic light', 9: 'traffic sign'}\n","Device: cuda\n","Epochs Final Training: 100\n","Batch Size: 96\n","Image Size: 640\n","Configuration Mode: Default (No Tuning)\n","Training Directory: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\n","W&B Logging: Enabled\n","  Training Project: yolo-bdd100k_yolo_limited-training\n","================================================================================\n"]}],"source":["# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Base directories\n","# Detect environment: Colab or local\n","\n","IS_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n","\n","USE_WANDB = True  # Set to False to disable W&B logging\n","\n","if IS_COLAB:\n","    #Mount Google Drive if not already mounted\n","    from google.colab import drive\n","    drive.mount('/content/Drive', force_remount=True)\n","    # Running in Google Colab\n","    BASE_DIR = Path('/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo')\n","\n","    # Configure W&B API key\n","    if USE_WANDB:\n","        # In Colab, get API key from secrets\n","        from google.colab import userdata\n","        wandb_api_key = userdata.get('wandb_api_key')\n","        os.environ['WANDB_API_KEY'] = wandb_api_key\n","        print('âœ“ W&B API key loaded from Colab secrets')\n","\n","else:\n","    # Running locally\n","    BASE_DIR = Path.cwd().parent\n","    if USE_WANDB:\n","        print('âœ“ Running locally - W&B will use existing login or prompt')\n","class DatasetSplit:\n","    \"\"\"Constants for dataset split names\"\"\"\n","    TRAIN = \"train\"\n","    VAL = \"val\"\n","    TEST = \"test\"\n","\n","class ModelConfig:\n","    \"\"\"Default model training configuration constants\"\"\"\n","    # Image processing\n","    DEFAULT_IMAGE_SIZE = 640  # Standard YOLO input size\n","\n","    # Training workers\n","    DEFAULT_WORKERS = 8  # Number of data loading workers\n","\n","    # Early stopping and checkpointing\n","    DEFAULT_PATIENCE = 10  # Epochs to wait before early stopping\n","    DEFAULT_SAVE_PERIOD = 1  # Save checkpoint every N epochs\n","\n","    # Augmentation timing\n","    CLOSE_MOSAIC_EPOCHS = 10  # Disable mosaic augmentation in last N epochs\n","\n","    # Data loading and caching\n","    DEFAULT_CACHE = False  # Cache images for faster training (use True for small datasets)\n","    DEFAULT_VAL = True  # Run validation during training\n","\n","    # Warmup configuration\n","    # MIN_WARMUP_EPOCHS = 0\n","    # MAX_WARMUP_EPOCHS = 3\n","    # MIN_WARMUP_MOMENTUM = 0.5\n","    # MAX_WARMUP_MOMENTUM = 0.95\n","    # MIN_WARMUP_BIAS_LR = 0.0\n","\n","    # MAX_WARMUP_BIAS_LR = 0.1\n","\n","\n","\n","\n","# Model Selection - Choose one of the following:\n","MODEL_NAME = \"yolov8m\"\n","\n","#yolov10n is for testing purpose only\n","#Mahdy will work yolov8m\n","\n","\n","# Selected models, to choose from, based on the performance and size:\n","# YOLOv8:  'yolov8s', 'yolov8m'\n","\n","# YOLOv10: 'yolov10s', 'yolov10m'\n","\n","# YOLO12: 'yolo12s'\n","\n","# Directory structure\n","MODELS_DIR = BASE_DIR / 'models' / MODEL_NAME\n","TMP_DIR = BASE_DIR / 'tmp' / MODEL_NAME\n","\n","# Dataset Selection\n","# Option 1: Full dataset (~100k images) - for final optimization: \"bdd100k_yolo\"\n","# Option 2: Limited dataset (representative samples) - for quick tuning: \"bdd100k_yolo_limited\"\n","dataset_name = 'bdd100k_yolo_limited'\n","\n","\n","YOLO_DATASET_ROOT = DATASET_BASE_DIR / dataset_name\n","\n","# data.yaml path\n","DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n","\n","# Verify dataset exists\n","if not DATA_YAML_PATH.exists():\n","    raise FileNotFoundError(\n","        f\"Dataset not found: {DATA_YAML_PATH}\\n\"\n","        f\"Please prepare the dataset first using process_bdd100k_to_yolo_dataset.py\"\n","    )\n","\n","# Update data.yaml path field for Colab compatibility\n","with open(DATA_YAML_PATH, 'r') as yaml_file:\n","    data_config = yaml.safe_load(yaml_file)\n","\n","# Validate required keys in data.yaml\n","required_yaml_keys = ['nc', 'names', 'path']\n","missing_keys = [key for key in required_yaml_keys if key not in data_config]\n","if missing_keys:\n","    raise ValueError(f\"Missing required keys in data.yaml: {missing_keys}\")\n","\n","# Update the 'path' field to use BASE_DIR\n","data_config['path'] = str(YOLO_DATASET_ROOT)\n","\n","# Create a temporary data.yaml with corrected paths\n","temp_data_yaml = TMP_DIR / 'data.yaml'\n","TMP_DIR.mkdir(parents=True, exist_ok=True)\n","with open(temp_data_yaml, 'w') as yaml_output_file:\n","    yaml.dump(data_config, yaml_output_file, default_flow_style=False, sort_keys=False)\n","\n","# Use the temporary data.yaml for training\n","DATA_YAML_PATH = temp_data_yaml\n","\n","# Training Configuration\n","EPOCHS_FINAL_TRAINING = 100  # Training epochs for final model = 150\n","BATCH_SIZE = 96  # Batch size for training\n","# for T4 GPU:\n","# 64 for 10n, 1 epoch 30 min\n","# 32 for 8m, 1 epoch 45 min\n","\n","# for A100 GPU:\n","# 64 for 10m 1 epoch 11 min, 5 epochs completed in 0.797 hours.\n","# 96 for 8m , 1 epoch 10 min, 5 epochs completed in 0.866 hours.\n","\n","IMAGE_SIZE = 640  # Input image size\n","\n","# Weights & Biases (optional)\n","USE_WANDB = True  # Set to True to enable W&B logging\n","WANDB_PROJECT_TRAINING = f\"yolo-{YOLO_DATASET_ROOT.name}-training\"\n","\n","# ============================================================================\n","# CONFIGURATION MODE: DEFAULT vs TUNED HYPERPARAMETERS\n","# ============================================================================\n","# Set to True to use default YOLO configuration (no hyperparameter tuning)\n","# Set to False to load hyperparameters from a tuning run\n","# ============================================================================\n","\n","USE_DEFAULT_CONFIG = True  # Set to True to skip tuning and use default YOLO config\n","\n","# ============================================================================\n","# TUNING RUN CONFIGURATION - SPECIFY WHICH TUNING RUN TO USE\n","# ============================================================================\n","# Specify the tuning run name to load best hyperparameters from\n","# This should match the directory name in tune_train/tune/\n","#\n","# Example: TUNING_RUN_NAME = \"yolov10n_tune_20251125_143022\"\n","# Leave as None to search for the latest tuning run for this model\n","# Note: Only used if USE_DEFAULT_CONFIG = False\n","# ============================================================================\n","\n","TUNING_RUN_NAME = None  # Set to specific tuning run name, or None to auto-detect latest\n","\n","# ============================================================================\n","# TRAINING RUN CONFIGURATION - RESUME OR CREATE NEW\n","# ============================================================================\n","# To RESUME an existing training run: Set RESUME_TRAINING_RUN_NAME to the run directory name\n","# To START NEW training: Leave RESUME_TRAINING_RUN_NAME as None or empty string\n","#\n","# Example to resume: RESUME_TRAINING_RUN_NAME = \"yolov10n_train_20251125_150000\"\n","# ============================================================================\n","\n","RESUME_TRAINING_RUN_NAME = None  # Set to run name to resume, or None to create new run\n","\n","# Find or verify tuning run (only if not using default config)\n","TUNE_TRAIN_BASE = BASE_DIR / 'tune_train'\n","TUNE_BASE_DIR = TUNE_TRAIN_BASE / 'tune'\n","\n","if USE_DEFAULT_CONFIG:\n","    # Using default configuration - skip tuning run search\n","    print('\\nâš™ï¸  CONFIGURATION MODE: Using Default YOLO Configuration')\n","    print('   No hyperparameter tuning will be applied')\n","    TUNE_DIR = None\n","    TUNING_RUN_NAME = None\n","    best_hyperparams_path = None\n","else:\n","    # Using tuned hyperparameters - find or verify tuning run\n","    print('\\nâš™ï¸  CONFIGURATION MODE: Using Tuned Hyperparameters')\n","\n","    if TUNING_RUN_NAME:\n","        # Use specified tuning run\n","        TUNE_DIR = TUNE_BASE_DIR / TUNING_RUN_NAME\n","        if not TUNE_DIR.exists():\n","            raise FileNotFoundError(\n","                f\"Specified tuning run not found: {TUNE_DIR}\\n\"\n","                f\"Available runs in {TUNE_BASE_DIR}:\\n\" +\n","                '\\n'.join(f\"  - {d.name}\" for d in TUNE_BASE_DIR.glob(f'{MODEL_NAME}_tune_*') if d.is_dir())\n","            )\n","        print(f'   ğŸ“‚ Using specified tuning run: {TUNING_RUN_NAME}')\n","    else:\n","        # Auto-detect latest tuning run for this model\n","        tuning_runs = sorted(TUNE_BASE_DIR.glob(f'{MODEL_NAME}_tune_*'), key=lambda p: p.name, reverse=True)\n","        if not tuning_runs:\n","            raise FileNotFoundError(\n","                f\"No tuning runs found for model {MODEL_NAME} in {TUNE_BASE_DIR}\\n\"\n","                f\"Please run the tuning notebook first, specify TUNING_RUN_NAME, or set USE_DEFAULT_CONFIG=True\"\n","            )\n","        TUNE_DIR = tuning_runs[0]\n","        TUNING_RUN_NAME = TUNE_DIR.name\n","        print(f'   ğŸ” Auto-detected latest tuning run: {TUNING_RUN_NAME}')\n","\n","    # Verify best hyperparameters exist\n","    best_hyperparams_path = TUNE_DIR / 'best_hyperparameters.json'\n","    if not best_hyperparams_path.exists():\n","        raise FileNotFoundError(\n","            f\"Best hyperparameters not found in tuning run: {best_hyperparams_path}\\n\"\n","            f\"Please ensure the tuning run completed successfully\"\n","        )\n","\n","    print(f'   âœ“ Found best hyperparameters: {best_hyperparams_path}')\n","\n","# Configure training run name\n","if RESUME_TRAINING_RUN_NAME:\n","    # Resume existing training run\n","    RUN_NAME_TRAINING = RESUME_TRAINING_RUN_NAME\n","    print(f'\\nğŸ”„ RESUME MODE: Will attempt to resume training run \"{RESUME_TRAINING_RUN_NAME}\"')\n","else:\n","    # Create new training run with timestamp\n","    RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    RUN_NAME_TRAINING = f'{MODEL_NAME}_train_{RUN_TIMESTAMP}'\n","    print(f'\\nğŸ†• NEW TRAINING MODE: Creating new run \"{RUN_NAME_TRAINING}\"')\n","\n","# Create training directory\n","TRAIN_DIR = TUNE_TRAIN_BASE / 'training' / RUN_NAME_TRAINING\n","TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Read dataset configuration\n","NUM_CLASSES = data_config['nc']\n","CLASS_NAMES = {i: name for i, name in enumerate(data_config['names'])}\n","CLASS_NAME_TO_ID = {name: i for i, name in enumerate(data_config['names'])}\n","\n","print('=' * 80)\n","print('CONFIGURATION SUMMARY')\n","print('=' * 80)\n","print(f'Environment: {\"Google Colab\" if \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\") else \"Local\"}')\n","print(f'Base Directory: {BASE_DIR}')\n","print(f'Model: {MODEL_NAME}')\n","print(f'Dataset: {YOLO_DATASET_ROOT.name}')\n","print(f'Data YAML: {DATA_YAML_PATH}')\n","print(f'  Dataset path in YAML: {data_config[\"path\"]}')\n","print(f'Classes: {NUM_CLASSES}')\n","print(f'Class Names: {CLASS_NAMES}')\n","print(f'Device: {device}')\n","print(f'Epochs Final Training: {EPOCHS_FINAL_TRAINING}')\n","print(f'Batch Size: {BATCH_SIZE}')\n","print(f'Image Size: {IMAGE_SIZE}')\n","print(f'Configuration Mode: {\"Default (No Tuning)\" if USE_DEFAULT_CONFIG else \"Tuned Hyperparameters\"}')\n","if not USE_DEFAULT_CONFIG:\n","    print(f'Tuning Run: {TUNING_RUN_NAME}')\n","print(f'Training Directory: {TRAIN_DIR}')\n","if USE_WANDB:\n","    print(f'W&B Logging: Enabled')\n","    print(f'  Training Project: {WANDB_PROJECT_TRAINING}')\n","else:\n","    print(f'W&B Logging: Disabled')\n","print('=' * 80)"]},{"cell_type":"markdown","id":"0af35c3f","metadata":{"id":"0af35c3f"},"source":["## 4. Load Base YOLO Model"]},{"cell_type":"code","execution_count":15,"id":"3deeda88","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3deeda88","executionInfo":{"status":"ok","timestamp":1764198851574,"user_tz":-180,"elapsed":246,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"552a125a-9b59-4148-bd0d-41fbed862c08"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Model loaded from /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m/yolov8m.pt\n","YOLOv8m summary: 169 layers, 25,902,640 parameters, 0 gradients, 79.3 GFLOPs\n","\n","ğŸ“Š Model Information:\n","  Model: yolov8m\n","  Classes in model: 80\n","  Task: detect\n","  Parameters: 25.9M\n","  Model Size: 0.0 MB\n","  FLOPs (640x640): 79.32 GFLOPs\n"]}],"source":["# Load YOLO model with automatic download\n","model_path = MODELS_DIR / f'{MODEL_NAME}.pt'\n","\n","if not model_path.exists():\n","    print(f'Model not found at {model_path}')\n","    print(f'Downloading {MODEL_NAME} ...')\n","\n","    try:\n","        # Download model - ensure .pt extension for ultralytics\n","        # Ultralytics expects model names with .pt extension for download\n","        if not MODEL_NAME.endswith('.pt'):\n","            model_name_for_download = MODEL_NAME + '.pt'\n","        else:\n","            model_name_for_download = MODEL_NAME\n","\n","        print(f'  Requesting model: {model_name_for_download}')\n","        model = YOLO(model_name_for_download)\n","\n","        # Create models directory\n","        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","        # Save model to our directory using export/save\n","        try:\n","            # Try to save using the model's save method\n","            if hasattr(model, 'save'):\n","                model.save(str(model_path))\n","                print(f'âœ“ Model downloaded and saved to {model_path}')\n","                print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n","            else:\n","                # Fallback: copy from cache\n","                cache_patterns = [\n","                    str(Path.home() / '.cache' / 'ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n","                    str(Path.home() / '.config' / 'Ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n","                ]\n","\n","                model_found = False\n","                for pattern in cache_patterns:\n","                    cache_paths = glob.glob(pattern, recursive=True)\n","                    if cache_paths:\n","                        shutil.copy(cache_paths[0], model_path)\n","                        print(f'âœ“ Model downloaded and saved to {model_path}')\n","                        print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n","                        model_found = True\n","                        break\n","\n","                if not model_found:\n","                    print(f'âœ“ Model loaded from ultralytics cache')\n","                    print(f'  Note: Model is in cache, not copied to {model_path}')\n","                    print(f'  This is normal and the model will work correctly')\n","        except Exception as save_error:\n","            print(f'âš ï¸  Could not save model to custom location: {save_error}')\n","            print(f'âœ“ Model loaded successfully from ultralytics cache')\n","\n","    except Exception as download_error:\n","        print(f'\\nâŒ Error downloading model: {download_error}')\n","        raise\n","else:\n","    model = YOLO(str(model_path))\n","    print(f'âœ“ Model loaded from {model_path}')\n","\n","# Get model information\n","model_info_dict = {}\n","model_info_result = model.info()\n","model_info_keys = [\"layers\", \"params\", \"size(MB)\", \"FLOPs(G)\"]\n","\n","for info_key, info_value in zip(model_info_keys, model_info_result):\n","    model_info_dict[info_key] = info_value\n","\n","model_params = model_info_dict.get(\"params\", 0)\n","model_size_mb = model_info_dict.get(\"size(MB)\", 0)\n","flops_gflops = model_info_dict.get(\"FLOPs(G)\", 0)\n","\n","\n","print(f'\\nğŸ“Š Model Information:')\n","print(f'  Model: {MODEL_NAME}')\n","print(f'  Classes in model: {len(model.names)}')\n","print(f'  Task: {model.task}')\n","print(f'  Parameters: {model_params / 1e6:.1f}M')\n","print(f'  Model Size: {model_size_mb:.1f} MB')\n","print(f'  FLOPs (640x640): {flops_gflops:.2f} GFLOPs')"]},{"cell_type":"markdown","id":"5fe0b94d","metadata":{"id":"5fe0b94d"},"source":["## 6. Verify Dataset Structure"]},{"cell_type":"code","execution_count":16,"id":"71b15401","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71b15401","executionInfo":{"status":"ok","timestamp":1764198856098,"user_tz":-180,"elapsed":909,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"f066e1e5-de1a-451f-b484-48758f2327cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Verifying YOLO dataset structure...\n","\n","ğŸ“ Dataset Root: /computer_vision_yolo/bdd100k_yolo_limited\n","  âœ“ train:  29959 images,  29959 labels\n","  âœ“ val  :  10000 images,  10000 labels\n","  âœ“ test :  20000 images,  20000 labels\n","\n","ğŸ“„ Configuration: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tmp/yolov8m/data.yaml\n","  Classes: 10\n","  Names: {0: 'person', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motor', 7: 'bike', 8: 'traffic light', 9: 'traffic sign'}\n","\n","âœ“ Dataset verified: 59,959 total images\n","âœ“ Ready for training\n"]}],"source":["# ============================================================================\n","# VERIFY DATASET STRUCTURE\n","# ============================================================================\n","\n","print('Verifying YOLO dataset structure...')\n","print(f'\\nğŸ“ Dataset Root: {YOLO_DATASET_ROOT}')\n","\n","# Check all splits using constants\n","dataset_stats = {}\n","for split in [DatasetSplit.TRAIN, DatasetSplit.VAL, DatasetSplit.TEST]:\n","    images_dir = YOLO_DATASET_ROOT / 'images' / split\n","    labels_dir = YOLO_DATASET_ROOT / 'labels' / split\n","\n","    if images_dir.exists() and labels_dir.exists():\n","        num_images = len(list(images_dir.glob('*.jpg'))) + len(list(images_dir.glob('*.png')))\n","        num_labels = len(list(labels_dir.glob('*.txt')))\n","        dataset_stats[split] = {'images': num_images, 'labels': num_labels}\n","        print(f'  âœ“ {split:5s}: {num_images:6d} images, {num_labels:6d} labels')\n","    else:\n","        print(f'  âš ï¸  {split:5s}: Directory not found')\n","        dataset_stats[split] = {'images': 0, 'labels': 0}\n","\n","print(f'\\nğŸ“„ Configuration: {DATA_YAML_PATH}')\n","print(f'  Classes: {NUM_CLASSES}')\n","print(f'  Names: {CLASS_NAMES}')\n","\n","total_images = sum(stats['images'] for stats in dataset_stats.values())\n","print(f'\\nâœ“ Dataset verified: {total_images:,} total images')\n","print('âœ“ Ready for training')"]},{"cell_type":"markdown","id":"f03686b4","metadata":{"id":"f03686b4"},"source":["## 5. Load Best Hyperparameters from Tuning"]},{"cell_type":"code","execution_count":17,"id":"f67e089e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f67e089e","executionInfo":{"status":"ok","timestamp":1764198857544,"user_tz":-180,"elapsed":17,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"9c6a4d0d-6c09-4369-881c-2df98c7249f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","USING DEFAULT YOLO CONFIGURATION\n","================================================================================\n","No hyperparameter tuning applied - using YOLO defaults\n","\n","âœ“ Training will use default YOLO hyperparameters\n","   Default values will be applied by the YOLO model\n","================================================================================\n"]}],"source":["# ============================================================================\n","# LOAD HYPERPARAMETERS (TUNED OR DEFAULT)\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","if USE_DEFAULT_CONFIG:\n","    print('USING DEFAULT YOLO CONFIGURATION')\n","    print('=' * 80)\n","    print('No hyperparameter tuning applied - using YOLO defaults')\n","\n","    # Use empty dict for hyperparameters - YOLO will use its defaults\n","    best_params = {}\n","\n","    print('\\nâœ“ Training will use default YOLO hyperparameters')\n","    print('   Default values will be applied by the YOLO model')\n","\n","else:\n","    print('LOADING BEST HYPERPARAMETERS FROM TUNING')\n","    print('=' * 80)\n","    print(f'Tuning Run: {TUNING_RUN_NAME}')\n","    print(f'Hyperparameters Path: {best_hyperparams_path}')\n","\n","    # Load best hyperparameters from JSON\n","    with open(best_hyperparams_path, 'r', encoding='utf-8') as f:\n","        best_params_file = json.load(f)\n","\n","    # Extract only the actual hyperparameters (not metadata)\n","    # The file structure has metadata fields and a 'hyperparameters' field with the actual params\n","    if 'hyperparameters' in best_params_file:\n","        # New format: metadata + hyperparameters nested\n","        best_params = best_params_file['hyperparameters']\n","        print('\\nâœ“ Loaded hyperparameters from nested structure')\n","    else:\n","        # Old format: hyperparameters directly in root\n","        # Filter out metadata fields that aren't YOLO parameters\n","        metadata_keys = {'model', 'dataset_root', 'data_yaml_path', 'notes',\n","                        'optimization_results', 'timestamp'}\n","        best_params = {k: v for k, v in best_params_file.items() if k not in metadata_keys}\n","        print('\\nâœ“ Loaded hyperparameters from flat structure (filtered metadata)')\n","\n","    print('\\nâœ“ Best Hyperparameters Loaded:')\n","    for key, value in sorted(best_params.items()):\n","        if isinstance(value, (int, float)):\n","            if isinstance(value, float):\n","                print(f'  {key:20s}: {value:.6f}')\n","            else:\n","                print(f'  {key:20s}: {value}')\n","        else:\n","            print(f'  {key:20s}: {value}')\n","\n","    # Load tuning metadata if available\n","    tuning_metadata_path = TUNE_DIR / 'optimization_metadata.json'\n","    if (not USE_DEFAULT_CONFIG and tuning_metadata_path.exists()):\n","        with open(tuning_metadata_path, 'r', encoding='utf-8') as f:\n","            tuning_metadata = json.load(f)\n","\n","        print('\\nğŸ“Š Tuning Run Summary:')\n","        print(f\"  Best Trial: {tuning_metadata.get('best_trial', 'N/A')}\")\n","        print(f\"  Best mAP@0.5: {tuning_metadata.get('best_map50', 0):.4f}\")\n","        print(f\"  Total Trials: {tuning_metadata.get('total_trials', 'N/A')}\")\n","        print(f\"  Completed Trials: {tuning_metadata.get('completed_trials', 'N/A')}\")\n","\n","        if 'optimization_duration' in tuning_metadata:\n","            print(f\"  Duration: {tuning_metadata['optimization_duration']}\")\n","\n","print('=' * 80)"]},{"cell_type":"markdown","id":"83385af0","metadata":{"id":"83385af0"},"source":["## 7. Train The Model"]},{"cell_type":"code","execution_count":18,"id":"df0bc9da","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"df0bc9da","executionInfo":{"status":"ok","timestamp":1764215355274,"user_tz":-180,"elapsed":16494368,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"c4c8c183-1be2-4d3c-93ee-6563c7739e9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","TRAINING FINAL MODEL WITH DEFAULT CONFIGURATION\n","================================================================================\n","\n","ğŸ“¦ Loading base model: yolov8m\n","\n","ğŸš€ Starting training...\n","  Configuration: Default YOLO\n","  Epochs: 100\n","  Batch Size: 96\n","  Dataset: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tmp/yolov8m/data.yaml\n","  Device: cuda\n","  Resume: False\n","\n","ğŸ“Š Using YOLO default hyperparameters (no custom values)\n","\n","This may take a while. Training progress will be displayed below.\n","================================================================================\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251126_231421-4ihd75pn</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/4ihd75pn' target=\"_blank\">yolov8m_train_20251126_231411</a></strong> to <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/4ihd75pn' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/4ihd75pn</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ“ W&B initialized: yolo-bdd100k_yolo_limited-training/yolov8m_train_20251126_231411\n","Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=96, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tmp/yolov8m/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m/yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8m_train_20251126_231411, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411, save_frames=False, save_json=False, save_period=1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","Overriding model.yaml nc=80 with nc=10\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   3781486  ultralytics.nn.modules.head.Detect           [10, [192, 384, 576]]         \n","Model summary: 169 layers, 25,862,110 parameters, 25,862,094 gradients, 79.1 GFLOPs\n","\n","Transferred 469/475 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1488.7Â±483.1 MB/s, size: 62.7 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/train.cache... 29959 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29959/29959 24.5Mit/s 0.0s\n","\u001b[34m\u001b[1mtrain: \u001b[0m/computer_vision_yolo/bdd100k_yolo_limited/images/train/75055858-7d04a650.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 920.2Â±535.1 MB/s, size: 51.7 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 10000/10000 9.1Mit/s 0.0s\n","Plotting labels to /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.00075), 83 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1m/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      1/100      38.1G      1.338       1.18      1.031        260        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:08\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.9s\n","                   all      10000     185578      0.651      0.411      0.441       0.25\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      2/100      35.8G      1.286     0.8349     0.9905        233        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 39.1s\n","                   all      10000     185578      0.567      0.402      0.436      0.239\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      3/100        36G      1.324     0.8509      1.006        180        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 39.1s\n","                   all      10000     185578      0.598      0.367      0.377        0.2\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      4/100      38.1G      1.344     0.8678      1.018        266        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 39.0s\n","                   all      10000     185578      0.599      0.377       0.39      0.209\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      5/100      35.9G      1.317     0.8301      1.011        187        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.8s\n","                   all      10000     185578       0.64        0.4      0.429      0.231\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      6/100      38.1G      1.299     0.8106      1.005        232        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.7s\n","                   all      10000     185578      0.658      0.402      0.434      0.236\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      7/100      38.2G      1.283     0.7889     0.9989        317        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.3it/s 39.4s\n","                   all      10000     185578      0.661       0.41      0.447      0.245\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      8/100      37.8G      1.271     0.7754     0.9941        301        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578      0.671      0.418      0.456      0.254\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      9/100      36.6G      1.265     0.7631     0.9915        327        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.3s\n","                   all      10000     185578      0.684      0.433      0.473      0.262\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     10/100      37.6G      1.255     0.7547     0.9872        262        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.7s\n","                   all      10000     185578      0.674      0.433      0.473      0.262\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     11/100      37.9G      1.248     0.7468     0.9846        316        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.5s\n","                   all      10000     185578      0.586      0.445      0.481      0.268\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     12/100      38.1G      1.242     0.7372     0.9825        341        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578      0.675      0.452      0.489      0.273\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     13/100      37.8G       1.24     0.7339     0.9801        290        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.3s\n","                   all      10000     185578      0.699      0.449      0.496      0.277\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     14/100      36.1G      1.231     0.7278     0.9772        308        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.1s\n","                   all      10000     185578      0.586       0.46      0.498      0.279\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     15/100      38.8G      1.231     0.7225     0.9776        316        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578      0.706       0.46      0.512      0.284\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     16/100      36.6G      1.224     0.7173     0.9732        293        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578      0.688      0.467      0.504      0.284\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     17/100      38.4G      1.225     0.7136      0.975        286        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.5s\n","                   all      10000     185578      0.702      0.463      0.509      0.288\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     18/100      38.5G      1.217     0.7089     0.9725        272        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.3s\n","                   all      10000     185578      0.715      0.468      0.518      0.294\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     19/100        38G      1.219     0.7092     0.9725        245        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578       0.71      0.466      0.515      0.293\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     20/100      38.7G      1.212     0.7028     0.9698        223        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.4s\n","                   all      10000     185578      0.712      0.469       0.52      0.295\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     21/100      37.7G       1.21     0.7003     0.9699        223        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.2s\n","                   all      10000     185578      0.613      0.477      0.524      0.296\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     22/100      38.1G      1.205     0.6947      0.968        148        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.2s\n","                   all      10000     185578      0.715      0.476      0.527      0.299\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     23/100      35.6G      1.206     0.6933     0.9666        334        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.2s\n","                   all      10000     185578      0.615      0.482      0.528      0.302\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     24/100      36.1G      1.201      0.689     0.9653        263        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.1s\n","                   all      10000     185578      0.712      0.483      0.533      0.304\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     25/100        38G      1.198     0.6855     0.9645        281        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.1s\n","                   all      10000     185578      0.718      0.482      0.532      0.304\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     26/100      37.6G      1.199     0.6836     0.9635        271        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.1s\n","                   all      10000     185578       0.72      0.486      0.539      0.305\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     27/100      37.8G      1.197     0.6827     0.9637        242        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.625      0.483      0.534      0.306\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     28/100      35.8G      1.195     0.6797     0.9619        278        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.628      0.486       0.54      0.308\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     29/100      37.4G       1.19     0.6751     0.9606        266        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.2s\n","                   all      10000     185578      0.722      0.485      0.539      0.309\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     30/100      38.5G      1.188     0.6724      0.961        225        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.623      0.489       0.54       0.31\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     31/100      38.4G      1.189     0.6708     0.9607        269        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.3s\n","                   all      10000     185578      0.718      0.495      0.545      0.314\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     32/100      37.9G      1.186     0.6682     0.9596        296        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.725      0.494      0.545      0.313\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     33/100      35.4G      1.184     0.6649      0.958        346        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.622      0.499      0.548      0.313\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     34/100      37.9G      1.182     0.6653     0.9565        209        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.721      0.496      0.544      0.311\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     35/100      38.9G      1.181     0.6631     0.9583        303        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.725      0.502      0.549      0.314\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     36/100      38.6G      1.179     0.6614     0.9559        243        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.633      0.496      0.549      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     37/100      36.5G      1.178     0.6592     0.9559        372        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.625      0.499      0.548      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     38/100        38G      1.177     0.6568     0.9564        333        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.621      0.501      0.547      0.314\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     39/100      37.7G      1.175     0.6555     0.9556        235        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.621        0.5      0.547      0.313\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     40/100      35.8G      1.172     0.6519     0.9539        268        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.1s\n","                   all      10000     185578      0.731      0.499       0.55      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     41/100      35.7G      1.172     0.6514     0.9536        304        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.629        0.5       0.55      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     42/100      37.2G      1.168     0.6487     0.9532        226        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.631      0.497      0.549      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     43/100      37.7G      1.167     0.6453     0.9509        307        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.622      0.503      0.549      0.314\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     44/100      37.8G      1.165     0.6437     0.9504        253        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.601      0.523      0.551      0.315\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     45/100      38.1G      1.162      0.642     0.9506        184        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.3s\n","                   all      10000     185578      0.593      0.521      0.552      0.316\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     46/100      36.9G      1.161     0.6405     0.9474        229        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.623      0.504      0.552      0.317\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     47/100        36G       1.16     0.6394      0.947        248        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.625      0.502      0.552      0.317\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     48/100      37.5G      1.158     0.6354      0.947        146        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.621      0.505      0.552      0.318\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     49/100      37.6G      1.158     0.6335     0.9455        261        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.624      0.506      0.554      0.318\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     50/100      37.6G      1.158     0.6329     0.9466        306        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.622      0.508      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     51/100      35.8G      1.156     0.6315     0.9455        215        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.625      0.508      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     52/100      38.1G      1.151     0.6273     0.9463        303        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.623      0.509      0.554      0.318\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     53/100      38.7G      1.151     0.6251     0.9432        292        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 38.0s\n","                   all      10000     185578      0.623      0.508      0.554      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     54/100      36.2G      1.148     0.6238     0.9451        231        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.622      0.507      0.554      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     55/100      38.5G      1.144     0.6196     0.9435        219        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.621      0.508      0.554      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     56/100      36.3G      1.145     0.6198     0.9444        219        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.624      0.506      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     57/100      36.1G      1.144     0.6178     0.9418        207        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.623      0.507      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     58/100        38G      1.143     0.6158     0.9419        310        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.623      0.508      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     59/100        36G       1.14     0.6143     0.9402        223        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.624      0.508      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     60/100      37.4G      1.139     0.6107     0.9418        181        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.623      0.509      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     61/100      37.8G      1.137     0.6089     0.9387        195        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.5s\n","                   all      10000     185578      0.595      0.528      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     62/100      37.7G      1.134     0.6061     0.9381        288        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578        0.6      0.528      0.555       0.32\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     63/100      37.8G      1.131     0.6039     0.9367        229        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.633      0.524      0.556       0.32\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     64/100      36.1G       1.13     0.6028     0.9349        208        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.597      0.527      0.556       0.32\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     65/100      36.3G      1.125     0.5981     0.9345        246        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.636      0.522      0.556       0.32\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     66/100      37.5G      1.128      0.598     0.9358        193        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.8s\n","                   all      10000     185578      0.616      0.534      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     67/100      36.3G      1.122     0.5945     0.9345        122        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:04\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.4s\n","                   all      10000     185578      0.617      0.533      0.556      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     68/100      38.4G      1.121      0.593     0.9346        285        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.617      0.533      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     69/100      37.9G      1.119     0.5893     0.9326        380        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.5s\n","                   all      10000     185578      0.613      0.534      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     70/100        39G      1.116     0.5885     0.9311        199        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.9s\n","                   all      10000     185578      0.613      0.534      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     71/100      36.7G      1.116     0.5867     0.9329        321        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.3s\n","                   all      10000     185578      0.612      0.534      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     72/100      36.6G      1.114     0.5847     0.9308        126        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.615      0.532      0.555      0.319\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     73/100      36.1G      1.109     0.5797     0.9298        196        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 313/313 1.7it/s 3:03\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.4it/s 37.7s\n","                   all      10000     185578      0.637      0.519      0.555      0.319\n","\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 63, best model saved as best.pt.\n","To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n","\n","73 epochs completed in 4.546 hours.\n","Optimizer stripped from /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/weights/last.pt, 52.0MB\n","Optimizer stripped from /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/weights/best.pt, 52.0MB\n","\n","Validating /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/weights/best.pt...\n","Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 53/53 1.1it/s 49.1s\n","                   all      10000     185578      0.596      0.527      0.555       0.32\n","                person       3220      13265      0.734      0.591      0.659      0.339\n","                 rider        515        649      0.584       0.51      0.513      0.262\n","                   car       9879     102540      0.806      0.731      0.796      0.501\n","                 truck       2689       4247      0.638      0.622      0.639      0.465\n","                   bus       1242       1597      0.642      0.611      0.639      0.496\n","                 train         14         15          0          0     0.0246     0.0172\n","                 motor        334        452      0.613      0.487      0.485      0.251\n","                  bike        578       1007      0.508      0.554      0.519      0.267\n","         traffic light       5653      26891      0.725      0.545      0.611      0.238\n","          traffic sign       8221      34915       0.71      0.619      0.669       0.36\n","Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\u001b[0m\n","\n","âœ“ Training completed successfully!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">yolov8m_train_20251126_231411</strong> at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/4ihd75pn' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/4ihd75pn</a><br> View project at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251126_231421-4ihd75pn/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ“ W&B run finished\n","\n","================================================================================\n","TRAINING SUMMARY\n","================================================================================\n","\n","ğŸ“Š Running final validation...\n","Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1568.8Â±695.5 MB/s, size: 61.3 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 10000/10000 15.0Mit/s 0.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 625/625 11.0it/s 56.7s\n","                   all      10000     185578      0.633      0.523      0.556       0.32\n","                person       3220      13265      0.753       0.58      0.659       0.34\n","                 rider        515        649      0.608      0.504      0.513       0.26\n","                   car       9879     102540      0.822      0.723      0.797      0.502\n","                 truck       2689       4247      0.652       0.61      0.639      0.466\n","                   bus       1242       1597      0.657      0.599      0.638      0.496\n","                 train         14         15      0.213     0.0569     0.0278     0.0194\n","                 motor        334        452      0.635       0.48      0.488      0.252\n","                  bike        578       1007      0.529      0.541      0.519      0.267\n","         traffic light       5653      26891      0.741      0.531      0.614       0.24\n","          traffic sign       8221      34915      0.725      0.608      0.669      0.361\n","Speed: 0.4ms preprocess, 1.5ms inference, 0.0ms loss, 0.8ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/final_val\u001b[0m\n","\n","ğŸ“Š Final Model Performance:\n","  mAP@0.5: 0.5564\n","  mAP@0.5:0.95: 0.3204\n","  Precision: 0.6335\n","  Recall: 0.5234\n","\n","ğŸ’¾ Training log saved: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/training_log.json\n","Last Weights: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/weights/last.pt\n","================================================================================\n","Best Weights: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/weights/best.pt\n","Training Directory: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\n","Start Time: 2025-11-26 23:14:22\n","Configuration: Default YOLO\n","End Time: 2025-11-27 03:48:13\n","Duration: 4:33:50.234315\n"]}],"source":["# ============================================================================\n","# TRAIN FINAL MODEL WITH OPTIMIZED HYPERPARAMETERS\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","if USE_DEFAULT_CONFIG:\n","    print('TRAINING FINAL MODEL WITH DEFAULT CONFIGURATION')\n","else:\n","    print('TRAINING FINAL MODEL WITH OPTIMIZED HYPERPARAMETERS')\n","print('=' * 80)\n","\n","# Check if resuming from previous training\n","checkpoint_path = TRAIN_DIR / 'weights' / 'last.pt'\n","training_log_path = TRAIN_DIR / 'training_log.json'\n","is_resuming = checkpoint_path.exists()\n","\n","if is_resuming:\n","    # Resume training\n","    print('\\n' + '=' * 80)\n","    print('ğŸ”„ RESUMING PREVIOUS TRAINING')\n","    print('=' * 80)\n","    print(f'Checkpoint: {checkpoint_path}')\n","\n","    # Load training log if available\n","    if training_log_path.exists():\n","        with open(training_log_path, 'r', encoding='utf-8') as f:\n","            training_log = json.load(f)\n","\n","        print(f'\\nğŸ“Š Previous Training Summary:')\n","        print(f\"  Started: {training_log.get('start_time', 'N/A')}\")\n","        if 'last_epoch' in training_log:\n","            print(f\"  Last Epoch: {training_log['last_epoch']}\")\n","        if 'best_map50' in training_log:\n","            print(f\"  Best mAP@0.5: {training_log['best_map50']:.4f}\")\n","        if 'last_checkpoint' in training_log:\n","            print(f\"  Last Checkpoint: {training_log['last_checkpoint']}\")\n","\n","    print(f'\\nâ¡ï¸  Resuming training from checkpoint')\n","    print('=' * 80)\n","\n","    # Load model from checkpoint\n","    print(f'\\nğŸ“¦ Loading model from checkpoint: {checkpoint_path}')\n","    final_model = YOLO(str(checkpoint_path))\n","    model_to_train = str(checkpoint_path)\n","    resume_training = True\n","\n","else:\n","    # Start new training\n","    print(f'\\nğŸ“¦ Loading base model: {MODEL_NAME}')\n","    final_model = YOLO(str(model_path))\n","    model_to_train = str(model_path)\n","    resume_training = False\n","\n","    # Initialize training log\n","    training_log = {\n","        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'model': MODEL_NAME,\n","        'dataset': YOLO_DATASET_ROOT.name,\n","        'config_mode': 'default' if USE_DEFAULT_CONFIG else 'tuned',\n","        'tuning_run': TUNING_RUN_NAME if not USE_DEFAULT_CONFIG else None,\n","        'best_hyperparameters': best_params,\n","        'epochs': EPOCHS_FINAL_TRAINING,\n","        'batch_size': BATCH_SIZE,\n","        'image_size': IMAGE_SIZE\n","    }\n","\n","# Prepare training parameters\n","# Note: Fixed parameters (not part of optimization) are always included\n","# Optimization parameters are added via **best_params (empty if using defaults)\n","final_training_params = {\n","    # ============================================================================\n","    # FIXED PARAMETERS - Always passed, not part of hyperparameter optimization\n","    # ============================================================================\n","    'data': str(DATA_YAML_PATH),              # Dataset configuration file\n","    'epochs': EPOCHS_FINAL_TRAINING,          # Number of training epochs\n","    'batch': BATCH_SIZE,                       # Batch size\n","    'imgsz': IMAGE_SIZE,                       # Input image size\n","    'device': device,                          # Training device (cuda/cpu)\n","    'project': str(TRAIN_DIR.parent),         # Project directory\n","    'name': TRAIN_DIR.name,                    # Run name\n","    'exist_ok': True,                          # Overwrite existing project\n","    'patience': ModelConfig.DEFAULT_PATIENCE,  # Early stopping patience\n","    'save_period': ModelConfig.DEFAULT_SAVE_PERIOD,  # Save checkpoint frequency\n","    'workers': ModelConfig.DEFAULT_WORKERS,    # Number of data loading workers\n","    'verbose': True,                           # Verbose output\n","    'seed': 42,                                # Random seed for reproducibility\n","    'close_mosaic': ModelConfig.CLOSE_MOSAIC_EPOCHS,  # Disable mosaic in final epochs\n","    'resume': resume_training,                 # Resume from checkpoint if exists\n","    'cache': ModelConfig.DEFAULT_CACHE,        # Cache images for faster training\n","    'val': ModelConfig.DEFAULT_VAL,            # Run validation during training\n","\n","    # ============================================================================\n","    # OPTIMIZATION PARAMETERS - From tuning (if USE_DEFAULT_CONFIG=False)\n","    # ============================================================================\n","    # Parameters like: lr0, lrf, momentum, weight_decay, warmup_epochs, etc.\n","    **best_params  # Empty dict if USE_DEFAULT_CONFIG=True, tuned params otherwise\n","}\n","\n","print(f'\\nğŸš€ {\"Resuming\" if resume_training else \"Starting\"} training...')\n","print(f'  Configuration: {\"Default YOLO\" if USE_DEFAULT_CONFIG else \"Tuned Hyperparameters\"}')\n","print(f'  Epochs: {final_training_params[\"epochs\"]}')\n","print(f'  Batch Size: {final_training_params[\"batch\"]}')\n","print(f'  Dataset: {DATA_YAML_PATH}')\n","print(f'  Device: {device}')\n","print(f'  Resume: {resume_training}')\n","\n","if best_params:\n","    print('\\nğŸ“Š Applied Hyperparameters:')\n","    for key, value in sorted(best_params.items()):\n","        if isinstance(value, float):\n","            print(f'  {key:20s}: {value:.6f}')\n","        else:\n","            print(f'  {key:20s}: {value}')\n","else:\n","    print('\\nğŸ“Š Using YOLO default hyperparameters (no custom values)')\n","\n","print('\\nThis may take a while. Training progress will be displayed below.')\n","print('=' * 80)\n","\n","# Initialize W&B for final training\n","if USE_WANDB:\n","    try:\n","        wandb_config = {\n","            'model': MODEL_NAME,\n","            'dataset': YOLO_DATASET_ROOT.name,\n","            'phase': 'final_training',\n","            'config_mode': 'default' if USE_DEFAULT_CONFIG else 'tuned',\n","            'tuning_run': TUNING_RUN_NAME if not USE_DEFAULT_CONFIG else None,\n","            'epochs': final_training_params['epochs'],\n","            'batch_size': final_training_params['batch'],\n","            'resume': resume_training,\n","            **best_params\n","        }\n","\n","        wandb_training_run = wandb.init(\n","            project=WANDB_PROJECT_TRAINING,\n","            name=RUN_NAME_TRAINING,\n","            id=training_log.get('wandb_run_id') if is_resuming else None,\n","            resume='allow' if is_resuming else None,\n","            config=wandb_config,\n","            group='final-training',\n","            tags=['final', 'optimized' if not USE_DEFAULT_CONFIG else 'default', MODEL_NAME, YOLO_DATASET_ROOT.name]\n","        )\n","\n","        # Save W&B run ID for future resume\n","        if not is_resuming:\n","            training_log['wandb_run_id'] = wandb_training_run.id\n","            with open(training_log_path, 'w', encoding='utf-8') as f:\n","                json.dump(training_log, f, indent=2)\n","\n","        print(f'âœ“ W&B initialized: {WANDB_PROJECT_TRAINING}/{RUN_NAME_TRAINING}')\n","    except Exception as wandb_error:\n","        print(f'âš ï¸  Could not initialize W&B: {wandb_error}')\n","        wandb_training_run = None\n","else:\n","    wandb_training_run = None\n","\n","# Train model\n","start_time = datetime.now()\n","try:\n","    final_results = final_model.train(**final_training_params)\n","\n","    # Update training log with completion\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    training_log['status'] = 'completed'\n","    training_log['duration'] = str(datetime.now() - start_time)\n","\n","    # Save final metrics\n","    if hasattr(final_results, 'results_dict'):\n","        training_log['final_metrics'] = final_results.results_dict\n","\n","    # Save updated training log\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","\n","    print('\\nâœ“ Training completed successfully!')\n","\n","except KeyboardInterrupt:\n","    print('\\nâš ï¸  Training interrupted by user')\n","    print(f'ğŸ’¾ Progress saved to: {TRAIN_DIR}')\n","    print(f'   - Last checkpoint: {checkpoint_path}')\n","    print(f'   - Training log: {training_log_path}')\n","    print(f'\\nğŸ”„ To resume: Simply re-run this notebook')\n","\n","    # Update training log\n","    training_log['last_interrupt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    training_log['status'] = 'interrupted'\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","    raise\n","\n","except Exception as e:\n","    print(f'\\nâŒ Training failed with error: {e}')\n","    training_log['status'] = 'failed'\n","    training_log['error'] = str(e)\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","    raise\n","\n","finally:\n","    if USE_WANDB and wandb_training_run is not None:\n","        wandb_training_run.finish()\n","        print('âœ“ W&B run finished')\n","\n","end_time = datetime.now()\n","duration = end_time - start_time\n","\n","print('\\n' + '=' * 80)\n","print('TRAINING SUMMARY')\n","print('=' * 80)\n","\n","# Get final validation metrics\n","print('\\nğŸ“Š Running final validation...')\n","final_val_results = final_model.val(\n","    data=str(DATA_YAML_PATH),\n","    project=str(TRAIN_DIR),\n","    name='final_val',\n",")\n","\n","final_metrics = {\n","    'map50': float(final_val_results.box.map50),\n","    'map50_95': float(final_val_results.box.map),\n","    'precision': float(final_val_results.box.mp),\n","    'recall': float(final_val_results.box.mr),\n","}\n","\n","print('\\nğŸ“Š Final Model Performance:')\n","print(f\"  mAP@0.5: {final_metrics['map50']:.4f}\")\n","print(f\"  mAP@0.5:0.95: {final_metrics['map50_95']:.4f}\")\n","print(f\"  Precision: {final_metrics['precision']:.4f}\")\n","print(f\"  Recall: {final_metrics['recall']:.4f}\")\n","\n","# Update training log with final metrics\n","training_log['final_metrics'] = final_metrics\n","training_log['best_model_path'] = str(TRAIN_DIR / 'weights' / 'best.pt')\n","training_log['last_model_path'] = str(TRAIN_DIR / 'weights' / 'last.pt')\n","\n","# Save final training log\n","with open(training_log_path, 'w', encoding='utf-8') as f:\n","    json.dump(training_log, f, indent=2)\n","\n","print(f'\\nğŸ’¾ Training log saved: {training_log_path}')\n","\n","# Compare with tuning results if available\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    tuning_best_map = tuning_metadata.get('best_map50', 0)\n","    improvement = final_metrics['map50'] - tuning_best_map\n","    print('\\nğŸ“ˆ Improvement vs Best Tuning Trial:')\n","    print(f\"  Best Tuning mAP@0.5: {tuning_best_map:.4f}\")\n","    print(f\"  Final Model mAP@0.5: {final_metrics['map50']:.4f}\")\n","\n","    print(f\"  Improvement: {improvement:+.4f} ({improvement/tuning_best_map*100:+.2f}%)\")\n","    print('=' * 80)\n","\n","print(f'Last Weights: {TRAIN_DIR / \"weights\" / \"last.pt\"}')\n","\n","print('=' * 80)\n","print(f'Best Weights: {TRAIN_DIR / \"weights\" / \"best.pt\"}')\n","\n","print(f'Training Directory: {TRAIN_DIR}')\n","\n","print(f'Start Time: {start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","print(f'Configuration: {\"Default YOLO\" if USE_DEFAULT_CONFIG else f\"Tuned ({TUNING_RUN_NAME})\"}')\n","\n","print(f'End Time: {end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","print(f'Duration: {duration}')"]},{"cell_type":"markdown","id":"b4b00a39","metadata":{"id":"b4b00a39"},"source":["## 8. Save Final Model and Metadata"]},{"cell_type":"code","execution_count":19,"id":"9d959bb4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d959bb4","executionInfo":{"status":"ok","timestamp":1764215355701,"user_tz":-180,"elapsed":192,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"2034732e-79ac-4801-e291-998e75a789ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","SAVING FINAL OPTIMIZED MODEL\n","================================================================================\n","\n","âœ“ Final model saved to: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","  Size: 49.6 MB\n","âœ“ Model metadata saved to: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127_metadata.json\n","\n","ğŸ“¦ Final Model Package:\n","  Model: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","  Metadata: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127_metadata.json\n","  Training Log: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/training_log.json\n","================================================================================\n"]}],"source":["# ============================================================================\n","# SAVE FINAL OPTIMIZED MODEL\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","print('SAVING FINAL OPTIMIZED MODEL')\n","print('=' * 80)\n","\n","date_stamp = datetime.now().strftime('%Y%m%d')\n","finetuned_model_name = f'{MODEL_NAME}_finetuned_{date_stamp}'\n","\n","# Create model directory if it doesn't exist\n","model_save_dir = BASE_DIR / 'models' / finetuned_model_name\n","model_save_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Define paths for saving\n","final_model_path = model_save_dir / f'{finetuned_model_name}.pt'\n","metadata_path = model_save_dir / f'{finetuned_model_name}_metadata.json'\n","\n","# Copy best weights from training directory\n","# Note: TRAIN_DIR already includes RUN_NAME_TRAINING\n","weights_path = TRAIN_DIR / 'weights' / 'best.pt'\n","\n","if weights_path.exists():\n","    shutil.copy(weights_path, final_model_path)\n","    print(f'\\nâœ“ Final model saved to: {final_model_path}')\n","    print(f'  Size: {final_model_path.stat().st_size / (1024*1024):.1f} MB')\n","else:\n","    print(f'\\nâš ï¸  Best weights not found at: {weights_path}')\n","    print('  Attempting to save current model state...')\n","    try:\n","        # Save current model state if weights not found\n","        final_model.save(str(final_model_path))\n","        print(f'âœ“ Model saved to: {final_model_path}')\n","    except Exception as save_error:\n","        print(f'âš ï¸  Error saving model: {save_error}')\n","\n","# Prepare optimization metadata\n","optimization_meta = {\n","    'tuning_run': TUNING_RUN_NAME,\n","    'tuning_run_path': str(TUNE_DIR),\n","}\n","\n","# Add tuning details if available\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    optimization_meta.update({\n","        'n_trials': tuning_metadata.get('total_trials', 'N/A'),\n","        'completed_trials': tuning_metadata.get('completed_trials', 'N/A'),\n","        'best_trial': tuning_metadata.get('best_trial', 'N/A'),\n","        'best_trial_map50': tuning_metadata.get('best_map50', 0),\n","        'optimization_duration': tuning_metadata.get('optimization_duration', 'N/A'),\n","    })\n","\n","# Calculate improvement if tuning metadata available\n","improvement_value = 0\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    tuning_best_map = tuning_metadata.get('best_map50', 0)\n","    if tuning_best_map > 0:\n","        improvement_value = float(final_metrics['map50'] - tuning_best_map)\n","\n","# Save model metadata\n","metadata = {\n","    'model_name': MODEL_NAME,\n","    'finetuned_name': finetuned_model_name,\n","    'model_path': str(final_model_path),\n","    'dataset': str(YOLO_DATASET_ROOT),\n","    'training_date': datetime.now().isoformat(),\n","    'training_run': RUN_NAME_TRAINING,\n","    'training_run_path': str(TRAIN_DIR),\n","    'optimization': optimization_meta,\n","    'best_hyperparameters': best_params,\n","    'training_params': {\n","        'epochs': EPOCHS_FINAL_TRAINING,\n","        'batch_size': BATCH_SIZE,\n","        'image_size': IMAGE_SIZE,\n","        'patience': ModelConfig.DEFAULT_PATIENCE,\n","        'save_period': ModelConfig.DEFAULT_SAVE_PERIOD,\n","    },\n","    'final_metrics': final_metrics,\n","    'improvement_vs_tuning': improvement_value,\n","}\n","\n","with open(metadata_path, 'w', encoding='utf-8') as f:\n","    json.dump(metadata, f, indent=2)\n","\n","print(f'âœ“ Model metadata saved to: {metadata_path}')\n","print('\\nğŸ“¦ Final Model Package:')\n","print(f'  Model: {final_model_path}')\n","print(f'  Metadata: {metadata_path}')\n","print(f'  Training Log: {training_log_path}')\n","print('=' * 80)"]},{"cell_type":"markdown","id":"338ab57b","metadata":{"id":"338ab57b"},"source":["## 9. Test Final Model"]},{"cell_type":"code","execution_count":20,"id":"6cfc6346","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6cfc6346","executionInfo":{"status":"ok","timestamp":1764215558084,"user_tz":-180,"elapsed":202382,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"fbc1f54c-0261-47d0-920c-21aad0ac61d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","RUNNING FINAL VALIDATION ON TEST SET\n","================================================================================\n","\n","ğŸ“‚ Dataset location mismatch detected\n","   Dataset is in: /computer_vision_yolo/bdd100k_yolo_limited\n","   Validation expects: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/bdd100k_yolo_limited\n","   Creating symbolic link...\n","   âœ“ Symbolic link created\n","\n","ğŸ” Validation Configuration:\n","   Base Dir: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo\n","   Dataset: bdd100k_yolo_limited\n","   Model: yolov8m_finetuned_20251127\n","   Expected dataset path: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/bdd100k_yolo_limited/data.yaml\n","   Expected model path: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","   Actual model path: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","âœ“ Device: cuda\n","  GPU: NVIDIA A100-SXM4-40GB\n","âœ“ W&B logging enabled\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251127_034915-gwzdkiat</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/gwzdkiat' target=\"_blank\">yolov8m_finetuned_20251127_bdd100k_yolo_limited_test_20251127_034915</a></strong> to <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/gwzdkiat' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/gwzdkiat</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","âœ“ Weights & Biases initialized: yolov8m_finetuned_20251127_bdd100k_yolo_limited_test_20251127_034915\n","âœ“ Dataset loaded\n","  Total images: 20000\n","  Images with labels: 20000\n","  Label files: 20000\n","\n","âš ï¸ Performance metadata not found: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/bdd100k_yolo_limited/representative_json/test_performance_analysis.json\n","âœ“ Model loaded from /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","Model summary: 169 layers, 25,862,110 parameters, 0 gradients, 79.1 GFLOPs\n","\n","ğŸ“Š Model Information:\n","  Model: yolov8m_finetuned_20251127\n","  Classes in model: 10\n","  Task: detect\n","  Parameters: 25.9M\n","  Model Size: 49.6 MB\n","  FLOPs (640x640): 79.09 GFLOPs\n","  Model Size: 49.6 MB\n","\n","Running YOLO validation...\n","Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1239.9Â±373.0 MB/s, size: 47.8 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/test... 20000 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 20000/20000 1.4Kit/s 13.9s\n","\u001b[34m\u001b[1mval: \u001b[0m/computer_vision_yolo/bdd100k_yolo_limited/images/test/e6f10c58-c46de527.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /computer_vision_yolo/bdd100k_yolo_limited/labels/test.cache\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 209/209 2.0it/s 1:44\n","                   all      20000     367727      0.633      0.544      0.568      0.324\n","                person       6213      24650      0.747      0.596      0.661       0.34\n","                 rider       1004       1294        0.6       0.53      0.537      0.283\n","                   car      19776     205149      0.815      0.734      0.795      0.501\n","                 truck       5500       8704      0.637      0.622      0.645      0.471\n","                   bus       2459       3217      0.607      0.589        0.6      0.463\n","                 train         26         28      0.336      0.107      0.117     0.0484\n","                 motor        640        841      0.587      0.536      0.523      0.271\n","                  bike       1182       1998      0.567      0.554      0.527      0.262\n","         traffic light      11051      52840      0.725      0.551      0.607      0.236\n","          traffic sign      16432      69006      0.708      0.625       0.67       0.36\n","Speed: 0.4ms preprocess, 1.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/yolo_test/runs/yolov8m_finetuned_20251127_testing_20251127_034915/yolo_validation\u001b[0m\n","\n","âœ“ YOLO validation completed in 124.67 seconds\n","\n","================================================================================\n","OFFICIAL YOLO VALIDATION RESULTS\n","================================================================================\n","Precision (mean): 0.6329\n","Recall (mean):    0.5445\n","mAP@0.5:          0.5683\n","mAP@0.5:0.95:     0.3236\n","Fitness:          0.3236\n","\n","âš¡ Performance Metrics:\n","  Total Time: 124.67s\n","  Average Inference Time: 2354.76 ms per image\n","  FPS (Frames Per Second): 424.67\n","================================================================================\n","(Green = Correct Predictions, Red = Incorrect Predictions, White = No Predictions)\n","\n","================================================================================\n","GENERATING SAMPLE COMPARISONS\n","================================================================================\n","\n","Generating 6 high-resolution comparison figures with attributes...\n"]},{"output_type":"stream","name":"stderr","text":["Generating comparisons: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Generated 6 comparison images\n","  Saved to: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/yolo_test/runs/yolov8m_finetuned_20251127_testing_20251127_034915/sample_comparisons\n","================================================================================\n","âœ“ COMPREHENSIVE REPORT GENERATED (script)\n","================================================================================\n","PDF Report: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/yolo_test/runs/yolov8m_finetuned_20251127_testing_20251127_034915/report.pdf\n","JSON Metrics: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/yolo_test/runs/yolov8m_finetuned_20251127_testing_20251127_034915/metrics_data.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">yolov8m_finetuned_20251127_bdd100k_yolo_limited_test_20251127_034915</strong> at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/gwzdkiat' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/gwzdkiat</a><br> View project at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251127_034915-gwzdkiat/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","âœ“ Weights & Biases run completed successfully\n","\n","ğŸ§¹ Cleaning up model from memory...\n","âœ“ Model removed from memory\n","\n","ğŸ“Š Final Validation Results:\n"]},{"output_type":"display_data","data":{"text/plain":["                   model_name               dataset split  iou  \\\n","0  yolov8m_finetuned_20251127  bdd100k_yolo_limited  test  0.5   \n","\n","   precision_confusion  recall_confusion  f1_confusion  precision_yolo  \\\n","0             0.697207          0.781419      0.736915        0.632902   \n","\n","   recall_yolo     map50  map50_95  params_m    size_mb         fps status  \\\n","0     0.544459  0.568252   0.32365  25.86211  49.609575  424.672358     ok   \n","\n","                                             run_dir  \\\n","0  /content/Drive/MyDrive/ksu_yolo_2025/computer_...   \n","\n","                                     hyperparameters  \n","0  {'data': '/content/Drive/MyDrive/ksu_yolo_2025...  "],"text/html":["\n","  <div id=\"df-bdc1fbc9-e47a-4682-bf86-45335bc03fc9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_name</th>\n","      <th>dataset</th>\n","      <th>split</th>\n","      <th>iou</th>\n","      <th>precision_confusion</th>\n","      <th>recall_confusion</th>\n","      <th>f1_confusion</th>\n","      <th>precision_yolo</th>\n","      <th>recall_yolo</th>\n","      <th>map50</th>\n","      <th>map50_95</th>\n","      <th>params_m</th>\n","      <th>size_mb</th>\n","      <th>fps</th>\n","      <th>status</th>\n","      <th>run_dir</th>\n","      <th>hyperparameters</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>yolov8m_finetuned_20251127</td>\n","      <td>bdd100k_yolo_limited</td>\n","      <td>test</td>\n","      <td>0.5</td>\n","      <td>0.697207</td>\n","      <td>0.781419</td>\n","      <td>0.736915</td>\n","      <td>0.632902</td>\n","      <td>0.544459</td>\n","      <td>0.568252</td>\n","      <td>0.32365</td>\n","      <td>25.86211</td>\n","      <td>49.609575</td>\n","      <td>424.672358</td>\n","      <td>ok</td>\n","      <td>/content/Drive/MyDrive/ksu_yolo_2025/computer_...</td>\n","      <td>{'data': '/content/Drive/MyDrive/ksu_yolo_2025...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdc1fbc9-e47a-4682-bf86-45335bc03fc9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bdc1fbc9-e47a-4682-bf86-45335bc03fc9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bdc1fbc9-e47a-4682-bf86-45335bc03fc9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_de0ecec5-8373-4055-ad02-54e49f80fb63\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_de0ecec5-8373-4055-ad02-54e49f80fb63 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('results_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"results_df","summary":"{\n  \"name\": \"results_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"yolov8m_finetuned_20251127\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bdd100k_yolo_limited\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iou\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5,\n        \"max\": 0.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6972074392144172,\n        \"max\": 0.6972074392144172,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6972074392144172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7814189662844638,\n        \"max\": 0.7814189662844638,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7814189662844638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7369151726367982,\n        \"max\": 0.7369151726367982,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7369151726367982\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_yolo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6329024084356512,\n        \"max\": 0.6329024084356512,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6329024084356512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_yolo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5444586806259059,\n        \"max\": 0.5444586806259059,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5444586806259059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5682523441502447,\n        \"max\": 0.5682523441502447,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5682523441502447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map50_95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3236495057319961,\n        \"max\": 0.3236495057319961,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3236495057319961\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_m\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 25.86211,\n        \"max\": 25.86211,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          25.86211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 49.609575271606445,\n        \"max\": 49.609575271606445,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          49.609575271606445\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 424.6723582254392,\n        \"max\": 424.6723582254392,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          424.6723582254392\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_dir\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/yolo_test/runs/yolov8m_finetuned_20251127_testing_20251127_034915\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hyperparameters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["# RUN FINAL VALIDATION ON TEST SET (ENHANCED)\n","# ============================================================================\n","print('\\n' + '=' * 80)\n","print('RUNNING FINAL VALIDATION ON TEST SET')\n","print('=' * 80)\n","\n","results_summary = []\n","IOU_THRESHOLDS = 0.5  # Could expand to [0.5, 0.55, 0.6] if needed\n","\n","# Verify that the final model exists before validation\n","if not final_model_path.exists():\n","    print(f\"âš ï¸  Warning: Final model not found at {final_model_path}\")\n","    print(f\"   Skipping test validation. Please complete training first.\")\n","else:\n","    # Add YOLO test scripts path safely\n","    scrpt_dir = BASE_DIR / \"yolo_test\"\n","    if str(scrpt_dir) not in sys.path:\n","        sys.path.append(str(scrpt_dir))\n","\n","    try:\n","        from run_yolo_validation_report import run_validation_pipeline\n","\n","        # Important:\n","        # - Datasets are in DATASET_BASE_DIR (can be different in Colab)\n","        # - Models are ALWAYS in BASE_DIR/models/\n","        #\n","        # Validation script uses base_dir for both:\n","        #   - Dataset path: base_dir / dataset_name / data.yaml\n","        #   - Model path: base_dir / models / model_name / model_name.pt\n","        #\n","        # Solution: Copy dataset to BASE_DIR temporarily, or use symlink\n","\n","        # For Colab: Need to ensure model is accessible\n","        if IS_COLAB:\n","            # Check if dataset exists in BASE_DIR\n","            base_dir_dataset = BASE_DIR / dataset_name\n","            if not (base_dir_dataset / 'data.yaml').exists() and YOLO_DATASET_ROOT.exists():\n","                print(f\"\\nğŸ“‚ Dataset location mismatch detected\")\n","                print(f\"   Dataset is in: {YOLO_DATASET_ROOT}\")\n","                print(f\"   Validation expects: {base_dir_dataset}\")\n","                print(f\"   Creating symbolic link...\")\n","                try:\n","                    import os\n","                    if not base_dir_dataset.exists():\n","                        os.symlink(str(YOLO_DATASET_ROOT), str(base_dir_dataset))\n","                        print(f\"   âœ“ Symbolic link created\")\n","                except Exception as symlink_error:\n","                    print(f\"   âš ï¸  Could not create symlink: {symlink_error}\")\n","                    print(f\"   Validation may fail if dataset path is incorrect\")\n","\n","            validation_base_dir = BASE_DIR\n","        else:\n","            validation_base_dir = BASE_DIR\n","\n","        print(f\"\\nğŸ” Validation Configuration:\")\n","        print(f\"   Base Dir: {validation_base_dir}\")\n","        print(f\"   Dataset: {dataset_name}\")\n","        print(f\"   Model: {finetuned_model_name}\")\n","        print(f\"   Expected dataset path: {validation_base_dir / dataset_name / 'data.yaml'}\")\n","        print(f\"   Expected model path: {validation_base_dir / 'models' / finetuned_model_name / f'{finetuned_model_name}.pt'}\")\n","        print(f\"   Actual model path: {final_model_path}\")\n","\n","        # Verify paths\n","        expected_model_path = validation_base_dir / 'models' / finetuned_model_name / f'{finetuned_model_name}.pt'\n","        if final_model_path != expected_model_path and not expected_model_path.exists():\n","            print(f\"\\nğŸ“¦ Copying model to expected location...\")\n","            expected_model_path.parent.mkdir(parents=True, exist_ok=True)\n","            shutil.copy(final_model_path, expected_model_path)\n","            print(f\"   âœ“ Model copied to {expected_model_path}\")\n","\n","        result = run_validation_pipeline(\n","            model_name=finetuned_model_name,\n","            dataset_name=dataset_name,\n","            split=\"test\",\n","            iou_threshold=IOU_THRESHOLDS,\n","            base_dir=validation_base_dir,\n","            use_wandb=True,\n","            save_reports=True,\n","            batch_size=BATCH_SIZE,\n","        )\n","\n","        overall = result[\"metrics\"][\"overall\"]\n","        yolo_overall = result[\"metrics\"][\"yolo_metrics\"]\n","\n","        results_summary.append({\n","            \"model_name\": finetuned_model_name,\n","            \"dataset\": dataset_name,\n","            \"split\": \"test\",\n","            \"iou\": IOU_THRESHOLDS,\n","            \"precision_confusion\": overall[\"precision\"],\n","            \"recall_confusion\": overall[\"recall\"],\n","            \"f1_confusion\": overall[\"f1\"],\n","            \"precision_yolo\": yolo_overall[\"precision\"],\n","            \"recall_yolo\": yolo_overall[\"recall\"],\n","            \"map50\": yolo_overall[\"map50\"],\n","            \"map50_95\": yolo_overall[\"map50_95\"],\n","            \"params_m\": result[\"model_info\"][\"params\"] / 1e6,\n","            \"size_mb\": result[\"model_info\"][\"size(MB)\"],\n","            \"fps\": result[\"metrics\"][\"fps\"],\n","            \"status\": \"ok\",\n","            \"run_dir\": str(result[\"run_dir\"]),\n","            \"hyperparameters\": final_training_params,  # traceable\n","        })\n","\n","    except Exception as e:\n","        print(f\"âš ï¸ Model {finetuned_model_name} failed during validation: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        results_summary.append({\n","            \"model_name\": finetuned_model_name,\n","            \"dataset\": dataset_name,\n","            \"split\": \"test\",\n","            \"iou\": IOU_THRESHOLDS,\n","            \"status\": \"error\",\n","            \"error_message\": str(e)\n","        })\n","\n","# Convert to DataFrame\n","if results_summary:\n","    results_df = pd.DataFrame(results_summary)\n","    print('\\nğŸ“Š Final Validation Results:')\n","    display(results_df)\n","else:\n","    print('\\nâš ï¸  No validation results - model training not completed yet')\n"]},{"cell_type":"markdown","id":"35471d99","metadata":{"id":"35471d99"},"source":["## 10. Generate Training Report (PDF)"]},{"cell_type":"code","execution_count":21,"id":"b66f810c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b66f810c","executionInfo":{"status":"ok","timestamp":1764215572467,"user_tz":-180,"elapsed":14369,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"c4d09dcd-8fed-4b36-ed9a-3eacfa7b64e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","GENERATING COMPREHENSIVE TRAINING PDF REPORT\n","================================================================================\n","\n","âœ“ Comprehensive Training PDF generated: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/yolov8m_training_report.pdf\n"]}],"source":["# GENERATE COMPREHENSIVE TRAINING PDF REPORT\n","# ============================================================================\n","from reportlab.lib.pagesizes import A4\n","from reportlab.lib import colors as rl_colors\n","from reportlab.lib.units import inch\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.enums import TA_CENTER, TA_LEFT\n","import platform\n","import psutil\n","\n","print('\\n' + '=' * 80)\n","print('GENERATING COMPREHENSIVE TRAINING PDF REPORT')\n","print('=' * 80)\n","\n","pdf_training_report_path = TRAIN_DIR / f'{MODEL_NAME}_training_report.pdf'\n","doc = SimpleDocTemplate(str(pdf_training_report_path), pagesize=A4,\n","                       rightMargin=30, leftMargin=30,\n","                       topMargin=30, bottomMargin=30)\n","story = []\n","styles = getSampleStyleSheet()\n","\n","# Custom styles\n","title_style = ParagraphStyle('Title', parent=styles['Heading1'], fontSize=24,\n","                             textColor=rl_colors.HexColor('#2c3e50'), alignment=TA_CENTER, spaceAfter=20)\n","heading_style = ParagraphStyle('Heading', parent=styles['Heading2'], fontSize=16,\n","                               textColor=rl_colors.HexColor('#34495e'), spaceAfter=12, spaceBefore=20)\n","normal_style = ParagraphStyle('Normal', parent=styles['Normal'], fontSize=10)\n","\n","# --- Title ---\n","story.append(Paragraph(f'{MODEL_NAME} Final Training Report', title_style))\n","story.append(Spacer(1, 12))\n","\n","# --- System Info ---\n","story.append(Paragraph('System Information', heading_style))\n","sys_info_data = [\n","    ['OS', platform.system() + ' ' + platform.release()],\n","    ['Python Version', platform.python_version()],\n","    ['PyTorch Version', torch.__version__],\n","    ['CUDA Available', str(torch.cuda.is_available())],\n","    ['Device', device],\n","    ['RAM (GB)', f\"{psutil.virtual_memory().total/1e9:.2f}\"],\n","]\n","sys_table = Table(sys_info_data, colWidths=[2.5*inch, 3.5*inch])\n","sys_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#95a5a6')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","]))\n","story.append(sys_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Dataset Info ---\n","story.append(Paragraph('Dataset Information', heading_style))\n","# Wrap class names text for better readability\n","class_names_text = ', '.join(str(name) for name in CLASS_NAMES.values())\n","class_names_wrapped = Paragraph(class_names_text, normal_style)\n","\n","dataset_info_data = [\n","    ['Property', 'Value'],\n","    ['Dataset', YOLO_DATASET_ROOT.name],\n","    ['Number of Classes', str(NUM_CLASSES)],\n","    ['Train Images', str(dataset_stats.get('train', {}).get('images', 'N/A'))],\n","    ['Val Images', str(dataset_stats.get('val', {}).get('images', 'N/A'))],\n","    ['Test Images', str(dataset_stats.get('test', {}).get('images', 'N/A'))],\n","    ['Data YAML', str(DATA_YAML_PATH.name)],\n","]\n","dataset_table = Table(dataset_info_data, colWidths=[2*inch, 4*inch])\n","dataset_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#16a085')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(dataset_table)\n","story.append(Spacer(1, 6))\n","# Add class names separately with wrapping\n","story.append(Paragraph('<b>Classes:</b>', normal_style))\n","story.append(class_names_wrapped)\n","story.append(Spacer(1, 12))\n","\n","# --- Optimization Summary ---\n","story.append(Paragraph('Optimization Summary', heading_style))\n","opt_summary_data = [\n","    ['Metric', 'Value'],\n","    ['Tuning Run', TUNING_RUN_NAME],\n","    ['Total Trials', str(tuning_metadata.get('total_trials', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Completed Trials', str(tuning_metadata.get('completed_trials', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Best Trial Number', str(tuning_metadata.get('best_trial', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Best Trial mAP@0.5', f\"{tuning_metadata.get('best_map50', 0):.4f}\" if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists() else 'N/A'],\n","    ['Final Training Epochs', str(EPOCHS_FINAL_TRAINING)],\n","]\n","opt_table = Table(opt_summary_data, colWidths=[3*inch, 3*inch])\n","opt_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#f39c12')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(opt_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Optimized Hyperparameters ---\n","story.append(PageBreak())\n","story.append(Paragraph('Optimized Hyperparameters Used', heading_style))\n","hyperparam_data = [['Parameter', 'Value']]\n","for key, value in best_params.items():\n","    hyperparam_data.append([key, f\"{value:.6f}\" if isinstance(value, float) else str(value)])\n","hyperparam_table = Table(hyperparam_data, colWidths=[3*inch, 3*inch])\n","hyperparam_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#3498db')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(hyperparam_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Training Process Details ---\n","story.append(PageBreak())\n","story.append(Paragraph('Training Process Analysis', heading_style))\n","\n","# Try to load training results CSV for detailed epoch-by-epoch analysis\n","# YOLO saves results.csv directly in the training run directory\n","results_csv = TRAIN_DIR / 'results.csv'\n","if results_csv.exists():\n","    try:\n","        import pandas as pd\n","        import matplotlib.pyplot as plt\n","        import matplotlib\n","        matplotlib.use('Agg')\n","\n","        # Load results\n","        training_results = pd.read_csv(results_csv)\n","        training_results.columns = training_results.columns.str.strip()\n","\n","        story.append(Paragraph('Epoch-by-Epoch Training Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # Create comprehensive training curves\n","        fig, axes = plt.subplots(3, 2, figsize=(12, 14))\n","        fig.suptitle('Training Progress Over Epochs', fontsize=16, fontweight='bold')\n","\n","        # 1. Loss Curves (Train/Box/Cls/DFL)\n","        ax = axes[0, 0]\n","        if 'train/box_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/box_loss'],\n","                   label='Box Loss', color='#e74c3c', linewidth=2)\n","        if 'train/cls_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/cls_loss'],\n","                   label='Class Loss', color='#3498db', linewidth=2)\n","        if 'train/dfl_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/dfl_loss'],\n","                   label='DFL Loss', color='#f39c12', linewidth=2)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Loss', fontsize=10)\n","        ax.set_title('Training Loss Components', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 2. Validation Loss Curves\n","        ax = axes[0, 1]\n","        if 'val/box_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/box_loss'],\n","                   label='Box Loss', color='#e74c3c', linewidth=2, linestyle='--')\n","        if 'val/cls_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/cls_loss'],\n","                   label='Class Loss', color='#3498db', linewidth=2, linestyle='--')\n","        if 'val/dfl_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/dfl_loss'],\n","                   label='DFL Loss', color='#f39c12', linewidth=2, linestyle='--')\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Loss', fontsize=10)\n","        ax.set_title('Validation Loss Components', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 3. mAP Metrics Over Epochs\n","        ax = axes[1, 0]\n","        if 'metrics/mAP50(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/mAP50(B)'],\n","                   label='mAP@0.5', color='#27ae60', linewidth=2.5, marker='o', markersize=4)\n","        if 'metrics/mAP50-95(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/mAP50-95(B)'],\n","                   label='mAP@0.5:0.95', color='#16a085', linewidth=2.5, marker='s', markersize=4)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('mAP', fontsize=10)\n","        ax.set_title('mAP Progression', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim(0, 1)\n","\n","        # 4. Precision and Recall\n","        ax = axes[1, 1]\n","        if 'metrics/precision(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/precision(B)'],\n","                   label='Precision', color='#9b59b6', linewidth=2.5, marker='^', markersize=4)\n","        if 'metrics/recall(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/recall(B)'],\n","                   label='Recall', color='#e67e22', linewidth=2.5, marker='v', markersize=4)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Score', fontsize=10)\n","        ax.set_title('Precision & Recall Progression', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim(0, 1)\n","\n","        # 5. Learning Rate Schedule\n","        ax = axes[2, 0]\n","        if 'lr/pg0' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg0'],\n","                   label='LR Group 0', color='#34495e', linewidth=2)\n","        if 'lr/pg1' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg1'],\n","                   label='LR Group 1', color='#7f8c8d', linewidth=2)\n","        if 'lr/pg2' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg2'],\n","                   label='LR Group 2', color='#95a5a6', linewidth=2)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Learning Rate', fontsize=10)\n","        ax.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 6. Combined Loss (Train vs Val)\n","        ax = axes[2, 1]\n","        # Calculate total train loss if components available\n","        train_loss_cols = [col for col in training_results.columns if 'train/' in col and 'loss' in col]\n","        val_loss_cols = [col for col in training_results.columns if 'val/' in col and 'loss' in col]\n","\n","        if train_loss_cols:\n","            train_total = training_results[train_loss_cols].sum(axis=1)\n","            ax.plot(training_results['epoch'], train_total,\n","                   label='Total Train Loss', color='#c0392b', linewidth=2.5)\n","        if val_loss_cols:\n","            val_total = training_results[val_loss_cols].sum(axis=1)\n","            ax.plot(training_results['epoch'], val_total,\n","                   label='Total Val Loss', color='#2980b9', linewidth=2.5, linestyle='--')\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Total Loss', fontsize=10)\n","        ax.set_title('Total Loss: Train vs Validation', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","\n","        # Save training curves\n","        training_curves_img = TRAIN_DIR / 'report_training_curves.png'\n","        plt.savefig(training_curves_img, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # Add to PDF\n","        story.append(Image(str(training_curves_img), width=6.5*inch, height=7.5*inch))\n","        story.append(Spacer(1, 12))\n","\n","        # Epoch-by-Epoch Summary Table (First 10, Middle 5, Last 10)\n","        story.append(PageBreak())\n","        story.append(Paragraph('Detailed Epoch Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # Select representative epochs\n","        total_epochs = len(training_results)\n","        if total_epochs <= 25:\n","            selected_epochs = training_results\n","        else:\n","            # First 10, middle 5, last 10\n","            first_10 = training_results.head(10)\n","            middle_start = total_epochs // 2 - 2\n","            middle_5 = training_results.iloc[middle_start:middle_start+5]\n","            last_10 = training_results.tail(10)\n","            selected_epochs = pd.concat([first_10, middle_5, last_10])\n","\n","        # Build table with key metrics\n","        epoch_table_data = [['Epoch', 'Train Loss', 'Val Loss', 'mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']]\n","\n","        for _, row in selected_epochs.iterrows():\n","            epoch_num = int(row['epoch']) if 'epoch' in row else '?'\n","\n","            # Calculate total losses\n","            train_loss = sum([row.get(col, 0) for col in train_loss_cols]) if train_loss_cols else 'N/A'\n","            val_loss = sum([row.get(col, 0) for col in val_loss_cols]) if val_loss_cols else 'N/A'\n","\n","            map50 = f\"{row.get('metrics/mAP50(B)', 0):.4f}\" if 'metrics/mAP50(B)' in row else 'N/A'\n","            map50_95 = f\"{row.get('metrics/mAP50-95(B)', 0):.4f}\" if 'metrics/mAP50-95(B)' in row else 'N/A'\n","            precision = f\"{row.get('metrics/precision(B)', 0):.4f}\" if 'metrics/precision(B)' in row else 'N/A'\n","            recall = f\"{row.get('metrics/recall(B)', 0):.4f}\" if 'metrics/recall(B)' in row else 'N/A'\n","\n","            epoch_table_data.append([\n","                str(epoch_num),\n","                f\"{train_loss:.4f}\" if isinstance(train_loss, (int, float)) else train_loss,\n","                f\"{val_loss:.4f}\" if isinstance(val_loss, (int, float)) else val_loss,\n","                map50,\n","                map50_95,\n","                precision,\n","                recall\n","            ])\n","\n","        epoch_table = Table(epoch_table_data, colWidths=[0.6*inch, 1*inch, 1*inch, 0.9*inch, 1.1*inch, 0.9*inch, 0.9*inch])\n","        epoch_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#8e44ad')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('FONTSIZE', (0,0), (-1,-1), 8),\n","            ('GRID', (0,0), (-1,-1), 0.5, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(epoch_table)\n","        story.append(Spacer(1, 12))\n","\n","        # Training Summary Statistics\n","        story.append(Paragraph('Training Statistics Summary', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        stats_data = [['Metric', 'Initial', 'Final', 'Best', 'Change']]\n","\n","        # mAP@0.5\n","        if 'metrics/mAP50(B)' in training_results.columns:\n","            map50_col = training_results['metrics/mAP50(B)']\n","            stats_data.append([\n","                'mAP@0.5',\n","                f\"{map50_col.iloc[0]:.4f}\",\n","                f\"{map50_col.iloc[-1]:.4f}\",\n","                f\"{map50_col.max():.4f}\",\n","                f\"+{map50_col.iloc[-1] - map50_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # mAP@0.5:0.95\n","        if 'metrics/mAP50-95(B)' in training_results.columns:\n","            map50_95_col = training_results['metrics/mAP50-95(B)']\n","            stats_data.append([\n","                'mAP@0.5:0.95',\n","                f\"{map50_95_col.iloc[0]:.4f}\",\n","                f\"{map50_95_col.iloc[-1]:.4f}\",\n","                f\"{map50_95_col.max():.4f}\",\n","                f\"+{map50_95_col.iloc[-1] - map50_95_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # Precision\n","        if 'metrics/precision(B)' in training_results.columns:\n","            prec_col = training_results['metrics/precision(B)']\n","            stats_data.append([\n","                'Precision',\n","                f\"{prec_col.iloc[0]:.4f}\",\n","                f\"{prec_col.iloc[-1]:.4f}\",\n","                f\"{prec_col.max():.4f}\",\n","                f\"+{prec_col.iloc[-1] - prec_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # Recall\n","        if 'metrics/recall(B)' in training_results.columns:\n","            recall_col = training_results['metrics/recall(B)']\n","            stats_data.append([\n","                'Recall',\n","                f\"{recall_col.iloc[0]:.4f}\",\n","                f\"{recall_col.iloc[-1]:.4f}\",\n","                f\"{recall_col.max():.4f}\",\n","                f\"+{recall_col.iloc[-1] - recall_col.iloc[0]:.4f}\"\n","            ])\n","\n","        stats_table = Table(stats_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1*inch, 1*inch])\n","        stats_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#2ecc71')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (1,1), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(stats_table)\n","        story.append(Spacer(1, 12))\n","\n","    except Exception as e:\n","        story.append(Paragraph(f'Could not load detailed training results: {str(e)}', normal_style))\n","        story.append(Spacer(1, 12))\n","else:\n","    story.append(Paragraph('Training results file (results.csv) not found. Train the model to generate detailed metrics.', normal_style))\n","    story.append(Spacer(1, 12))\n","\n","# --- Final Model Performance ---\n","if 'final_metrics' in globals():\n","    story.append(PageBreak())\n","    story.append(Paragraph('Final Model Performance', heading_style))\n","\n","    perf_data = [\n","        ['Metric', 'Value'],\n","        ['mAP@0.5', f\"{final_metrics['map50']:.4f}\"],\n","        ['mAP@0.5:0.95', f\"{final_metrics['map50_95']:.4f}\"],\n","        ['Precision', f\"{final_metrics['precision']:.4f}\"],\n","        ['Recall', f\"{final_metrics['recall']:.4f}\"],\n","    ]\n","    perf_table = Table(perf_data, colWidths=[3*inch, 3*inch])\n","    perf_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#27ae60')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ]))\n","    story.append(perf_table)\n","    story.append(Spacer(1, 12))\n","\n","# --- Test Set Validation Results ---\n","if 'result' in globals() and 'metrics' in result:\n","    story.append(PageBreak())\n","    story.append(Paragraph('Test Set Validation Results', heading_style))\n","    story.append(Spacer(1, 6))\n","\n","    # Test metrics summary\n","    test_metrics = result['metrics']\n","    test_overall = test_metrics['overall']\n","    test_yolo = test_metrics['yolo_metrics']\n","    test_model_info = result['model_info']\n","\n","    # Model Architecture and Performance Summary\n","    story.append(Paragraph('Model Architecture & Performance', styles['Heading3']))\n","    model_arch_data = [\n","        ['Metric', 'Value'],\n","        ['Model Name', finetuned_model_name],\n","        ['Parameters (M)', f\"{test_model_info.get('params', 0) / 1e6:.2f}\"],\n","        ['Model Size (MB)', f\"{test_model_info.get('size(MB)', 0):.2f}\"],\n","        ['FLOPs (G)', f\"{test_model_info.get('FLOPs(G)', 0):.2f}\"],\n","        ['Layers', str(test_model_info.get('layers', 'N/A'))],\n","        ['Inference Speed (FPS)', f\"{test_metrics['fps']:.2f}\"],\n","        ['IoU Threshold', f\"{IOU_THRESHOLDS:.2f}\"],\n","    ]\n","    model_arch_table = Table(model_arch_data, colWidths=[2.5*inch, 3.5*inch])\n","    model_arch_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#34495e')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","        ('ALIGN', (1,1), (-1,-1), 'CENTER'),\n","    ]))\n","    story.append(model_arch_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Overall Performance Metrics\n","    story.append(Paragraph('Overall Performance Metrics on Test Set', styles['Heading3']))\n","    test_perf_data = [\n","        ['Metric', 'Confusion Matrix', 'YOLO Validation'],\n","        ['Precision', f\"{test_overall['precision']:.4f}\", f\"{test_yolo['precision']:.4f}\"],\n","        ['Recall', f\"{test_overall['recall']:.4f}\", f\"{test_yolo['recall']:.4f}\"],\n","        ['F1-Score', f\"{test_overall['f1']:.4f}\", 'N/A'],\n","        ['mAP@0.5 (Overall)', 'N/A', f\"{test_yolo['map50']:.4f}\"],\n","        ['mAP@0.5:0.95 (Overall)', 'N/A', f\"{test_yolo['map50_95']:.4f}\"],\n","    ]\n","    test_perf_table = Table(test_perf_data, colWidths=[2*inch, 2*inch, 2*inch])\n","    test_perf_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#e74c3c')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","        ('ALIGN', (1,0), (-1,-1), 'CENTER'),\n","    ]))\n","    story.append(test_perf_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Per-Class mAP@0.5 and Performance\n","    if 'df_metrics' in result and not result['df_metrics'].empty:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Per-Class Performance Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        df_metrics = result['df_metrics']\n","\n","        # Per-class table with all metrics\n","        per_class_data = [['Class', 'Precision', 'Recall', 'F1-Score', 'mAP@0.5', 'TP', 'FP', 'FN']]\n","        for _, row in df_metrics.iterrows():\n","            per_class_data.append([\n","                str(row['Class']),\n","                f\"{row['Precision']:.4f}\",\n","                f\"{row['Recall']:.4f}\",\n","                f\"{row['F1-Score']:.4f}\",\n","                f\"{row['mAP@0.5']:.4f}\",\n","                str(int(row['TP'])),\n","                str(int(row['FP'])),\n","                str(int(row['FN']))\n","            ])\n","\n","        per_class_table = Table(per_class_data, colWidths=[1.2*inch, 0.8*inch, 0.7*inch, 0.8*inch, 0.8*inch, 0.5*inch, 0.5*inch, 0.5*inch])\n","        per_class_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#9b59b6')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('FONTSIZE', (0,0), (-1,-1), 8),\n","            ('GRID', (0,0), (-1,-1), 0.5, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (1,0), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(per_class_table)\n","        story.append(Spacer(1, 12))\n","\n","        # mAP@0.5 by Class visualization\n","        map50_by_class_img = result['figures'].get('map50_by_class')\n","        if map50_by_class_img and Path(map50_by_class_img).exists():\n","            try:\n","                story.append(Paragraph('mAP@0.5 Distribution by Class', styles['Heading4']))\n","                story.append(Spacer(1, 4))\n","                story.append(Image(str(map50_by_class_img), width=6.5*inch, height=4.5*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load mAP by class chart: {str(img_error)}', normal_style))\n","\n","    # IoU Information\n","    story.append(PageBreak())\n","    story.append(Paragraph('Intersection over Union (IoU) Analysis', styles['Heading3']))\n","    story.append(Spacer(1, 6))\n","\n","    iou_info_text = f\"\"\"\n","    <b>IoU Threshold Used:</b> {IOU_THRESHOLDS:.2f}<br/>\n","    <br/>\n","    IoU (Intersection over Union) measures the overlap between predicted and ground truth bounding boxes.\n","    A prediction is considered correct (True Positive) when IoU â‰¥ {IOU_THRESHOLDS:.2f}.<br/>\n","    <br/>\n","    <b>Per-Class IoU Performance:</b><br/>\n","    The confusion matrix and per-class metrics above show detection accuracy at IoU={IOU_THRESHOLDS:.2f} threshold.\n","    Each class's True Positives (TP) represent detections with IoU â‰¥ {IOU_THRESHOLDS:.2f}.\n","    \"\"\"\n","    story.append(Paragraph(iou_info_text, normal_style))\n","    story.append(Spacer(1, 12))\n","\n","    # Confusion Matrix\n","    story.append(PageBreak())\n","    story.append(Paragraph('Confusion Matrix (Test Set)', styles['Heading3']))\n","    story.append(Spacer(1, 6))\n","\n","    confusion_matrix_img = result['figures'].get('confusion_matrix')\n","    if confusion_matrix_img and Path(confusion_matrix_img).exists():\n","        try:\n","            with PILImage.open(confusion_matrix_img) as img:\n","                img_width, img_height = img.size\n","                aspect_ratio = img_height / img_width\n","                pdf_width = 6*inch\n","                pdf_height = pdf_width * aspect_ratio\n","                if pdf_height > 6*inch:\n","                    pdf_height = 6*inch\n","                    pdf_width = pdf_height / aspect_ratio\n","                story.append(Image(str(confusion_matrix_img), width=pdf_width, height=pdf_height))\n","                story.append(Spacer(1, 12))\n","        except Exception as img_error:\n","            story.append(Paragraph(f'Could not load confusion matrix: {str(img_error)}', normal_style))\n","    else:\n","        story.append(Paragraph('Confusion matrix image not available.', normal_style))\n","    story.append(Spacer(1, 12))\n","\n","    # Test Performance Curves - Only add section if curves exist\n","    pr_curve_img = result['figures'].get('pr_curve')\n","    f1_curve_img = result['figures'].get('f1_curve')\n","    overall_metrics_img = result['figures'].get('overall_metrics')\n","\n","    has_curves = (\n","        (pr_curve_img and Path(pr_curve_img).exists()) or\n","        (f1_curve_img and Path(f1_curve_img).exists()) or\n","        (overall_metrics_img and Path(overall_metrics_img).exists())\n","    )\n","\n","    if has_curves:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Test Set Performance Curves', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # PR Curve\n","        if pr_curve_img and Path(pr_curve_img).exists():\n","            try:\n","                story.append(Paragraph('Precision-Recall Curve', styles['Heading4']))\n","                story.append(Image(str(pr_curve_img), width=6*inch, height=4*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load PR curve: {str(img_error)}', normal_style))\n","\n","        # F1 Curve\n","        if f1_curve_img and Path(f1_curve_img).exists():\n","            try:\n","                story.append(Paragraph('F1-Score Curve', styles['Heading4']))\n","                story.append(Image(str(f1_curve_img), width=6*inch, height=4*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load F1 curve: {str(img_error)}', normal_style))\n","\n","        # Overall Metrics\n","        if overall_metrics_img and Path(overall_metrics_img).exists():\n","            try:\n","                story.append(Paragraph('Overall Metrics Visualization', styles['Heading4']))\n","                story.append(Image(str(overall_metrics_img), width=6.5*inch, height=5*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load overall metrics: {str(img_error)}', normal_style))\n","\n","    # Sample Comparison Images\n","    if 'comparison_data' in result and result['comparison_data']:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Sample Predictions: Ground Truth vs Model Output', heading_style))\n","        story.append(Spacer(1, 6))\n","\n","        # Add up to 6 comparison images\n","        for idx, comp in enumerate(result['comparison_data'][:6], 1):\n","            comp_img_path = comp.get('comparison_image_path')\n","            if comp_img_path and Path(comp_img_path).exists():\n","                try:\n","                    # Add attributes info\n","                    attributes = comp.get('attributes', {})\n","                    attr_text = f\"Sample {idx} - Weather: {attributes.get('weather', 'unknown')}, Scene: {attributes.get('scene', 'unknown')}, Time: {attributes.get('timeofday', 'unknown')}\"\n","                    story.append(Paragraph(attr_text, normal_style))\n","                    story.append(Spacer(1, 4))\n","\n","                    # Add comparison image\n","                    with PILImage.open(comp_img_path) as img:\n","                        img_width, img_height = img.size\n","                        aspect_ratio = img_height / img_width\n","                        pdf_width = 6.5*inch\n","                        pdf_height = pdf_width * aspect_ratio\n","                        if pdf_height > 4*inch:\n","                            pdf_height = 4*inch\n","                            pdf_width = pdf_height / aspect_ratio\n","                        story.append(Image(str(comp_img_path), width=pdf_width, height=pdf_height))\n","\n","                    # Add object count info\n","                    gt_count = comp.get('gt_count', 0)\n","                    pred_count = comp.get('pred_count', 0)\n","                    count_text = f\"Ground Truth: {gt_count} objects | Predictions: {pred_count} objects\"\n","                    story.append(Paragraph(count_text, ParagraphStyle('Small', parent=normal_style, fontSize=8, textColor=rl_colors.grey)))\n","                    story.append(Spacer(1, 15))\n","\n","                    # Page break after every 2 comparisons\n","                    if idx % 2 == 0 and idx < len(result['comparison_data'][:6]):\n","                        story.append(PageBreak())\n","\n","                except Exception as img_error:\n","                    story.append(Paragraph(f'Could not load comparison {idx}: {str(img_error)}', normal_style))\n","                    story.append(Spacer(1, 12))\n","\n","elif 'results_summary' in globals() and len(results_summary) > 0 and results_summary[0].get('status') == 'ok':\n","    # Fallback: Show basic info from results_summary\n","    story.append(PageBreak())\n","    story.append(Paragraph('Test Set Validation Results', heading_style))\n","\n","    res = results_summary[0]\n","    fallback_data = [\n","        ['Metric', 'Value'],\n","        ['Model', res.get('model_name', 'N/A')],\n","        ['Precision (YOLO)', f\"{res.get('precision_yolo', 0):.4f}\"],\n","        ['Recall (YOLO)', f\"{res.get('recall_yolo', 0):.4f}\"],\n","        ['mAP@0.5', f\"{res.get('map50', 0):.4f}\"],\n","        ['mAP@0.5:0.95', f\"{res.get('map50_95', 0):.4f}\"],\n","        ['Parameters (M)', f\"{res.get('params_m', 0):.2f}\"],\n","        ['Size (MB)', f\"{res.get('size_mb', 0):.2f}\"],\n","        ['FPS', f\"{res.get('fps', 0):.2f}\"],\n","    ]\n","    fallback_table = Table(fallback_data, colWidths=[3*inch, 3*inch])\n","    fallback_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#95a5a6')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ]))\n","    story.append(fallback_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Try to load images from run_dir if available\n","    if 'run_dir' in res:\n","        run_dir = Path(res['run_dir'])\n","\n","        # Try confusion matrix\n","        confusion_img = run_dir / 'confusion_matrix.png'\n","        if confusion_img.exists():\n","            try:\n","                story.append(PageBreak())\n","                story.append(Paragraph('Confusion Matrix', styles['Heading3']))\n","                story.append(Image(str(confusion_img), width=6*inch, height=5*inch))\n","                story.append(Spacer(1, 12))\n","            except:\n","                pass\n","\n","        # Try comparison images\n","        comparisons_dir = run_dir / 'sample_comparisons'\n","        if comparisons_dir.exists():\n","            comparison_imgs = sorted(comparisons_dir.glob('comparison_*.png'))[:4]\n","            if comparison_imgs:\n","                story.append(PageBreak())\n","                story.append(Paragraph('Sample Predictions', styles['Heading3']))\n","                for comp_img in comparison_imgs:\n","                    try:\n","                        story.append(Image(str(comp_img), width=6.5*inch, height=3.5*inch))\n","                        story.append(Spacer(1, 10))\n","                    except:\n","                        pass\n","\n","# --- Footer ---\n","story.append(Spacer(1, 20))\n","story.append(Paragraph('Generated by YOLO Training Notebook', ParagraphStyle('Footer', parent=styles['Normal'], alignment=TA_CENTER, textColor=rl_colors.grey)))\n","story.append(Paragraph('BDD100K Dataset - Computer Vision Project', ParagraphStyle('Footer2', parent=styles['Normal'], alignment=TA_CENTER, textColor=rl_colors.grey)))\n","\n","# Build PDF\n","try:\n","    doc.build(story)\n","    print(f'\\nâœ“ Comprehensive Training PDF generated: {pdf_training_report_path}')\n","except Exception as e:\n","    print(f'\\nâŒ Error generating PDF: {e}')\n","    import traceback\n","    traceback.print_exc()\n"]},{"cell_type":"markdown","id":"7d09278e","metadata":{"id":"7d09278e"},"source":["## 11. Final Summary"]},{"cell_type":"code","execution_count":22,"id":"9e9576f4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e9576f4","executionInfo":{"status":"ok","timestamp":1764215572485,"user_tz":-180,"elapsed":16,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"6e64036a-32ce-407f-992a-c00fcce27d43"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","================================================================================\n","FINAL TRAINING COMPLETE!\n","================================================================================\n","\n","ğŸ“Š Project: yolov8m on bdd100k_yolo_limited\n","ğŸ“… Date: 2025-11-27 03:52:52\n","\n","ğŸ”¬ Tuning Run Used:\n","  Run Name: None\n","  Run Path: None\n","\n","ğŸ¯ Training Run:\n","  Run Name: yolov8m_train_20251126_231411\n","  Run Path: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\n","  Epochs: 100\n","  Batch Size: 96\n","\n","ğŸ¯ Final Model Performance:\n","  mAP@0.5: 0.5564\n","  mAP@0.5:0.95: 0.3204\n","  Precision: 0.6335\n","  Recall: 0.5234\n","\n","ğŸ“ Generated Files:\n","\n","  ğŸ¯ Training Results (in yolov8m_train_20251126_231411):\n","    - training_log.json\n","    - weights/best.pt\n","    - weights/last.pt\n","    - results.csv\n","  ğŸ“„ Training PDF Report:\n","    - yolov8m_training_report.pdf\n","\n","  ğŸ¯ Final Model Package:\n","    - yolov8m_finetuned_20251127.pt\n","    - yolov8m_finetuned_20251127_metadata.json\n","    Location: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127\n","\n","ğŸ“‚ All results saved to:\n","  Tuning: None\n","  Training: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\n","  Final Model: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127\n","\n","ğŸš€ Next Steps:\n","  1. Review training PDF report: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411/yolov8m_training_report.pdf\n","  2. Review training plots and metrics in: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/tune_train/training/yolov8m_train_20251126_231411\n","  3. Use final model for inference: /content/Drive/MyDrive/ksu_yolo_2025/computer_vision_yolo/models/yolov8m_finetuned_20251127/yolov8m_finetuned_20251127.pt\n","  4. Evaluate on test set using yolo_test scripts\n","  5. Consider fine-tuning with different datasets or model sizes\n","\n","ğŸ“ To Resume Training:\n","  Set RESUME_TRAINING_RUN_NAME = \"yolov8m_train_20251126_231411\"\n","  Then re-run this notebook\n","\n","================================================================================\n","SUCCESS! âœ“\n","================================================================================\n"]}],"source":["# FINAL SUMMARY\n","# ============================================================================\n","\n","print('\\n\\n')\n","print('=' * 80)\n","print('FINAL TRAINING COMPLETE!')\n","print('=' * 80)\n","\n","print(f'\\nğŸ“Š Project: {MODEL_NAME} on {YOLO_DATASET_ROOT.name}')\n","print(f'ğŸ“… Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","\n","# Tuning Summary\n","print(f'\\nğŸ”¬ Tuning Run Used:')\n","print(f'  Run Name: {TUNING_RUN_NAME}')\n","print(f'  Run Path: {TUNE_DIR}')\n","\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    print(f'  Total Trials: {tuning_metadata.get(\"total_trials\", \"N/A\")}')\n","    print(f'  Completed Trials: {tuning_metadata.get(\"completed_trials\", \"N/A\")}')\n","    print(f'  Best Trial: {tuning_metadata.get(\"best_trial\", \"N/A\")}')\n","    print(f'  Best Trial mAP@0.5: {tuning_metadata.get(\"best_map50\", 0):.4f}')\n","    if 'optimization_duration' in tuning_metadata:\n","        print(f'  Tuning Duration: {tuning_metadata[\"optimization_duration\"]}')\n","\n","# Training Summary\n","print(f'\\nğŸ¯ Training Run:')\n","print(f'  Run Name: {RUN_NAME_TRAINING}')\n","print(f'  Run Path: {TRAIN_DIR}')\n","print(f'  Epochs: {EPOCHS_FINAL_TRAINING}')\n","print(f'  Batch Size: {BATCH_SIZE}')\n","\n","if 'final_metrics' in globals():\n","    print(f'\\nğŸ¯ Final Model Performance:')\n","    print(f'  mAP@0.5: {final_metrics[\"map50\"]:.4f}')\n","    print(f'  mAP@0.5:0.95: {final_metrics[\"map50_95\"]:.4f}')\n","    print(f'  Precision: {final_metrics[\"precision\"]:.4f}')\n","    print(f'  Recall: {final_metrics[\"recall\"]:.4f}')\n","\n","    # Show improvement if available\n","    if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","        tuning_best_map = tuning_metadata.get('best_map50', 0)\n","        if tuning_best_map > 0:\n","            improvement = final_metrics['map50'] - tuning_best_map\n","            print(f'\\nğŸ“ˆ Improvement vs Tuning:')\n","            print(f'  Tuning Best: {tuning_best_map:.4f}')\n","            print(f'  Training Final: {final_metrics[\"map50\"]:.4f}')\n","            print(f'  Improvement: {improvement:+.4f} ({improvement/tuning_best_map*100:+.2f}%)')\n","\n","print(f'\\nğŸ“ Generated Files:')\n","if not USE_DEFAULT_CONFIG:\n","    print(f'\\n  ğŸ“Š Tuning Results (in {TUNE_DIR.name}):')\n","    print(f'    - best_hyperparameters.json')\n","    print(f'    - best_hparams.yaml')\n","    print(f'    - checkpoint_log.json')\n","    print(f'    - optuna_study.pkl')\n","\n","print(f'\\n  ğŸ¯ Training Results (in {TRAIN_DIR.name}):')\n","print(f'    - training_log.json')\n","print(f'    - weights/best.pt')\n","print(f'    - weights/last.pt')\n","print(f'    - results.csv')\n","print(f'  ğŸ“„ Training PDF Report:')\n","print(f'    - {MODEL_NAME}_training_report.pdf')\n","\n","if 'final_model_path' in globals():\n","    print(f'\\n  ğŸ¯ Final Model Package:')\n","    print(f'    - {final_model_path.name}')\n","    print(f'    - {metadata_path.name}')\n","    print(f'    Location: {model_save_dir}')\n","\n","print(f'\\nğŸ“‚ All results saved to:')\n","print(f'  Tuning: {TUNE_DIR}')\n","print(f'  Training: {TRAIN_DIR}')\n","if 'model_save_dir' in globals():\n","    print(f'  Final Model: {model_save_dir}')\n","\n","print(f'\\nğŸš€ Next Steps:')\n","print(f'  1. Review training PDF report: {TRAIN_DIR / f\"{MODEL_NAME}_training_report.pdf\"}')\n","print(f'  2. Review training plots and metrics in: {TRAIN_DIR}')\n","if 'final_model_path' in globals():\n","    print(f'  3. Use final model for inference: {final_model_path}')\n","    print(f'  4. Evaluate on test set using yolo_test scripts')\n","else:\n","    print(f'  3. Complete training to generate final model')\n","print(f'  5. Consider fine-tuning with different datasets or model sizes')\n","\n","print('\\nğŸ“ To Resume Training:')\n","print(f'  Set RESUME_TRAINING_RUN_NAME = \"{RUN_NAME_TRAINING}\"')\n","print(f'  Then re-run this notebook')\n","\n","print('\\n' + '=' * 80)\n","print('SUCCESS! âœ“')\n","print('=' * 80)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}