{"cells":[{"cell_type":"markdown","id":"8b26f5d6","metadata":{"id":"8b26f5d6"},"source":["# YOLO Training\n","- Support for YOLOv8, YOLOv9, YOLOv10, YOLO11, YOLO12\n"]},{"cell_type":"markdown","id":"a7e53f76","metadata":{"id":"a7e53f76"},"source":["# 1. Set Directories"]},{"cell_type":"code","execution_count":1,"id":"c698714c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c698714c","executionInfo":{"status":"ok","timestamp":1764367838507,"user_tz":-180,"elapsed":4036,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"016a35b2-f4d8-4878-86ce-eeb44f03e536"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Drive\n","‚úì W&B API key loaded from Colab secrets\n"]}],"source":["# Base directories\n","# Detect environment: Colab or local\n","\n","import os\n","from pathlib import Path\n","\n","\n","IS_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n","\n","USE_WANDB = True  # Set to False to disable W&B logging\n","\n","\n","\n","if IS_COLAB:\n","    #Mount Google Drive if not already mounted\n","    from google.colab import drive\n","    drive.mount('/content/Drive', force_remount=True)\n","    # Running in Google Colab\n","    BASE_DIR = Path('/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo')\n","\n","    # Configure W&B API key\n","    if USE_WANDB:\n","        # In Colab, get API key from secrets\n","        from google.colab import userdata\n","        wandb_api_key = userdata.get('wandb_api_key')\n","        os.environ['WANDB_API_KEY'] = wandb_api_key\n","        print('‚úì W&B API key loaded from Colab secrets')\n","\n","    DATASET_BASE_DIR = Path('/computer_vision_yolo')\n","\n","else:\n","    # Running locally\n","    BASE_DIR = Path.cwd().parent\n","    if USE_WANDB:\n","        print('‚úì Running locally - W&B will use existing login or prompt')\n","\n","    DATASET_BASE_DIR = Path.cwd().parent\n"]},{"cell_type":"code","execution_count":2,"id":"c8064a27","metadata":{"id":"c8064a27","executionInfo":{"status":"ok","timestamp":1764367838510,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}}},"outputs":[],"source":["#  ! cd /content/Drive/MyDrive/ksu_yolo_2025 && git clone https://github.com/m3mahdy/computer_vision_yolo"]},{"cell_type":"code","execution_count":3,"id":"16c67ad5","metadata":{"id":"16c67ad5","executionInfo":{"status":"ok","timestamp":1764367838524,"user_tz":-180,"elapsed":10,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}}},"outputs":[],"source":["# ! cd {BASE_DIR} && pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":4,"id":"43f33666","metadata":{"id":"43f33666","executionInfo":{"status":"ok","timestamp":1764367838526,"user_tz":-180,"elapsed":1,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}}},"outputs":[],"source":["# download limited dataset\n","# !mkdir {DATASET_BASE_DIR}\n","# !cd {BASE_DIR}/dataset && cp 8_download_extract_other_datasets.py {DATASET_BASE_DIR} && cd {DATASET_BASE_DIR} && python 8_download_extract_other_datasets.py\n"]},{"cell_type":"code","execution_count":4,"id":"a7ddb177","metadata":{"id":"a7ddb177","executionInfo":{"status":"ok","timestamp":1764367838529,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"109394c4","metadata":{"id":"109394c4"},"source":["## 2. Import Required Libraries"]},{"cell_type":"code","execution_count":5,"id":"fc6f98f1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc6f98f1","executionInfo":{"status":"ok","timestamp":1764367842411,"user_tz":-180,"elapsed":3875,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"a5e53870-1a08-496f-8d68-a11911fe9393"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Libraries imported successfully\n","‚úì Device: cuda\n","  GPU: NVIDIA A100-SXM4-40GB\n","  CUDA Version: 12.6\n","  Available Memory: 42.47 GB\n"]}],"source":["# Install required libraries (uncomment if running in Colab)\n","# !pip install -q ultralytics wandb pyyaml\n","\n","import os\n","import sys\n","import gc\n","import yaml\n","import json\n","import torch\n","import shutil\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","from datetime import datetime\n","from tqdm import tqdm\n","import pickle\n","import platform\n","import psutil\n","\n","import wandb\n","\n","# YOLO imports\n","from ultralytics import YOLO\n","\n","# ReportLab imports for PDF generation\n","from reportlab.lib.pagesizes import A4\n","from reportlab.lib import colors as rl_colors\n","from reportlab.lib.units import inch\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.enums import TA_CENTER, TA_LEFT\n","from PIL import Image as PILImage\n","\n","warnings.filterwarnings('ignore')\n","\n","# Configure matplotlib for notebook display\n","%matplotlib inline\n","sns.set_style('whitegrid')\n","plt.rcParams['figure.figsize'] = (15, 10)\n","\n","# Check GPU availability\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'‚úì Libraries imported successfully')\n","print(f'‚úì Device: {device}')\n","if device == 'cuda':\n","    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n","    print(f'  CUDA Version: {torch.version.cuda}')\n","    print(f'  Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n"]},{"cell_type":"markdown","id":"2659c792","metadata":{"id":"2659c792"},"source":["## 3. Configuration"]},{"cell_type":"code","execution_count":6,"id":"d163f0be","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d163f0be","executionInfo":{"status":"ok","timestamp":1764367846728,"user_tz":-180,"elapsed":4307,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"eb0af949-56af-4d7b-e46e-f7cc6f3bdb83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Drive\n","‚úì W&B API key loaded from Colab secrets\n","\n","‚öôÔ∏è  CONFIGURATION MODE: Using Tuned Hyperparameters\n","   üìÇ Using specified tuning run: yolov8m_finetuned_1_tune_20251127_230340\n","   ‚úì Found best hyperparameters: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/tune/yolov8m_finetuned_1_tune_20251127_230340/best_hyperparameters.json\n","\n","üÜï NEW TRAINING MODE: Creating new run \"yolov8m_finetuned_1_train_20251128_221046\"\n","================================================================================\n","CONFIGURATION SUMMARY\n","================================================================================\n","Environment: Google Colab\n","Base Directory: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo\n","Model: yolov8m_finetuned_1\n","Dataset: bdd100k_yolo_limited\n","Data YAML: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tmp/yolov8m_finetuned_1/data.yaml\n","  Dataset path in YAML: /computer_vision_yolo/bdd100k_yolo_limited\n","Classes: 10\n","Class Names: {0: 'person', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motor', 7: 'bike', 8: 'traffic light', 9: 'traffic sign'}\n","Device: cuda\n","Epochs Final Training: 100\n","Batch Size: 64\n","Configuration Mode: Tuned Hyperparameters\n","Tuning Run: yolov8m_finetuned_1_tune_20251127_230340\n","Training Directory: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\n","W&B Logging: Enabled\n","  Training Project: yolo-bdd100k_yolo_limited-training\n","================================================================================\n"]}],"source":["# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Base directories\n","# Detect environment: Colab or local\n","\n","IS_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n","\n","USE_WANDB = True  # Set to False to disable W&B logging\n","\n","if IS_COLAB:\n","    #Mount Google Drive if not already mounted\n","    from google.colab import drive\n","    drive.mount('/content/Drive', force_remount=True)\n","    # Running in Google Colab\n","    BASE_DIR = Path('/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo')\n","\n","    # Configure W&B API key\n","    if USE_WANDB:\n","        # In Colab, get API key from secrets\n","        from google.colab import userdata\n","        wandb_api_key = userdata.get('wandb_api_key')\n","        os.environ['WANDB_API_KEY'] = wandb_api_key\n","        print('‚úì W&B API key loaded from Colab secrets')\n","\n","else:\n","    # Running locally\n","    BASE_DIR = Path.cwd().parent\n","    if USE_WANDB:\n","        print('‚úì Running locally - W&B will use existing login or prompt')\n","class DatasetSplit:\n","    \"\"\"Constants for dataset split names\"\"\"\n","    TRAIN = \"train\"\n","    VAL = \"val\"\n","    TEST = \"test\"\n","\n","class ModelConfig:\n","    \"\"\"Default model training configuration constants\"\"\"\n","    # Image processing\n","    DEFAULT_IMAGE_SIZE = 640  # Standard YOLO input size\n","\n","    # Training workers\n","    DEFAULT_WORKERS = 8  # Number of data loading workers\n","\n","    # Early stopping and checkpointing\n","    DEFAULT_PATIENCE = 10  # Epochs to wait before early stopping\n","    DEFAULT_SAVE_PERIOD = 3  # Save checkpoint every N epochs\n","\n","    # Augmentation timing\n","    CLOSE_MOSAIC_EPOCHS = 10  # Disable mosaic augmentation in last N epochs\n","\n","    # Data loading and caching\n","    DEFAULT_CACHE = False  # Cache images for faster training (use True for small datasets)\n","    DEFAULT_VAL = True  # Run validation during training\n","\n","    # Warmup configuration\n","    # MIN_WARMUP_EPOCHS = 0\n","    # MAX_WARMUP_EPOCHS = 3\n","    # MIN_WARMUP_MOMENTUM = 0.5\n","    # MAX_WARMUP_MOMENTUM = 0.95\n","    # MIN_WARMUP_BIAS_LR = 0.0\n","\n","    # MAX_WARMUP_BIAS_LR = 0.1\n","\n","\n","\n","\n","# Model Selection - Choose one of the following:\n","MODEL_NAME = \"yolov8m_finetuned_1\"\n","\n","#yolov10n is for testing purpose only\n","#Mahdy will work yolov8m\n","\n","\n","# Selected models, to choose from, based on the performance and size:\n","# YOLOv8:  'yolov8s', 'yolov8m'\n","\n","# YOLOv10: 'yolov10s', 'yolov10m'\n","\n","# YOLO12: 'yolo12s'\n","\n","# Directory structure\n","MODELS_DIR = BASE_DIR / 'models' / MODEL_NAME\n","TMP_DIR = BASE_DIR / 'tmp' / MODEL_NAME\n","\n","# Dataset Selection\n","# Option 1: Full dataset (~100k images) - for final optimization: \"bdd100k_yolo\"\n","# Option 2: Limited dataset (representative samples) - for quick tuning: \"bdd100k_yolo_limited\"\n","dataset_name = 'bdd100k_yolo_limited'\n","\n","\n","YOLO_DATASET_ROOT = DATASET_BASE_DIR / dataset_name\n","\n","# data.yaml path\n","DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n","\n","# Verify dataset exists\n","if not DATA_YAML_PATH.exists():\n","    raise FileNotFoundError(\n","        f\"Dataset not found: {DATA_YAML_PATH}\\n\"\n","        f\"Please prepare the dataset first using process_bdd100k_to_yolo_dataset.py\"\n","    )\n","\n","# Update data.yaml path field for Colab compatibility\n","with open(DATA_YAML_PATH, 'r') as yaml_file:\n","    data_config = yaml.safe_load(yaml_file)\n","\n","# Validate required keys in data.yaml\n","required_yaml_keys = ['nc', 'names', 'path']\n","missing_keys = [key for key in required_yaml_keys if key not in data_config]\n","if missing_keys:\n","    raise ValueError(f\"Missing required keys in data.yaml: {missing_keys}\")\n","\n","# Update the 'path' field to use BASE_DIR\n","data_config['path'] = str(YOLO_DATASET_ROOT)\n","\n","# Create a temporary data.yaml with corrected paths\n","temp_data_yaml = TMP_DIR / 'data.yaml'\n","TMP_DIR.mkdir(parents=True, exist_ok=True)\n","with open(temp_data_yaml, 'w') as yaml_output_file:\n","    yaml.dump(data_config, yaml_output_file, default_flow_style=False, sort_keys=False)\n","\n","# Use the temporary data.yaml for training\n","DATA_YAML_PATH = temp_data_yaml\n","\n","# Training Configuration\n","EPOCHS_FINAL_TRAINING = 100  # Training epochs for final model = 150\n","BATCH_SIZE = 64  # Batch size for training\n","# for T4 GPU:\n","# 64 for 10n, 1 epoch 30 min\n","# 32 for 8m, 1 epoch 45 min\n","\n","# for A100 GPU:\n","# 64 for 10m 1 epoch 11 min, 5 epochs completed in 0.797 hours.\n","# 96 for 8m , 1 epoch 10 min, 5 epochs completed in 0.866 hours.\n","\n","# NOTE: Image size (imgsz) is loaded from best hyperparameters if USE_DEFAULT_CONFIG=False\n","# Defaults to 640 if using default configuration (USE_DEFAULT_CONFIG=True)\n","\n","# Weights & Biases (optional)\n","USE_WANDB = True  # Set to True to enable W&B logging\n","WANDB_PROJECT_TRAINING = f\"yolo-{YOLO_DATASET_ROOT.name}-training\"\n","\n","# ============================================================================\n","# CONFIGURATION MODE: DEFAULT vs TUNED HYPERPARAMETERS\n","# ============================================================================\n","# Set to True to use default YOLO configuration (no hyperparameter tuning)\n","# Set to False to load hyperparameters from a tuning run\n","# ============================================================================\n","\n","USE_DEFAULT_CONFIG = False  # Set to True to skip tuning and use default YOLO config\n","\n","# ============================================================================\n","# TUNING RUN CONFIGURATION - SPECIFY WHICH TUNING RUN TO USE\n","# ============================================================================\n","# Specify the tuning run name to load best hyperparameters from\n","# This should match the directory name in tune_train/tune/\n","#\n","# Example: TUNING_RUN_NAME = \"yolov10n_tune_20251125_143022\"\n","# Leave as None to search for the latest tuning run for this model\n","# Note: Only used if USE_DEFAULT_CONFIG = False\n","# ============================================================================\n","\n","TUNING_RUN_NAME = \"yolov8m_finetuned_1_tune_20251127_230340\"  # Set to specific tuning run name, or None to auto-detect latest\n","\n","# ============================================================================\n","# TRAINING RUN CONFIGURATION - RESUME OR CREATE NEW\n","# ============================================================================\n","# To RESUME an existing training run: Set RESUME_TRAINING_RUN_NAME to the run directory name\n","# To START NEW training: Leave RESUME_TRAINING_RUN_NAME as None or empty string\n","#\n","# Example to resume: RESUME_TRAINING_RUN_NAME = \"yolov10n_train_20251125_150000\"\n","# ============================================================================\n","\n","RESUME_TRAINING_RUN_NAME = None  # Set to run name to resume, or None to create new run\n","\n","# Find or verify tuning run (only if not using default config)\n","TUNE_TRAIN_BASE = BASE_DIR / 'tune_train'\n","TUNE_BASE_DIR = TUNE_TRAIN_BASE / 'tune'\n","\n","if USE_DEFAULT_CONFIG:\n","    # Using default configuration - skip tuning run search\n","    print('\\n‚öôÔ∏è  CONFIGURATION MODE: Using Default YOLO Configuration')\n","    print('   No hyperparameter tuning will be applied')\n","    TUNE_DIR = None\n","    TUNING_RUN_NAME = None\n","    best_hyperparams_path = None\n","else:\n","    # Using tuned hyperparameters - find or verify tuning run\n","    print('\\n‚öôÔ∏è  CONFIGURATION MODE: Using Tuned Hyperparameters')\n","\n","    if TUNING_RUN_NAME:\n","        # Use specified tuning run\n","        TUNE_DIR = TUNE_BASE_DIR / TUNING_RUN_NAME\n","        if not TUNE_DIR.exists():\n","            raise FileNotFoundError(\n","                f\"Specified tuning run not found: {TUNE_DIR}\\n\"\n","                f\"Available runs in {TUNE_BASE_DIR}:\\n\" +\n","                '\\n'.join(f\"  - {d.name}\" for d in TUNE_BASE_DIR.glob(f'{MODEL_NAME}_tune_*') if d.is_dir())\n","            )\n","        print(f'   üìÇ Using specified tuning run: {TUNING_RUN_NAME}')\n","    else:\n","        # Auto-detect latest tuning run for this model\n","        tuning_runs = sorted(TUNE_BASE_DIR.glob(f'{MODEL_NAME}_tune_*'), key=lambda p: p.name, reverse=True)\n","        if not tuning_runs:\n","            raise FileNotFoundError(\n","                f\"No tuning runs found for model {MODEL_NAME} in {TUNE_BASE_DIR}\\n\"\n","                f\"Please run the tuning notebook first, specify TUNING_RUN_NAME, or set USE_DEFAULT_CONFIG=True\"\n","            )\n","        TUNE_DIR = tuning_runs[0]\n","        TUNING_RUN_NAME = TUNE_DIR.name\n","        print(f'   üîç Auto-detected latest tuning run: {TUNING_RUN_NAME}')\n","\n","    # Verify best hyperparameters exist\n","    best_hyperparams_path = TUNE_DIR / 'best_hyperparameters.json'\n","    if not best_hyperparams_path.exists():\n","        raise FileNotFoundError(\n","            f\"Best hyperparameters not found in tuning run: {best_hyperparams_path}\\n\"\n","            f\"Please ensure the tuning run completed successfully\"\n","        )\n","\n","    print(f'   ‚úì Found best hyperparameters: {best_hyperparams_path}')\n","\n","# Configure training run name\n","if RESUME_TRAINING_RUN_NAME:\n","    # Resume existing training run\n","    RUN_NAME_TRAINING = RESUME_TRAINING_RUN_NAME\n","    print(f'\\nüîÑ RESUME MODE: Will attempt to resume training run \"{RESUME_TRAINING_RUN_NAME}\"')\n","else:\n","    # Create new training run with timestamp\n","    RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    RUN_NAME_TRAINING = f'{MODEL_NAME}_train_{RUN_TIMESTAMP}'\n","    print(f'\\nüÜï NEW TRAINING MODE: Creating new run \"{RUN_NAME_TRAINING}\"')\n","\n","# Create training directory\n","TRAIN_DIR = TUNE_TRAIN_BASE / 'training' / RUN_NAME_TRAINING\n","TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Read dataset configuration\n","NUM_CLASSES = data_config['nc']\n","CLASS_NAMES = {i: name for i, name in enumerate(data_config['names'])}\n","CLASS_NAME_TO_ID = {name: i for i, name in enumerate(data_config['names'])}\n","\n","print('=' * 80)\n","print('CONFIGURATION SUMMARY')\n","print('=' * 80)\n","print(f'Environment: {\"Google Colab\" if \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\") else \"Local\"}')\n","print(f'Base Directory: {BASE_DIR}')\n","print(f'Model: {MODEL_NAME}')\n","print(f'Dataset: {YOLO_DATASET_ROOT.name}')\n","print(f'Data YAML: {DATA_YAML_PATH}')\n","print(f'  Dataset path in YAML: {data_config[\"path\"]}')\n","print(f'Classes: {NUM_CLASSES}')\n","print(f'Class Names: {CLASS_NAMES}')\n","print(f'Device: {device}')\n","print(f'Epochs Final Training: {EPOCHS_FINAL_TRAINING}')\n","print(f'Batch Size: {BATCH_SIZE}')\n","print(f'Configuration Mode: {\"Default (No Tuning)\" if USE_DEFAULT_CONFIG else \"Tuned Hyperparameters\"}')\n","if not USE_DEFAULT_CONFIG:\n","    print(f'Tuning Run: {TUNING_RUN_NAME}')\n","print(f'Training Directory: {TRAIN_DIR}')\n","if USE_WANDB:\n","    print(f'W&B Logging: Enabled')\n","    print(f'  Training Project: {WANDB_PROJECT_TRAINING}')\n","else:\n","    print(f'W&B Logging: Disabled')\n","print('=' * 80)"]},{"cell_type":"markdown","id":"0af35c3f","metadata":{"id":"0af35c3f"},"source":["## 4. Load Base YOLO Model"]},{"cell_type":"code","execution_count":7,"id":"3deeda88","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3deeda88","executionInfo":{"status":"ok","timestamp":1764367847175,"user_tz":-180,"elapsed":431,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"6a29089e-3027-4c07-fd79-48d5fb519e98"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Model loaded from /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1/yolov8m_finetuned_1.pt\n","Model summary: 169 layers, 25,862,110 parameters, 0 gradients, 79.1 GFLOPs\n","\n","üìä Model Information:\n","  Model: yolov8m_finetuned_1\n","  Classes in model: 10\n","  Task: detect\n","  Parameters: 25.9M\n","  Model Size: 0.0 MB\n","  FLOPs (640x640): 79.09 GFLOPs\n"]}],"source":["# Load YOLO model with automatic download\n","model_path = MODELS_DIR / f'{MODEL_NAME}.pt'\n","\n","if not model_path.exists():\n","    print(f'Model not found at {model_path}')\n","    print(f'Downloading {MODEL_NAME} ...')\n","\n","    try:\n","        # Download model - ensure .pt extension for ultralytics\n","        # Ultralytics expects model names with .pt extension for download\n","        if not MODEL_NAME.endswith('.pt'):\n","            model_name_for_download = MODEL_NAME + '.pt'\n","        else:\n","            model_name_for_download = MODEL_NAME\n","\n","        print(f'  Requesting model: {model_name_for_download}')\n","        model = YOLO(model_name_for_download)\n","\n","        # Create models directory\n","        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","        # Save model to our directory using export/save\n","        try:\n","            # Try to save using the model's save method\n","            if hasattr(model, 'save'):\n","                model.save(str(model_path))\n","                print(f'‚úì Model downloaded and saved to {model_path}')\n","                print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n","            else:\n","                # Fallback: copy from cache\n","                cache_patterns = [\n","                    str(Path.home() / '.cache' / 'ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n","                    str(Path.home() / '.config' / 'Ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n","                ]\n","\n","                model_found = False\n","                for pattern in cache_patterns:\n","                    cache_paths = glob.glob(pattern, recursive=True)\n","                    if cache_paths:\n","                        shutil.copy(cache_paths[0], model_path)\n","                        print(f'‚úì Model downloaded and saved to {model_path}')\n","                        print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n","                        model_found = True\n","                        break\n","\n","                if not model_found:\n","                    print(f'‚úì Model loaded from ultralytics cache')\n","                    print(f'  Note: Model is in cache, not copied to {model_path}')\n","                    print(f'  This is normal and the model will work correctly')\n","        except Exception as save_error:\n","            print(f'‚ö†Ô∏è  Could not save model to custom location: {save_error}')\n","            print(f'‚úì Model loaded successfully from ultralytics cache')\n","\n","    except Exception as download_error:\n","        print(f'\\n‚ùå Error downloading model: {download_error}')\n","        raise\n","else:\n","    model = YOLO(str(model_path))\n","    print(f'‚úì Model loaded from {model_path}')\n","\n","# Get model information\n","model_info_dict = {}\n","model_info_result = model.info()\n","model_info_keys = [\"layers\", \"params\", \"size(MB)\", \"FLOPs(G)\"]\n","\n","for info_key, info_value in zip(model_info_keys, model_info_result):\n","    model_info_dict[info_key] = info_value\n","\n","model_params = model_info_dict.get(\"params\", 0)\n","model_size_mb = model_info_dict.get(\"size(MB)\", 0)\n","flops_gflops = model_info_dict.get(\"FLOPs(G)\", 0)\n","\n","\n","print(f'\\nüìä Model Information:')\n","print(f'  Model: {MODEL_NAME}')\n","print(f'  Classes in model: {len(model.names)}')\n","print(f'  Task: {model.task}')\n","print(f'  Parameters: {model_params / 1e6:.1f}M')\n","print(f'  Model Size: {model_size_mb:.1f} MB')\n","print(f'  FLOPs (640x640): {flops_gflops:.2f} GFLOPs')"]},{"cell_type":"markdown","id":"5fe0b94d","metadata":{"id":"5fe0b94d"},"source":["## 6. Verify Dataset Structure"]},{"cell_type":"code","execution_count":8,"id":"71b15401","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71b15401","executionInfo":{"status":"ok","timestamp":1764367848047,"user_tz":-180,"elapsed":869,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"5eeebcf5-3e45-4c02-b4d0-285f8750b767"},"outputs":[{"output_type":"stream","name":"stdout","text":["Verifying YOLO dataset structure...\n","\n","üìÅ Dataset Root: /computer_vision_yolo/bdd100k_yolo_limited\n","  ‚úì train:  29959 images,  29959 labels\n","  ‚úì val  :  10000 images,  10000 labels\n","  ‚úì test :  20000 images,  20000 labels\n","\n","üìÑ Configuration: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tmp/yolov8m_finetuned_1/data.yaml\n","  Classes: 10\n","  Names: {0: 'person', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motor', 7: 'bike', 8: 'traffic light', 9: 'traffic sign'}\n","\n","‚úì Dataset verified: 59,959 total images\n","‚úì Ready for training\n"]}],"source":["# ============================================================================\n","# VERIFY DATASET STRUCTURE\n","# ============================================================================\n","\n","print('Verifying YOLO dataset structure...')\n","print(f'\\nüìÅ Dataset Root: {YOLO_DATASET_ROOT}')\n","\n","# Check all splits using constants\n","dataset_stats = {}\n","for split in [DatasetSplit.TRAIN, DatasetSplit.VAL, DatasetSplit.TEST]:\n","    images_dir = YOLO_DATASET_ROOT / 'images' / split\n","    labels_dir = YOLO_DATASET_ROOT / 'labels' / split\n","\n","    if images_dir.exists() and labels_dir.exists():\n","        num_images = len(list(images_dir.glob('*.jpg'))) + len(list(images_dir.glob('*.png')))\n","        num_labels = len(list(labels_dir.glob('*.txt')))\n","        dataset_stats[split] = {'images': num_images, 'labels': num_labels}\n","        print(f'  ‚úì {split:5s}: {num_images:6d} images, {num_labels:6d} labels')\n","    else:\n","        print(f'  ‚ö†Ô∏è  {split:5s}: Directory not found')\n","        dataset_stats[split] = {'images': 0, 'labels': 0}\n","\n","print(f'\\nüìÑ Configuration: {DATA_YAML_PATH}')\n","print(f'  Classes: {NUM_CLASSES}')\n","print(f'  Names: {CLASS_NAMES}')\n","\n","total_images = sum(stats['images'] for stats in dataset_stats.values())\n","print(f'\\n‚úì Dataset verified: {total_images:,} total images')\n","print('‚úì Ready for training')"]},{"cell_type":"markdown","id":"f03686b4","metadata":{"id":"f03686b4"},"source":["## 5. Load Best Hyperparameters from Tuning"]},{"cell_type":"code","execution_count":9,"id":"f67e089e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f67e089e","executionInfo":{"status":"ok","timestamp":1764367848074,"user_tz":-180,"elapsed":24,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"34af8243-eb7f-4d90-969d-ad71d7cfb5b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","LOADING BEST HYPERPARAMETERS FROM TUNING\n","================================================================================\n","Tuning Run: yolov8m_finetuned_1_tune_20251127_230340\n","Hyperparameters Path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/tune/yolov8m_finetuned_1_tune_20251127_230340/best_hyperparameters.json\n","\n","‚úì Loaded hyperparameters from nested structure\n","\n","‚úì Best Hyperparameters Loaded:\n","  imgsz               : 768\n","  lr0                 : 0.000161\n","  mixup               : 0.023789\n","  momentum            : 0.909850\n","  mosaic              : 0.820538\n","  optimizer           : SGD\n","  warmup_bias_lr      : 0.018938\n","  warmup_epochs       : 1\n","  warmup_momentum     : 0.514931\n","  weight_decay        : 0.000428\n","================================================================================\n"]}],"source":["# ============================================================================\n","# LOAD HYPERPARAMETERS (TUNED OR DEFAULT)\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","if USE_DEFAULT_CONFIG:\n","    print('USING DEFAULT YOLO CONFIGURATION')\n","    print('=' * 80)\n","    print('No hyperparameter tuning applied - using YOLO defaults')\n","\n","    # Use empty dict for hyperparameters - YOLO will use its defaults\n","    best_params = {}\n","\n","    print('\\n‚úì Training will use default YOLO hyperparameters')\n","    print('   Default values will be applied by the YOLO model')\n","\n","else:\n","    print('LOADING BEST HYPERPARAMETERS FROM TUNING')\n","    print('=' * 80)\n","    print(f'Tuning Run: {TUNING_RUN_NAME}')\n","    print(f'Hyperparameters Path: {best_hyperparams_path}')\n","\n","    # Load best hyperparameters from JSON\n","    with open(best_hyperparams_path, 'r', encoding='utf-8') as f:\n","        best_params_file = json.load(f)\n","\n","    # Extract only the actual hyperparameters (not metadata)\n","    # The file structure has metadata fields and a 'hyperparameters' field with the actual params\n","    if 'hyperparameters' in best_params_file:\n","        # New format: metadata + hyperparameters nested\n","        best_params = best_params_file['hyperparameters']\n","        print('\\n‚úì Loaded hyperparameters from nested structure')\n","    else:\n","        # Old format: hyperparameters directly in root\n","        # Filter out metadata fields that aren't YOLO parameters\n","        metadata_keys = {'model', 'dataset_root', 'data_yaml_path', 'notes',\n","                        'optimization_results', 'timestamp'}\n","        best_params = {k: v for k, v in best_params_file.items() if k not in metadata_keys}\n","        print('\\n‚úì Loaded hyperparameters from flat structure (filtered metadata)')\n","\n","    print('\\n‚úì Best Hyperparameters Loaded:')\n","    for key, value in sorted(best_params.items()):\n","        if isinstance(value, (int, float)):\n","            if isinstance(value, float):\n","                print(f'  {key:20s}: {value:.6f}')\n","            else:\n","                print(f'  {key:20s}: {value}')\n","        else:\n","            print(f'  {key:20s}: {value}')\n","\n","    # Load tuning metadata if available\n","    tuning_metadata_path = TUNE_DIR / 'optimization_metadata.json'\n","    if (not USE_DEFAULT_CONFIG and tuning_metadata_path.exists()):\n","        with open(tuning_metadata_path, 'r', encoding='utf-8') as f:\n","            tuning_metadata = json.load(f)\n","\n","        print('\\nüìä Tuning Run Summary:')\n","        print(f\"  Best Trial: {tuning_metadata.get('best_trial', 'N/A')}\")\n","        print(f\"  Best mAP@0.5: {tuning_metadata.get('best_map50', 0):.4f}\")\n","        print(f\"  Total Trials: {tuning_metadata.get('total_trials', 'N/A')}\")\n","        print(f\"  Completed Trials: {tuning_metadata.get('completed_trials', 'N/A')}\")\n","\n","        if 'optimization_duration' in tuning_metadata:\n","            print(f\"  Duration: {tuning_metadata['optimization_duration']}\")\n","\n","print('=' * 80)"]},{"cell_type":"markdown","id":"83385af0","metadata":{"id":"83385af0"},"source":["## 7. Train The Model"]},{"cell_type":"code","execution_count":10,"id":"df0bc9da","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"df0bc9da","executionInfo":{"status":"ok","timestamp":1764371388163,"user_tz":-180,"elapsed":3540077,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"d903e7d2-c078-416d-aeb2-5c45aeaa78ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","TRAINING FINAL MODEL WITH OPTIMIZED HYPERPARAMETERS\n","================================================================================\n","\n","üì¶ Loading base model: yolov8m_finetuned_1\n","\n","üöÄ Starting training...\n","  Configuration: Tuned Hyperparameters\n","  Epochs: 100\n","  Batch Size: 64\n","  Dataset: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tmp/yolov8m_finetuned_1/data.yaml\n","  Device: cuda\n","  Resume: False\n","\n","üìä Applied Hyperparameters:\n","  imgsz               : 768\n","  lr0                 : 0.000161\n","  mixup               : 0.023789\n","  momentum            : 0.909850\n","  mosaic              : 0.820538\n","  optimizer           : SGD\n","  warmup_bias_lr      : 0.018938\n","  warmup_epochs       : 1\n","  warmup_momentum     : 0.514931\n","  weight_decay        : 0.000428\n","\n","This may take a while. Training progress will be displayed below.\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm3mahdy\u001b[0m (\u001b[33mm3mahdy-king-saud-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251128_221050-uxr3c2ja</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/uxr3c2ja' target=\"_blank\">yolov8m_finetuned_1_train_20251128_221046</a></strong> to <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/uxr3c2ja' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/uxr3c2ja</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úì W&B initialized: yolo-bdd100k_yolo_limited-training/yolov8m_finetuned_1_train_20251128_221046\n","Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tmp/yolov8m_finetuned_1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=768, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001614596240725037, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.023789168654792303, mode=train, model=/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1/yolov8m_finetuned_1.pt, momentum=0.9098500376932075, mosaic=0.8205379408033115, multi_scale=False, name=yolov8m_finetuned_1_train_20251128_221046, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046, save_frames=False, save_json=False, save_period=3, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.018938388657529233, warmup_epochs=1, warmup_momentum=0.5149307497845836, weight_decay=0.00042786651935909105, workers=8, workspace=None\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   3781486  ultralytics.nn.modules.head.Detect           [10, [192, 384, 576]]         \n","Model summary: 169 layers, 25,862,110 parameters, 25,862,094 gradients, 79.1 GFLOPs\n","\n","Transferred 475/475 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1935.7¬±542.2 MB/s, size: 55.3 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/train.cache... 29959 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 29959/29959 20.8Mit/s 0.0s\n","\u001b[34m\u001b[1mtrain: \u001b[0m/computer_vision_yolo/bdd100k_yolo_limited/images/train/75055858-7d04a650.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 918.5¬±412.1 MB/s, size: 41.1 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10000/10000 7.0Mit/s 0.0s\n","Plotting labels to /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0001614596240725037, momentum=0.9098500376932075) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.00042786651935909105), 83 bias(decay=0.0)\n","Image sizes 768 train, 768 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1m/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      1/100      38.9G      1.159      0.604     0.9662        311        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:27\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 43.6s\n","                   all      10000     185578      0.654       0.53      0.576      0.333\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      2/100      35.2G      1.149     0.5967     0.9595        276        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.8s\n","                   all      10000     185578      0.605      0.545      0.576      0.333\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      3/100      36.7G      1.148     0.5962     0.9598        303        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 43.0s\n","                   all      10000     185578       0.64      0.521      0.576      0.332\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      4/100      37.7G      1.148      0.596     0.9583        206        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.8s\n","                   all      10000     185578      0.633      0.527      0.575      0.331\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      5/100      37.5G      1.146     0.5948     0.9574        163        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.8s\n","                   all      10000     185578       0.65      0.539      0.574      0.331\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      6/100        38G      1.148     0.5948     0.9587        168        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:22\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.9s\n","                   all      10000     185578      0.659      0.538      0.575      0.331\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      7/100        35G      1.145     0.5927     0.9571        247        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.9s\n","                   all      10000     185578      0.646      0.544      0.575      0.332\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      8/100      36.9G      1.147     0.5937     0.9569        186        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:22\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 43.0s\n","                   all      10000     185578      0.658      0.536      0.575      0.332\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      9/100      37.2G      1.145     0.5928     0.9568        267        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 43.2s\n","                   all      10000     185578      0.645      0.548      0.576      0.331\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     10/100      35.1G      1.145     0.5914     0.9571        362        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 43.0s\n","                   all      10000     185578      0.625      0.552      0.574       0.33\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     11/100      36.9G      1.143     0.5917     0.9558        250        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 1.8it/s 4:21\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.8it/s 42.8s\n","                   all      10000     185578       0.63       0.55      0.575      0.331\n","\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 1, best model saved as best.pt.\n","To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n","\n","11 epochs completed in 0.940 hours.\n","Optimizer stripped from /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/weights/last.pt, 52.0MB\n","Optimizer stripped from /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/weights/best.pt, 52.0MB\n","\n","Validating /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/weights/best.pt...\n","Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 1.5it/s 53.9s\n","                   all      10000     185578      0.653       0.53      0.576      0.333\n","                person       3220      13265      0.791      0.584      0.687      0.358\n","                 rider        515        649      0.633       0.49      0.508      0.272\n","                   car       9879     102540      0.846      0.726      0.816      0.514\n","                 truck       2689       4247      0.664       0.61      0.654      0.479\n","                   bus       1242       1597       0.66      0.603      0.652      0.504\n","                 train         14         15      0.164     0.0667     0.0416     0.0274\n","                 motor        334        452      0.652      0.482      0.492      0.254\n","                  bike        578       1007      0.612      0.537      0.542      0.284\n","         traffic light       5653      26891      0.757      0.568      0.654      0.258\n","          traffic sign       8221      34915      0.753      0.635      0.708      0.382\n","Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\u001b[0m\n","\n","‚úì Training completed successfully!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">yolov8m_finetuned_1_train_20251128_221046</strong> at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/uxr3c2ja' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training/runs/uxr3c2ja</a><br> View project at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251128_221050-uxr3c2ja/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úì W&B run finished\n","\n","================================================================================\n","TRAINING SUMMARY\n","================================================================================\n","\n","üìä Running final validation...\n","Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2084.4¬±440.5 MB/s, size: 79.9 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10000/10000 16.7Mit/s 0.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 625/625 9.7it/s 1:04\n","                   all      10000     185578      0.653      0.531      0.577      0.335\n","                person       3220      13265      0.791      0.584      0.687      0.359\n","                 rider        515        649      0.638      0.495      0.516      0.273\n","                   car       9879     102540      0.846      0.727      0.817      0.515\n","                 truck       2689       4247      0.664      0.611      0.655      0.479\n","                   bus       1242       1597      0.659      0.603      0.653      0.507\n","                 train         14         15      0.161     0.0667     0.0416     0.0274\n","                 motor        334        452      0.648       0.48      0.491      0.256\n","                  bike        578       1007      0.612      0.538      0.544      0.287\n","         traffic light       5653      26891      0.759       0.57      0.656       0.26\n","          traffic sign       8221      34915      0.753      0.636      0.709      0.383\n","Speed: 0.5ms preprocess, 1.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/final_val\u001b[0m\n","\n","üìä Final Model Performance:\n","  mAP@0.5: 0.5769\n","  mAP@0.5:0.95: 0.3347\n","  Precision: 0.6530\n","  Recall: 0.5310\n","\n","üíæ Training log saved: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/training_log.json\n","Last Weights: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/weights/last.pt\n","================================================================================\n","Best Weights: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/weights/best.pt\n","Training Directory: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\n","Start Time: 2025-11-28 22:10:52\n","Configuration: Tuned (yolov8m_finetuned_1_tune_20251127_230340)\n","End Time: 2025-11-28 23:08:38\n","Duration: 0:57:45.776090\n"]}],"source":["# ============================================================================\n","# TRAIN FINAL MODEL WITH OPTIMIZED HYPERPARAMETERS\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","if USE_DEFAULT_CONFIG:\n","    print('TRAINING FINAL MODEL WITH DEFAULT CONFIGURATION')\n","else:\n","    print('TRAINING FINAL MODEL WITH OPTIMIZED HYPERPARAMETERS')\n","print('=' * 80)\n","\n","# Check if resuming from previous training\n","checkpoint_path = TRAIN_DIR / 'weights' / 'last.pt'\n","training_log_path = TRAIN_DIR / 'training_log.json'\n","is_resuming = checkpoint_path.exists()\n","\n","if is_resuming:\n","    # Resume training\n","    print('\\n' + '=' * 80)\n","    print('üîÑ RESUMING PREVIOUS TRAINING')\n","    print('=' * 80)\n","    print(f'Checkpoint: {checkpoint_path}')\n","\n","    # Load training log if available\n","    if training_log_path.exists():\n","        with open(training_log_path, 'r', encoding='utf-8') as f:\n","            training_log = json.load(f)\n","\n","        print(f'\\nüìä Previous Training Summary:')\n","        print(f\"  Started: {training_log.get('start_time', 'N/A')}\")\n","        if 'last_epoch' in training_log:\n","            print(f\"  Last Epoch: {training_log['last_epoch']}\")\n","        if 'best_map50' in training_log:\n","            print(f\"  Best mAP@0.5: {training_log['best_map50']:.4f}\")\n","        if 'last_checkpoint' in training_log:\n","            print(f\"  Last Checkpoint: {training_log['last_checkpoint']}\")\n","\n","    print(f'\\n‚û°Ô∏è  Resuming training from checkpoint')\n","    print('=' * 80)\n","\n","    # Load model from checkpoint\n","    print(f'\\nüì¶ Loading model from checkpoint: {checkpoint_path}')\n","    final_model = YOLO(str(checkpoint_path))\n","    model_to_train = str(checkpoint_path)\n","    resume_training = True\n","\n","else:\n","    # Start new training\n","    print(f'\\nüì¶ Loading base model: {MODEL_NAME}')\n","    final_model = YOLO(str(model_path))\n","    model_to_train = str(model_path)\n","    resume_training = False\n","\n","    # Initialize training log\n","    training_log = {\n","        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'model': MODEL_NAME,\n","        'dataset': YOLO_DATASET_ROOT.name,\n","        'config_mode': 'default' if USE_DEFAULT_CONFIG else 'tuned',\n","        'tuning_run': TUNING_RUN_NAME if not USE_DEFAULT_CONFIG else None,\n","        'best_hyperparameters': best_params,\n","        'epochs': EPOCHS_FINAL_TRAINING,\n","        'batch_size': BATCH_SIZE,\n","        'image_size': best_params.get('imgsz', 640)\n","    }\n","\n","# Prepare training parameters\n","# Note: Fixed parameters (not part of optimization) are always included\n","# Optimization parameters are added via **best_params (empty if using defaults)\n","# NOTE: 'imgsz' removed - will come from best_params if tuned, or YOLO default if not\n","\n","final_training_params = {\n","    # ============================================================================\n","    # FIXED PARAMETERS - Always passed, not part of hyperparameter optimization\n","    # ============================================================================\n","    'data': str(DATA_YAML_PATH),              # Dataset configuration file\n","    'epochs': EPOCHS_FINAL_TRAINING,          # Number of training epochs\n","    'batch': BATCH_SIZE,                       # Batch size\n","    'device': device,                          # Training device (cuda/cpu)\n","    'project': str(TRAIN_DIR.parent),         # Project directory\n","    'name': TRAIN_DIR.name,                    # Run name\n","    'exist_ok': True,                          # Overwrite existing project\n","    'patience': ModelConfig.DEFAULT_PATIENCE,  # Early stopping patience\n","    'save_period': ModelConfig.DEFAULT_SAVE_PERIOD,  # Save checkpoint frequency\n","    'workers': ModelConfig.DEFAULT_WORKERS,    # Number of data loading workers\n","    'verbose': True,                           # Verbose output\n","    'seed': 42,                                # Random seed for reproducibility\n","    'close_mosaic': ModelConfig.CLOSE_MOSAIC_EPOCHS,  # Disable mosaic in final epochs\n","    'resume': resume_training,                 # Resume from checkpoint if exists\n","    'cache': ModelConfig.DEFAULT_CACHE,        # Cache images for faster training\n","    'val': ModelConfig.DEFAULT_VAL,            # Run validation during training\n","\n","    # ============================================================================\n","    # OPTIMIZATION PARAMETERS - From tuning (if USE_DEFAULT_CONFIG=False)\n","    # ============================================================================\n","    # Parameters like: lr0, lrf, momentum, weight_decay, warmup_epochs, etc.\n","    **best_params  # Empty dict if USE_DEFAULT_CONFIG=True, tuned params otherwise\n","}\n","\n","print(f'\\nüöÄ {\"Resuming\" if resume_training else \"Starting\"} training...')\n","print(f'  Configuration: {\"Default YOLO\" if USE_DEFAULT_CONFIG else \"Tuned Hyperparameters\"}')\n","print(f'  Epochs: {final_training_params[\"epochs\"]}')\n","print(f'  Batch Size: {final_training_params[\"batch\"]}')\n","print(f'  Dataset: {DATA_YAML_PATH}')\n","print(f'  Device: {device}')\n","print(f'  Resume: {resume_training}')\n","\n","if best_params:\n","    print('\\nüìä Applied Hyperparameters:')\n","    for key, value in sorted(best_params.items()):\n","        if isinstance(value, float):\n","            print(f'  {key:20s}: {value:.6f}')\n","        else:\n","            print(f'  {key:20s}: {value}')\n","else:\n","    print('\\nüìä Using YOLO default hyperparameters (no custom values)')\n","\n","print('\\nThis may take a while. Training progress will be displayed below.')\n","print('=' * 80)\n","\n","# Initialize W&B for final training\n","if USE_WANDB:\n","    try:\n","        wandb_config = {\n","            'model': MODEL_NAME,\n","            'dataset': YOLO_DATASET_ROOT.name,\n","            'phase': 'final_training',\n","            'config_mode': 'default' if USE_DEFAULT_CONFIG else 'tuned',\n","            'tuning_run': TUNING_RUN_NAME if not USE_DEFAULT_CONFIG else None,\n","            'epochs': final_training_params['epochs'],\n","            'batch_size': final_training_params['batch'],\n","            'resume': resume_training,\n","            **best_params\n","        }\n","\n","        wandb_training_run = wandb.init(\n","            project=WANDB_PROJECT_TRAINING,\n","            name=RUN_NAME_TRAINING,\n","            id=training_log.get('wandb_run_id') if is_resuming else None,\n","            resume='allow' if is_resuming else None,\n","            config=wandb_config,\n","            group='final-training',\n","            tags=['final', 'optimized' if not USE_DEFAULT_CONFIG else 'default', MODEL_NAME, YOLO_DATASET_ROOT.name]\n","        )\n","\n","        # Save W&B run ID for future resume\n","        if not is_resuming:\n","            training_log['wandb_run_id'] = wandb_training_run.id\n","            with open(training_log_path, 'w', encoding='utf-8') as f:\n","                json.dump(training_log, f, indent=2)\n","\n","        print(f'‚úì W&B initialized: {WANDB_PROJECT_TRAINING}/{RUN_NAME_TRAINING}')\n","    except Exception as wandb_error:\n","        print(f'‚ö†Ô∏è  Could not initialize W&B: {wandb_error}')\n","        wandb_training_run = None\n","else:\n","    wandb_training_run = None\n","\n","# Train model\n","start_time = datetime.now()\n","try:\n","    final_results = final_model.train(**final_training_params)\n","\n","    # Update training log with completion\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    training_log['status'] = 'completed'\n","    training_log['duration'] = str(datetime.now() - start_time)\n","\n","    # Save final metrics\n","    if hasattr(final_results, 'results_dict'):\n","        training_log['final_metrics'] = final_results.results_dict\n","\n","    # Save updated training log\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","\n","    print('\\n‚úì Training completed successfully!')\n","\n","except KeyboardInterrupt:\n","    print('\\n‚ö†Ô∏è  Training interrupted by user')\n","    print(f'üíæ Progress saved to: {TRAIN_DIR}')\n","    print(f'   - Last checkpoint: {checkpoint_path}')\n","    print(f'   - Training log: {training_log_path}')\n","    print(f'\\nüîÑ To resume: Simply re-run this notebook')\n","\n","    # Update training log\n","    training_log['last_interrupt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    training_log['status'] = 'interrupted'\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","    raise\n","\n","except Exception as e:\n","    print(f'\\n‚ùå Training failed with error: {e}')\n","    training_log['status'] = 'failed'\n","    training_log['error'] = str(e)\n","    training_log['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    with open(training_log_path, 'w', encoding='utf-8') as f:\n","        json.dump(training_log, f, indent=2)\n","    raise\n","\n","finally:\n","    if USE_WANDB and wandb_training_run is not None:\n","        wandb_training_run.finish()\n","        print('‚úì W&B run finished')\n","\n","end_time = datetime.now()\n","duration = end_time - start_time\n","\n","print('\\n' + '=' * 80)\n","print('TRAINING SUMMARY')\n","print('=' * 80)\n","\n","# Get final validation metrics\n","print('\\nüìä Running final validation...')\n","final_val_results = final_model.val(\n","    data=str(DATA_YAML_PATH),\n","    project=str(TRAIN_DIR),\n","    name='final_val',\n",")\n","\n","final_metrics = {\n","    'map50': float(final_val_results.box.map50),\n","    'map50_95': float(final_val_results.box.map),\n","    'precision': float(final_val_results.box.mp),\n","    'recall': float(final_val_results.box.mr),\n","}\n","\n","print('\\nüìä Final Model Performance:')\n","print(f\"  mAP@0.5: {final_metrics['map50']:.4f}\")\n","print(f\"  mAP@0.5:0.95: {final_metrics['map50_95']:.4f}\")\n","print(f\"  Precision: {final_metrics['precision']:.4f}\")\n","print(f\"  Recall: {final_metrics['recall']:.4f}\")\n","\n","# Update training log with final metrics\n","training_log['final_metrics'] = final_metrics\n","training_log['best_model_path'] = str(TRAIN_DIR / 'weights' / 'best.pt')\n","training_log['last_model_path'] = str(TRAIN_DIR / 'weights' / 'last.pt')\n","\n","# Save final training log\n","with open(training_log_path, 'w', encoding='utf-8') as f:\n","    json.dump(training_log, f, indent=2)\n","\n","print(f'\\nüíæ Training log saved: {training_log_path}')\n","\n","# Compare with tuning results if available\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    tuning_best_map = tuning_metadata.get('best_map50', 0)\n","    improvement = final_metrics['map50'] - tuning_best_map\n","    print('\\nüìà Improvement vs Best Tuning Trial:')\n","    print(f\"  Best Tuning mAP@0.5: {tuning_best_map:.4f}\")\n","    print(f\"  Final Model mAP@0.5: {final_metrics['map50']:.4f}\")\n","\n","    print(f\"  Improvement: {improvement:+.4f} ({improvement/tuning_best_map*100:+.2f}%)\")\n","    print('=' * 80)\n","\n","print(f'Last Weights: {TRAIN_DIR / \"weights\" / \"last.pt\"}')\n","\n","print('=' * 80)\n","print(f'Best Weights: {TRAIN_DIR / \"weights\" / \"best.pt\"}')\n","\n","print(f'Training Directory: {TRAIN_DIR}')\n","\n","print(f'Start Time: {start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","print(f'Configuration: {\"Default YOLO\" if USE_DEFAULT_CONFIG else f\"Tuned ({TUNING_RUN_NAME})\"}')\n","\n","print(f'End Time: {end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","print(f'Duration: {duration}')"]},{"cell_type":"markdown","id":"b4b00a39","metadata":{"id":"b4b00a39"},"source":["## 8. Save Final Model and Metadata"]},{"cell_type":"code","execution_count":11,"id":"9d959bb4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d959bb4","executionInfo":{"status":"ok","timestamp":1764371388335,"user_tz":-180,"elapsed":165,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"33a9ff4b-7d37-4e1a-9403-1ef5b596013a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","SAVING FINAL OPTIMIZED MODEL\n","================================================================================\n","\n","‚úì Final model saved to: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","  Size: 49.6 MB\n","‚úì Model metadata saved to: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128_metadata.json\n","\n","üì¶ Final Model Package:\n","  Model: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","  Metadata: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128_metadata.json\n","  Training Log: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/training_log.json\n","================================================================================\n"]}],"source":["# ============================================================================\n","# SAVE FINAL OPTIMIZED MODEL\n","# ============================================================================\n","\n","print('\\n' + '=' * 80)\n","print('SAVING FINAL OPTIMIZED MODEL')\n","print('=' * 80)\n","\n","date_stamp = datetime.now().strftime('%Y%m%d')\n","finetuned_model_name = f'{MODEL_NAME}_finetuned_{date_stamp}'\n","\n","# Create model directory if it doesn't exist\n","model_save_dir = BASE_DIR / 'models' / finetuned_model_name\n","model_save_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Define paths for saving\n","final_model_path = model_save_dir / f'{finetuned_model_name}.pt'\n","metadata_path = model_save_dir / f'{finetuned_model_name}_metadata.json'\n","\n","# Copy best weights from training directory\n","# Note: TRAIN_DIR already includes RUN_NAME_TRAINING\n","weights_path = TRAIN_DIR / 'weights' / 'best.pt'\n","\n","if weights_path.exists():\n","    shutil.copy(weights_path, final_model_path)\n","    print(f'\\n‚úì Final model saved to: {final_model_path}')\n","    print(f'  Size: {final_model_path.stat().st_size / (1024*1024):.1f} MB')\n","else:\n","    print(f'\\n‚ö†Ô∏è  Best weights not found at: {weights_path}')\n","    print('  Attempting to save current model state...')\n","    try:\n","        # Save current model state if weights not found\n","        final_model.save(str(final_model_path))\n","        print(f'‚úì Model saved to: {final_model_path}')\n","    except Exception as save_error:\n","        print(f'‚ö†Ô∏è  Error saving model: {save_error}')\n","\n","# Prepare optimization metadata\n","optimization_meta = {\n","    'tuning_run': TUNING_RUN_NAME,\n","    'tuning_run_path': str(TUNE_DIR),\n","}\n","\n","# Add tuning details if available\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    optimization_meta.update({\n","        'n_trials': tuning_metadata.get('total_trials', 'N/A'),\n","        'completed_trials': tuning_metadata.get('completed_trials', 'N/A'),\n","        'best_trial': tuning_metadata.get('best_trial', 'N/A'),\n","        'best_trial_map50': tuning_metadata.get('best_map50', 0),\n","        'optimization_duration': tuning_metadata.get('optimization_duration', 'N/A'),\n","    })\n","\n","# Calculate improvement if tuning metadata available\n","improvement_value = 0\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    tuning_best_map = tuning_metadata.get('best_map50', 0)\n","    if tuning_best_map > 0:\n","        improvement_value = float(final_metrics['map50'] - tuning_best_map)\n","\n","# Save model metadata\n","metadata = {\n","    'model_name': MODEL_NAME,\n","    'finetuned_name': finetuned_model_name,\n","    'model_path': str(final_model_path),\n","    'dataset': str(YOLO_DATASET_ROOT),\n","    'training_date': datetime.now().isoformat(),\n","    'training_run': RUN_NAME_TRAINING,\n","    'training_run_path': str(TRAIN_DIR),\n","    'optimization': optimization_meta,\n","    'best_hyperparameters': best_params,\n","    'training_params': {\n","        'epochs': EPOCHS_FINAL_TRAINING,\n","        'batch_size': BATCH_SIZE,\n","        'image_size': best_params.get('imgsz', 640),\n","        'patience': ModelConfig.DEFAULT_PATIENCE,\n","        'save_period': ModelConfig.DEFAULT_SAVE_PERIOD,\n","    },\n","    'final_metrics': final_metrics,\n","    'improvement_vs_tuning': improvement_value,\n","}\n","\n","with open(metadata_path, 'w', encoding='utf-8') as f:\n","    json.dump(metadata, f, indent=2)\n","\n","print(f'‚úì Model metadata saved to: {metadata_path}')\n","print('\\nüì¶ Final Model Package:')\n","print(f'  Model: {final_model_path}')\n","print(f'  Metadata: {metadata_path}')\n","print(f'  Training Log: {training_log_path}')\n","print('=' * 80)"]},{"cell_type":"markdown","id":"338ab57b","metadata":{"id":"338ab57b"},"source":["## 9. Test Final Model"]},{"cell_type":"code","execution_count":12,"id":"6cfc6346","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6cfc6346","executionInfo":{"status":"ok","timestamp":1764372353870,"user_tz":-180,"elapsed":965531,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"9ba0f434-6f7d-4a65-bce3-11182e69179c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","RUNNING FINAL VALIDATION ON TEST SET\n","================================================================================\n","\n","üìÇ Dataset location mismatch detected\n","   Dataset is in: /computer_vision_yolo/bdd100k_yolo_limited\n","   Validation expects: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/bdd100k_yolo_limited\n","   Creating symbolic link...\n","   ‚úì Symbolic link created\n","\n","üîç Validation Configuration:\n","   Base Dir: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo\n","   Dataset: bdd100k_yolo_limited\n","   Model: yolov8m_finetuned_1_finetuned_20251128\n","   Expected dataset path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/bdd100k_yolo_limited/data.yaml\n","   Expected model path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","   Actual model path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","‚úì Device: cuda\n","  GPU: NVIDIA A100-SXM4-40GB\n","‚úì W&B logging enabled\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251128_230949-g4xnialu</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/g4xnialu' target=\"_blank\">yolov8m_finetuned_1_finetuned_20251128_bdd100k_yolo_limited_test_20251128_230949</a></strong> to <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/g4xnialu' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/g4xnialu</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úì Weights & Biases initialized: yolov8m_finetuned_1_finetuned_20251128_bdd100k_yolo_limited_test_20251128_230949\n","‚úì Dataset loaded\n","  Total images: 20000\n","  Images with labels: 20000\n","  Label files: 20000\n","\n","‚úì Metadata loaded: test_metadata.json\n","  Images with attributes: 20000\n","‚úì Model loaded from /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","Model summary: 169 layers, 25,862,110 parameters, 0 gradients, 79.1 GFLOPs\n","\n","üìä Model Information:\n","  Model: yolov8m_finetuned_1_finetuned_20251128\n","  Classes in model: 10\n","  Task: detect\n","  Parameters: 25.9M\n","  Model Size: 49.6 MB\n","  FLOPs (640x640): 79.09 GFLOPs\n","  Model Size: 49.6 MB\n","\n","================================================================================\n","PHASE 1: OFFICIAL YOLO VALIDATION (for accurate metrics)\n","================================================================================\n","WARNING ‚ö†Ô∏è 'save_hybrid' is deprecated and will be removed in the future.\n","Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Model summary (fused): 92 layers, 25,845,550 parameters, 0 gradients, 78.7 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1235.2¬±486.4 MB/s, size: 52.1 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /computer_vision_yolo/bdd100k_yolo_limited/labels/test... 20000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20000/20000 1.5Kit/s 13.4s\n","\u001b[34m\u001b[1mval: \u001b[0m/computer_vision_yolo/bdd100k_yolo_limited/images/test/e6f10c58-c46de527.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /computer_vision_yolo/bdd100k_yolo_limited/labels/test.cache\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 313/313 2.6it/s 1:59\n","                   all      20000     367727      0.637       0.55      0.585      0.334\n","                person       6213      24650      0.782      0.605      0.693      0.358\n","                 rider       1004       1294      0.645       0.54      0.552      0.291\n","                   car      19776     205149      0.838       0.74      0.815      0.511\n","                 truck       5500       8704      0.649      0.622      0.653      0.477\n","                   bus       2459       3217      0.625      0.596      0.611      0.469\n","                 train         26         28     0.0926     0.0357      0.058     0.0352\n","                 motor        640        841      0.619      0.552      0.543      0.276\n","                  bike       1182       1998      0.645      0.564      0.563      0.282\n","         traffic light      11051      52840      0.742      0.591      0.654      0.257\n","          traffic sign      16432      69006      0.733       0.65      0.709      0.382\n","Speed: 0.5ms preprocess, 1.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n","Results saved to \u001b[1m/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/yolo_validation\u001b[0m\n","\n","‚úì Official validation complete in 139.92s\n","‚úì Confusion matrix shape: (11, 11)\n","\n","‚úì Official Metrics:\n","  - Precision: 0.6371\n","  - Recall: 0.5496\n","  - mAP@0.5: 0.5850\n","  - mAP@0.5:0.95: 0.3338\n","\n","================================================================================\n","PHASE 2: PER-IMAGE PREDICTION (for attribute-based analysis)\n","================================================================================\n","\n","================================================================================\n","COLLECTING PER-IMAGE PREDICTIONS FOR DETAILED ANALYSIS\n","================================================================================\n","‚úì Found 20000 images to evaluate\n","‚úì Model device: cuda\n","‚úì IoU threshold: 0.5\n","‚úì Collecting per-image details for attribute-based analysis...\n","‚úì Tracking 10 classes\n","  Model classes: ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motor', 'bike', 'traffic light', 'traffic sign']\n","\n","================================================================================\n","PROCESSING IMAGES\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  10%|‚ñà         | 2002/20000 [01:09<10:16, 29.21img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 2000/20000 images | Avg: 19.6ms/img | ETA: 352.9s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  20%|‚ñà‚ñà        | 4003/20000 [02:19<09:15, 28.79img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 4000/20000 images | Avg: 19.6ms/img | ETA: 313.0s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  30%|‚ñà‚ñà‚ñà       | 6003/20000 [03:28<07:46, 30.03img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 6000/20000 images | Avg: 19.5ms/img | ETA: 273.0s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  40%|‚ñà‚ñà‚ñà‚ñà      | 8005/20000 [04:37<06:28, 30.84img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 8000/20000 images | Avg: 19.4ms/img | ETA: 233.2s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10005/20000 [05:46<05:29, 30.36img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 10000/20000 images | Avg: 19.4ms/img | ETA: 194.3s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12002/20000 [06:55<04:53, 27.26img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 12000/20000 images | Avg: 19.4ms/img | ETA: 155.2s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14005/20000 [08:04<03:15, 30.73img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 14000/20000 images | Avg: 19.4ms/img | ETA: 116.4s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16003/20000 [09:13<02:09, 30.78img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 16000/20000 images | Avg: 19.4ms/img | ETA: 77.6s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18003/20000 [10:23<01:06, 30.18img/s]"]},{"output_type":"stream","name":"stdout","text":["  Progress: 18000/20000 images | Avg: 19.4ms/img | ETA: 38.9s\n"]},{"output_type":"stream","name":"stderr","text":["Collecting image details: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [11:34<00:00, 28.81img/s]\n"]},{"output_type":"stream","name":"stdout","text":["  Progress: 20000/20000 images | Avg: 19.5ms/img | ETA: 0.0s\n","\n","================================================================================\n","PER-IMAGE COLLECTION COMPLETE\n","================================================================================\n","‚úì Total images processed: 20000\n","‚úì Collection time: 694.15s\n","‚úì Average time per image: 0.035s\n","================================================================================\n","\n","\n","‚úì Hybrid validation complete. Official validation time: 139.92s\n","\n","‚úì Calculated per-class metrics from confusion matrix:\n","  Total TP: 266452, FP: 101275, FN: 72607\n","\n","================================================================================\n","OFFICIAL YOLO VALIDATION RESULTS\n","================================================================================\n","Precision (mean): 0.6371\n","Recall (mean):    0.5496\n","mAP@0.5:          0.5850\n","mAP@0.5:0.95:     0.3338\n","Fitness:          0.3338\n","\n","‚ö° Performance Metrics:\n","  Total Time: 139.92s\n","  Average Inference Time: 3034.72 ms per image\n","  FPS (Frames Per Second): 329.52\n","================================================================================\n","(Green = Correct Predictions, Red = Incorrect Predictions, White = No Predictions)\n","‚úì Copied confusion matrix from YOLO validation\n","‚úì Copied normalized confusion matrix from YOLO validation\n","\n","================================================================================\n","GENERATING SAMPLE COMPARISONS\n","================================================================================\n","\n","Generating 6 high-resolution comparison figures with attributes...\n"]},{"output_type":"stream","name":"stderr","text":["Generating comparisons: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:13<00:00,  2.23s/it]\n"]},{"output_type":"stream","name":"stdout","text":["‚úì Generated 6 comparison images\n","  Saved to: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/sample_comparisons\n","\n","================================================================================\n","GENERATING DETAILED PERFORMANCE ANALYSIS\n","================================================================================\n","\n","================================================================================\n","GENERATING COMPREHENSIVE FAILURE ANALYSIS\n","Analyzing relationship between attributes and prediction accuracy...\n","================================================================================\n","\n","üîç Attempting to load training metadata from: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/bdd100k_yolo_limited/representative_json/train_metadata.json\n","  File exists: True\n","\n","Loading training split metadata for exposure analysis...\n","  Processing 29959 training images...\n","‚úì Training metadata loaded: 29959 images, 629064 objects\n","  - Classes found in training data: 10\n","  - Sample class counts: {2: 327554, 3: 16965, 0: 56706}\n","\n","================================================================================\n","ANALYZING FAILURES BY ATTRIBUTES\n","================================================================================\n","‚úì Analysis results will be saved to: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/performance_analysis\n","‚úì Training metadata loaded with 10 classes\n","Processing 20000 images for attribute analysis...\n"]},{"output_type":"stream","name":"stderr","text":["Analyzing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:37<00:00, 536.44img/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úì Analysis complete:\n","  - Total images: 20000\n","  - Total objects: 367728\n","  - Matched objects: 313471\n","  - Overall accuracy: 85.25%\n","\n","‚úì Saving analysis results to CSV files...\n","‚úì Saved 10 CSV files to /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/performance_analysis\n","\n","‚úì Attribute-based analysis complete\n","  Total images analyzed: 20000\n","  Overall accuracy: 85.25%\n","\n","üìä Training Exposure Analysis Status:\n","  - train_class_counts populated: True (10 classes)\n","  - df_train_test populated: True (10 rows)\n","  - Sample df_train_test rows:\n"," class_id class_name  train_count  test_count  test_accuracy  train_test_ratio\n","        0     person        56706       24650       0.827992          2.300446\n","        1      rider         4346        1294       0.626739          3.358578\n","        2        car       327554      205150       0.872878          1.596656\n","  ‚úì Training exposure charts will be generated\n","\n","‚úì Analysis charts will be saved to: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/performance_analysis\n","\n"," Generating accuracy analysis charts...\n","\n","üîç Training Exposure Charts Generation Check:\n","  - train_class_counts: 10 classes\n","  - df_train_test: 10 rows\n","  - Condition (not df_train_test.empty and train_class_counts): {2: 327554, 3: 16965, 0: 56706, 1: 4346, 7: 6784, 8: 91122, 9: 114390, 6: 3002, 4: 8059, 5: 136}\n","  ‚úì Generated: accuracy_vs_training_exposure.png\n","\n","================================================================================\n","ANALYSIS SUMMARY\n","================================================================================\n","Overall Accuracy: 85.25%\n","Total Images: 20000\n","Expected Objects: 367728\n","Matched Objects: 313471\n","\n","Weakest Weather Conditions:\n","  - overcast: 84.72% (2568 images)\n","  - clear: 85.04% (10756 images)\n","  - partly cloudy: 85.05% (1441 images)\n","\n","Weakest Scenes:\n","  - gas stations: 82.35% (6 images)\n","  - highway: 84.16% (5069 images)\n","  - city street: 85.22% (12288 images)\n","\n","Weakest Times of Day:\n","  - dawn/dusk: 85.06% (1476 images)\n","  - undefined: 85.10% (42 images)\n","  - night: 85.13% (8036 images)\n","\n","Accuracy by Object Size (Scale/Distance):\n","  - small: 83.51% (321502 objects)\n","  - medium: 97.32% (33242 objects)\n","  - large: 97.21% (12984 objects)\n","\n","Train-Test Comparison (Classes with Lowest Accuracy):\n","  - train: 17.86% accuracy | Train: 136 objs, Test: 28 objs | Ratio: 4.9x\n","  - rider: 62.67% accuracy | Train: 4346 objs, Test: 1294 objs | Ratio: 3.4x\n","  - bus: 69.35% accuracy | Train: 8059 objs, Test: 3217 objs | Ratio: 2.5x\n","\n","‚úì Comprehensive failure analysis complete\n","  - Charts saved: 10\n","  - CSV files saved: 12\n","  - JSON summary: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/failure_analysis_comprehensive.json\n","================================================================================\n","================================================================================\n","‚úì COMPREHENSIVE REPORT GENERATED (script)\n","================================================================================\n","PDF Report: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/report.pdf\n","JSON Metrics: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949/metrics_data.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">yolov8m_finetuned_1_finetuned_20251128_bdd100k_yolo_limited_test_20251128_230949</strong> at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/g4xnialu' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing/runs/g4xnialu</a><br> View project at: <a href='https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing' target=\"_blank\">https://wandb.ai/m3mahdy-king-saud-university/yolo-bdd100k_yolo_limited-testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251128_230949-g4xnialu/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úì Weights & Biases run completed successfully\n","\n","üßπ Cleaning up model from memory...\n","‚úì Model removed from memory\n","\n","üìä Final Validation Results:\n"]},{"output_type":"display_data","data":{"text/plain":["                               model_name               dataset split  iou  \\\n","0  yolov8m_finetuned_1_finetuned_20251128  bdd100k_yolo_limited  test  0.5   \n","\n","   precision_confusion  recall_confusion  f1_confusion  precision_yolo  \\\n","0             0.724592          0.785857      0.753982         0.63714   \n","\n","   recall_yolo  map50  map50_95  params_m    size_mb        fps status  \\\n","0     0.549557  0.585  0.333778  25.86211  49.613909  329.51935     ok   \n","\n","                                             run_dir  \\\n","0  /content/Drive/MyDrive/ksu_yolo_tuning_2025/co...   \n","\n","                                     hyperparameters  \n","0  {'data': '/content/Drive/MyDrive/ksu_yolo_tuni...  "],"text/html":["\n","  <div id=\"df-3fe4b1bf-5484-4c2c-84b5-ec2b23327a06\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_name</th>\n","      <th>dataset</th>\n","      <th>split</th>\n","      <th>iou</th>\n","      <th>precision_confusion</th>\n","      <th>recall_confusion</th>\n","      <th>f1_confusion</th>\n","      <th>precision_yolo</th>\n","      <th>recall_yolo</th>\n","      <th>map50</th>\n","      <th>map50_95</th>\n","      <th>params_m</th>\n","      <th>size_mb</th>\n","      <th>fps</th>\n","      <th>status</th>\n","      <th>run_dir</th>\n","      <th>hyperparameters</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>yolov8m_finetuned_1_finetuned_20251128</td>\n","      <td>bdd100k_yolo_limited</td>\n","      <td>test</td>\n","      <td>0.5</td>\n","      <td>0.724592</td>\n","      <td>0.785857</td>\n","      <td>0.753982</td>\n","      <td>0.63714</td>\n","      <td>0.549557</td>\n","      <td>0.585</td>\n","      <td>0.333778</td>\n","      <td>25.86211</td>\n","      <td>49.613909</td>\n","      <td>329.51935</td>\n","      <td>ok</td>\n","      <td>/content/Drive/MyDrive/ksu_yolo_tuning_2025/co...</td>\n","      <td>{'data': '/content/Drive/MyDrive/ksu_yolo_tuni...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fe4b1bf-5484-4c2c-84b5-ec2b23327a06')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3fe4b1bf-5484-4c2c-84b5-ec2b23327a06 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3fe4b1bf-5484-4c2c-84b5-ec2b23327a06');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_959e0008-0848-450f-8f62-95a0ddb6023d\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_959e0008-0848-450f-8f62-95a0ddb6023d button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('results_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"results_df","summary":"{\n  \"name\": \"results_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"yolov8m_finetuned_1_finetuned_20251128\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bdd100k_yolo_limited\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iou\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5,\n        \"max\": 0.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.724591884740582,\n        \"max\": 0.724591884740582,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.724591884740582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7858573286655125,\n        \"max\": 0.7858573286655125,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7858573286655125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_confusion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.753982110568121,\n        \"max\": 0.753982110568121,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.753982110568121\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_yolo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6371404882961246,\n        \"max\": 0.6371404882961246,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6371404882961246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_yolo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5495569988501637,\n        \"max\": 0.5495569988501637,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5495569988501637\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5849999243636923,\n        \"max\": 0.5849999243636923,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5849999243636923\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map50_95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.33377787407257303,\n        \"max\": 0.33377787407257303,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.33377787407257303\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_m\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 25.86211,\n        \"max\": 25.86211,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          25.86211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 49.613908767700195,\n        \"max\": 49.613908767700195,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          49.613908767700195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 329.5193504616976,\n        \"max\": 329.5193504616976,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          329.5193504616976\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_dir\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/yolo_test/analysis_runs/yolov8m_finetuned_1_finetuned_20251128_testing_20251128_230949\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hyperparameters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["# RUN FINAL VALIDATION ON TEST SET (ENHANCED)\n","# ============================================================================\n","print('\\n' + '=' * 80)\n","print('RUNNING FINAL VALIDATION ON TEST SET')\n","print('=' * 80)\n","\n","results_summary = []\n","IOU_THRESHOLDS = 0.5  # Could expand to [0.5, 0.55, 0.6] if needed\n","\n","# Verify that the final model exists before validation\n","if not final_model_path.exists():\n","    print(f\"‚ö†Ô∏è  Warning: Final model not found at {final_model_path}\")\n","    print(f\"   Skipping test validation. Please complete training first.\")\n","else:\n","    # Add YOLO test scripts path safely\n","    scrpt_dir = BASE_DIR / \"yolo_test\"\n","    if str(scrpt_dir) not in sys.path:\n","        sys.path.append(str(scrpt_dir))\n","\n","    try:\n","        from run_yolo_detailed_testing_report import run_validation_pipeline\n","\n","        # Important:\n","        # - Datasets are in DATASET_BASE_DIR (can be different in Colab)\n","        # - Models are ALWAYS in BASE_DIR/models/\n","        #\n","        # Validation script uses base_dir for both:\n","        #   - Dataset path: base_dir / dataset_name / data.yaml\n","        #   - Model path: base_dir / models / model_name / model_name.pt\n","        #\n","        # Solution: Copy dataset to BASE_DIR temporarily, or use symlink\n","\n","        # For Colab: Need to ensure model is accessible\n","        if IS_COLAB:\n","            # Check if dataset exists in BASE_DIR\n","            base_dir_dataset = BASE_DIR / dataset_name\n","            if not (base_dir_dataset / 'data.yaml').exists() and YOLO_DATASET_ROOT.exists():\n","                print(f\"\\nüìÇ Dataset location mismatch detected\")\n","                print(f\"   Dataset is in: {YOLO_DATASET_ROOT}\")\n","                print(f\"   Validation expects: {base_dir_dataset}\")\n","                print(f\"   Creating symbolic link...\")\n","                try:\n","                    import os\n","                    if not base_dir_dataset.exists():\n","                        os.symlink(str(YOLO_DATASET_ROOT), str(base_dir_dataset))\n","                        print(f\"   ‚úì Symbolic link created\")\n","                except Exception as symlink_error:\n","                    print(f\"   ‚ö†Ô∏è  Could not create symlink: {symlink_error}\")\n","                    print(f\"   Validation may fail if dataset path is incorrect\")\n","\n","            validation_base_dir = BASE_DIR\n","        else:\n","            validation_base_dir = BASE_DIR\n","\n","        print(f\"\\nüîç Validation Configuration:\")\n","        print(f\"   Base Dir: {validation_base_dir}\")\n","        print(f\"   Dataset: {dataset_name}\")\n","        print(f\"   Model: {finetuned_model_name}\")\n","        print(f\"   Expected dataset path: {validation_base_dir / dataset_name / 'data.yaml'}\")\n","        print(f\"   Expected model path: {validation_base_dir / 'models' / finetuned_model_name / f'{finetuned_model_name}.pt'}\")\n","        print(f\"   Actual model path: {final_model_path}\")\n","\n","        # Verify paths\n","        expected_model_path = validation_base_dir / 'models' / finetuned_model_name / f'{finetuned_model_name}.pt'\n","        if final_model_path != expected_model_path and not expected_model_path.exists():\n","            print(f\"\\nüì¶ Copying model to expected location...\")\n","            expected_model_path.parent.mkdir(parents=True, exist_ok=True)\n","            shutil.copy(final_model_path, expected_model_path)\n","            print(f\"   ‚úì Model copied to {expected_model_path}\")\n","\n","        result = run_validation_pipeline(\n","            model_name=finetuned_model_name,\n","            dataset_name=dataset_name,\n","            split=\"test\",\n","            iou_threshold=IOU_THRESHOLDS,\n","            base_dir=validation_base_dir,\n","            use_wandb=True,\n","            save_reports=True,\n","            batch_size=BATCH_SIZE,\n","            include_training_exposure_analysis=True\n","        )\n","\n","        overall = result[\"metrics\"][\"overall\"]\n","        yolo_overall = result[\"metrics\"][\"yolo_metrics\"]\n","\n","        results_summary.append({\n","            \"model_name\": finetuned_model_name,\n","            \"dataset\": dataset_name,\n","            \"split\": \"test\",\n","            \"iou\": IOU_THRESHOLDS,\n","            \"precision_confusion\": overall[\"precision\"],\n","            \"recall_confusion\": overall[\"recall\"],\n","            \"f1_confusion\": overall[\"f1\"],\n","            \"precision_yolo\": yolo_overall[\"precision\"],\n","            \"recall_yolo\": yolo_overall[\"recall\"],\n","            \"map50\": yolo_overall[\"map50\"],\n","            \"map50_95\": yolo_overall[\"map50_95\"],\n","            \"params_m\": result[\"model_info\"][\"params\"] / 1e6,\n","            \"size_mb\": result[\"model_info\"][\"size(MB)\"],\n","            \"fps\": result[\"metrics\"][\"fps\"],\n","            \"status\": \"ok\",\n","            \"run_dir\": str(result[\"run_dir\"]),\n","            \"hyperparameters\": final_training_params,  # traceable\n","        })\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Model {finetuned_model_name} failed during validation: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        results_summary.append({\n","            \"model_name\": finetuned_model_name,\n","            \"dataset\": dataset_name,\n","            \"split\": \"test\",\n","            \"iou\": IOU_THRESHOLDS,\n","            \"status\": \"error\",\n","            \"error_message\": str(e)\n","        })\n","\n","# Convert to DataFrame\n","if results_summary:\n","    results_df = pd.DataFrame(results_summary)\n","    print('\\nüìä Final Validation Results:')\n","    display(results_df)\n","else:\n","    print('\\n‚ö†Ô∏è  No validation results - model training not completed yet')\n"]},{"cell_type":"markdown","id":"35471d99","metadata":{"id":"35471d99"},"source":["## 10. Generate Training Report (PDF)"]},{"cell_type":"code","execution_count":13,"id":"b66f810c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b66f810c","executionInfo":{"status":"ok","timestamp":1764372368442,"user_tz":-180,"elapsed":14571,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"8748506f-ec27-46df-abb4-ff4eab0d74b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","GENERATING COMPREHENSIVE TRAINING PDF REPORT\n","================================================================================\n","\n","‚úì Comprehensive Training PDF generated: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/yolov8m_finetuned_1_training_report.pdf\n"]}],"source":["# GENERATE COMPREHENSIVE TRAINING PDF REPORT\n","# ============================================================================\n","from reportlab.lib.pagesizes import A4\n","from reportlab.lib import colors as rl_colors\n","from reportlab.lib.units import inch\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.enums import TA_CENTER, TA_LEFT\n","import platform\n","import psutil\n","\n","print('\\n' + '=' * 80)\n","print('GENERATING COMPREHENSIVE TRAINING PDF REPORT')\n","print('=' * 80)\n","\n","pdf_training_report_path = TRAIN_DIR / f'{MODEL_NAME}_training_report.pdf'\n","doc = SimpleDocTemplate(str(pdf_training_report_path), pagesize=A4,\n","                       rightMargin=30, leftMargin=30,\n","                       topMargin=30, bottomMargin=30)\n","story = []\n","styles = getSampleStyleSheet()\n","\n","# Custom styles\n","title_style = ParagraphStyle('Title', parent=styles['Heading1'], fontSize=24,\n","                             textColor=rl_colors.HexColor('#2c3e50'), alignment=TA_CENTER, spaceAfter=20)\n","heading_style = ParagraphStyle('Heading', parent=styles['Heading2'], fontSize=16,\n","                               textColor=rl_colors.HexColor('#34495e'), spaceAfter=12, spaceBefore=20)\n","normal_style = ParagraphStyle('Normal', parent=styles['Normal'], fontSize=10)\n","\n","# --- Title ---\n","story.append(Paragraph(f'{MODEL_NAME} Final Training Report', title_style))\n","story.append(Spacer(1, 12))\n","\n","# --- System Info ---\n","story.append(Paragraph('System Information', heading_style))\n","sys_info_data = [\n","    ['OS', platform.system() + ' ' + platform.release()],\n","    ['Python Version', platform.python_version()],\n","    ['PyTorch Version', torch.__version__],\n","    ['CUDA Available', str(torch.cuda.is_available())],\n","    ['Device', device],\n","    ['RAM (GB)', f\"{psutil.virtual_memory().total/1e9:.2f}\"],\n","]\n","sys_table = Table(sys_info_data, colWidths=[2.5*inch, 3.5*inch])\n","sys_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#95a5a6')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","]))\n","story.append(sys_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Dataset Info ---\n","story.append(Paragraph('Dataset Information', heading_style))\n","# Wrap class names text for better readability\n","class_names_text = ', '.join(str(name) for name in CLASS_NAMES.values())\n","class_names_wrapped = Paragraph(class_names_text, normal_style)\n","\n","dataset_info_data = [\n","    ['Property', 'Value'],\n","    ['Dataset', YOLO_DATASET_ROOT.name],\n","    ['Number of Classes', str(NUM_CLASSES)],\n","    ['Train Images', str(dataset_stats.get('train', {}).get('images', 'N/A'))],\n","    ['Val Images', str(dataset_stats.get('val', {}).get('images', 'N/A'))],\n","    ['Test Images', str(dataset_stats.get('test', {}).get('images', 'N/A'))],\n","    ['Data YAML', str(DATA_YAML_PATH.name)],\n","]\n","dataset_table = Table(dataset_info_data, colWidths=[2*inch, 4*inch])\n","dataset_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#16a085')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(dataset_table)\n","story.append(Spacer(1, 6))\n","# Add class names separately with wrapping\n","story.append(Paragraph('<b>Classes:</b>', normal_style))\n","story.append(class_names_wrapped)\n","story.append(Spacer(1, 12))\n","\n","# --- Optimization Summary ---\n","story.append(Paragraph('Optimization Summary', heading_style))\n","opt_summary_data = [\n","    ['Metric', 'Value'],\n","    ['Tuning Run', TUNING_RUN_NAME],\n","    ['Total Trials', str(tuning_metadata.get('total_trials', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Completed Trials', str(tuning_metadata.get('completed_trials', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Best Trial Number', str(tuning_metadata.get('best_trial', 'N/A')) if not USE_DEFAULT_CONFIG and  tuning_metadata_path.exists() else 'N/A'],\n","    ['Best Trial mAP@0.5', f\"{tuning_metadata.get('best_map50', 0):.4f}\" if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists() else 'N/A'],\n","    ['Final Training Epochs', str(EPOCHS_FINAL_TRAINING)],\n","]\n","opt_table = Table(opt_summary_data, colWidths=[3*inch, 3*inch])\n","opt_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#f39c12')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(opt_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Optimized Hyperparameters ---\n","story.append(PageBreak())\n","story.append(Paragraph('Optimized Hyperparameters Used', heading_style))\n","hyperparam_data = [['Parameter', 'Value']]\n","for key, value in best_params.items():\n","    hyperparam_data.append([key, f\"{value:.6f}\" if isinstance(value, float) else str(value)])\n","hyperparam_table = Table(hyperparam_data, colWidths=[3*inch, 3*inch])\n","hyperparam_table.setStyle(TableStyle([\n","    ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#3498db')),\n","    ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","    ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","]))\n","story.append(hyperparam_table)\n","story.append(Spacer(1, 12))\n","\n","# --- Training Process Details ---\n","story.append(PageBreak())\n","story.append(Paragraph('Training Process Analysis', heading_style))\n","\n","# Try to load training results CSV for detailed epoch-by-epoch analysis\n","# YOLO saves results.csv directly in the training run directory\n","results_csv = TRAIN_DIR / 'results.csv'\n","if results_csv.exists():\n","    try:\n","        import pandas as pd\n","        import matplotlib.pyplot as plt\n","        import matplotlib\n","        matplotlib.use('Agg')\n","\n","        # Load results\n","        training_results = pd.read_csv(results_csv)\n","        training_results.columns = training_results.columns.str.strip()\n","\n","        story.append(Paragraph('Epoch-by-Epoch Training Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # Create comprehensive training curves\n","        fig, axes = plt.subplots(3, 2, figsize=(12, 14))\n","        fig.suptitle('Training Progress Over Epochs', fontsize=16, fontweight='bold')\n","\n","        # 1. Loss Curves (Train/Box/Cls/DFL)\n","        ax = axes[0, 0]\n","        if 'train/box_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/box_loss'],\n","                   label='Box Loss', color='#e74c3c', linewidth=2)\n","        if 'train/cls_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/cls_loss'],\n","                   label='Class Loss', color='#3498db', linewidth=2)\n","        if 'train/dfl_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['train/dfl_loss'],\n","                   label='DFL Loss', color='#f39c12', linewidth=2)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Loss', fontsize=10)\n","        ax.set_title('Training Loss Components', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 2. Validation Loss Curves\n","        ax = axes[0, 1]\n","        if 'val/box_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/box_loss'],\n","                   label='Box Loss', color='#e74c3c', linewidth=2, linestyle='--')\n","        if 'val/cls_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/cls_loss'],\n","                   label='Class Loss', color='#3498db', linewidth=2, linestyle='--')\n","        if 'val/dfl_loss' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['val/dfl_loss'],\n","                   label='DFL Loss', color='#f39c12', linewidth=2, linestyle='--')\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Loss', fontsize=10)\n","        ax.set_title('Validation Loss Components', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 3. mAP Metrics Over Epochs\n","        ax = axes[1, 0]\n","        if 'metrics/mAP50(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/mAP50(B)'],\n","                   label='mAP@0.5', color='#27ae60', linewidth=2.5, marker='o', markersize=4)\n","        if 'metrics/mAP50-95(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/mAP50-95(B)'],\n","                   label='mAP@0.5:0.95', color='#16a085', linewidth=2.5, marker='s', markersize=4)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('mAP', fontsize=10)\n","        ax.set_title('mAP Progression', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim(0, 1)\n","\n","        # 4. Precision and Recall\n","        ax = axes[1, 1]\n","        if 'metrics/precision(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/precision(B)'],\n","                   label='Precision', color='#9b59b6', linewidth=2.5, marker='^', markersize=4)\n","        if 'metrics/recall(B)' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['metrics/recall(B)'],\n","                   label='Recall', color='#e67e22', linewidth=2.5, marker='v', markersize=4)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Score', fontsize=10)\n","        ax.set_title('Precision & Recall Progression', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim(0, 1)\n","\n","        # 5. Learning Rate Schedule\n","        ax = axes[2, 0]\n","        if 'lr/pg0' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg0'],\n","                   label='LR Group 0', color='#34495e', linewidth=2)\n","        if 'lr/pg1' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg1'],\n","                   label='LR Group 1', color='#7f8c8d', linewidth=2)\n","        if 'lr/pg2' in training_results.columns:\n","            ax.plot(training_results['epoch'], training_results['lr/pg2'],\n","                   label='LR Group 2', color='#95a5a6', linewidth=2)\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Learning Rate', fontsize=10)\n","        ax.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        # 6. Combined Loss (Train vs Val)\n","        ax = axes[2, 1]\n","        # Calculate total train loss if components available\n","        train_loss_cols = [col for col in training_results.columns if 'train/' in col and 'loss' in col]\n","        val_loss_cols = [col for col in training_results.columns if 'val/' in col and 'loss' in col]\n","\n","        if train_loss_cols:\n","            train_total = training_results[train_loss_cols].sum(axis=1)\n","            ax.plot(training_results['epoch'], train_total,\n","                   label='Total Train Loss', color='#c0392b', linewidth=2.5)\n","        if val_loss_cols:\n","            val_total = training_results[val_loss_cols].sum(axis=1)\n","            ax.plot(training_results['epoch'], val_total,\n","                   label='Total Val Loss', color='#2980b9', linewidth=2.5, linestyle='--')\n","        ax.set_xlabel('Epoch', fontsize=10)\n","        ax.set_ylabel('Total Loss', fontsize=10)\n","        ax.set_title('Total Loss: Train vs Validation', fontsize=12, fontweight='bold')\n","        ax.legend(loc='best', fontsize=9)\n","        ax.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","\n","        # Save training curves\n","        training_curves_img = TRAIN_DIR / 'report_training_curves.png'\n","        plt.savefig(training_curves_img, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # Add to PDF\n","        story.append(Image(str(training_curves_img), width=6.5*inch, height=7.5*inch))\n","        story.append(Spacer(1, 12))\n","\n","        # Epoch-by-Epoch Summary Table (First 10, Middle 5, Last 10)\n","        story.append(PageBreak())\n","        story.append(Paragraph('Detailed Epoch Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # Select representative epochs\n","        total_epochs = len(training_results)\n","        if total_epochs <= 25:\n","            selected_epochs = training_results\n","        else:\n","            # First 10, middle 5, last 10\n","            first_10 = training_results.head(10)\n","            middle_start = total_epochs // 2 - 2\n","            middle_5 = training_results.iloc[middle_start:middle_start+5]\n","            last_10 = training_results.tail(10)\n","            selected_epochs = pd.concat([first_10, middle_5, last_10])\n","\n","        # Build table with key metrics\n","        epoch_table_data = [['Epoch', 'Train Loss', 'Val Loss', 'mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']]\n","\n","        for _, row in selected_epochs.iterrows():\n","            epoch_num = int(row['epoch']) if 'epoch' in row else '?'\n","\n","            # Calculate total losses\n","            train_loss = sum([row.get(col, 0) for col in train_loss_cols]) if train_loss_cols else 'N/A'\n","            val_loss = sum([row.get(col, 0) for col in val_loss_cols]) if val_loss_cols else 'N/A'\n","\n","            map50 = f\"{row.get('metrics/mAP50(B)', 0):.4f}\" if 'metrics/mAP50(B)' in row else 'N/A'\n","            map50_95 = f\"{row.get('metrics/mAP50-95(B)', 0):.4f}\" if 'metrics/mAP50-95(B)' in row else 'N/A'\n","            precision = f\"{row.get('metrics/precision(B)', 0):.4f}\" if 'metrics/precision(B)' in row else 'N/A'\n","            recall = f\"{row.get('metrics/recall(B)', 0):.4f}\" if 'metrics/recall(B)' in row else 'N/A'\n","\n","            epoch_table_data.append([\n","                str(epoch_num),\n","                f\"{train_loss:.4f}\" if isinstance(train_loss, (int, float)) else train_loss,\n","                f\"{val_loss:.4f}\" if isinstance(val_loss, (int, float)) else val_loss,\n","                map50,\n","                map50_95,\n","                precision,\n","                recall\n","            ])\n","\n","        epoch_table = Table(epoch_table_data, colWidths=[0.6*inch, 1*inch, 1*inch, 0.9*inch, 1.1*inch, 0.9*inch, 0.9*inch])\n","        epoch_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#8e44ad')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('FONTSIZE', (0,0), (-1,-1), 8),\n","            ('GRID', (0,0), (-1,-1), 0.5, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(epoch_table)\n","        story.append(Spacer(1, 12))\n","\n","        # Training Summary Statistics\n","        story.append(Paragraph('Training Statistics Summary', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        stats_data = [['Metric', 'Initial', 'Final', 'Best', 'Change']]\n","\n","        # mAP@0.5\n","        if 'metrics/mAP50(B)' in training_results.columns:\n","            map50_col = training_results['metrics/mAP50(B)']\n","            stats_data.append([\n","                'mAP@0.5',\n","                f\"{map50_col.iloc[0]:.4f}\",\n","                f\"{map50_col.iloc[-1]:.4f}\",\n","                f\"{map50_col.max():.4f}\",\n","                f\"+{map50_col.iloc[-1] - map50_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # mAP@0.5:0.95\n","        if 'metrics/mAP50-95(B)' in training_results.columns:\n","            map50_95_col = training_results['metrics/mAP50-95(B)']\n","            stats_data.append([\n","                'mAP@0.5:0.95',\n","                f\"{map50_95_col.iloc[0]:.4f}\",\n","                f\"{map50_95_col.iloc[-1]:.4f}\",\n","                f\"{map50_95_col.max():.4f}\",\n","                f\"+{map50_95_col.iloc[-1] - map50_95_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # Precision\n","        if 'metrics/precision(B)' in training_results.columns:\n","            prec_col = training_results['metrics/precision(B)']\n","            stats_data.append([\n","                'Precision',\n","                f\"{prec_col.iloc[0]:.4f}\",\n","                f\"{prec_col.iloc[-1]:.4f}\",\n","                f\"{prec_col.max():.4f}\",\n","                f\"+{prec_col.iloc[-1] - prec_col.iloc[0]:.4f}\"\n","            ])\n","\n","        # Recall\n","        if 'metrics/recall(B)' in training_results.columns:\n","            recall_col = training_results['metrics/recall(B)']\n","            stats_data.append([\n","                'Recall',\n","                f\"{recall_col.iloc[0]:.4f}\",\n","                f\"{recall_col.iloc[-1]:.4f}\",\n","                f\"{recall_col.max():.4f}\",\n","                f\"+{recall_col.iloc[-1] - recall_col.iloc[0]:.4f}\"\n","            ])\n","\n","        stats_table = Table(stats_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1*inch, 1*inch])\n","        stats_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#2ecc71')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (1,1), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(stats_table)\n","        story.append(Spacer(1, 12))\n","\n","    except Exception as e:\n","        story.append(Paragraph(f'Could not load detailed training results: {str(e)}', normal_style))\n","        story.append(Spacer(1, 12))\n","else:\n","    story.append(Paragraph('Training results file (results.csv) not found. Train the model to generate detailed metrics.', normal_style))\n","    story.append(Spacer(1, 12))\n","\n","# --- Final Model Performance ---\n","if 'final_metrics' in globals():\n","    story.append(PageBreak())\n","    story.append(Paragraph('Final Model Performance', heading_style))\n","\n","    perf_data = [\n","        ['Metric', 'Value'],\n","        ['mAP@0.5', f\"{final_metrics['map50']:.4f}\"],\n","        ['mAP@0.5:0.95', f\"{final_metrics['map50_95']:.4f}\"],\n","        ['Precision', f\"{final_metrics['precision']:.4f}\"],\n","        ['Recall', f\"{final_metrics['recall']:.4f}\"],\n","    ]\n","    perf_table = Table(perf_data, colWidths=[3*inch, 3*inch])\n","    perf_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#27ae60')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ]))\n","    story.append(perf_table)\n","    story.append(Spacer(1, 12))\n","\n","# --- Test Set Validation Results ---\n","if 'result' in globals() and 'metrics' in result:\n","    story.append(PageBreak())\n","    story.append(Paragraph('Test Set Validation Results', heading_style))\n","    story.append(Spacer(1, 6))\n","\n","    # Test metrics summary\n","    test_metrics = result['metrics']\n","    test_overall = test_metrics['overall']\n","    test_yolo = test_metrics['yolo_metrics']\n","    test_model_info = result['model_info']\n","\n","    # Model Architecture and Performance Summary\n","    story.append(Paragraph('Model Architecture & Performance', styles['Heading3']))\n","    model_arch_data = [\n","        ['Metric', 'Value'],\n","        ['Model Name', finetuned_model_name],\n","        ['Parameters (M)', f\"{test_model_info.get('params', 0) / 1e6:.2f}\"],\n","        ['Model Size (MB)', f\"{test_model_info.get('size(MB)', 0):.2f}\"],\n","        ['FLOPs (G)', f\"{test_model_info.get('FLOPs(G)', 0):.2f}\"],\n","        ['Layers', str(test_model_info.get('layers', 'N/A'))],\n","        ['Inference Speed (FPS)', f\"{test_metrics['fps']:.2f}\"],\n","        ['IoU Threshold', f\"{IOU_THRESHOLDS:.2f}\"],\n","    ]\n","    model_arch_table = Table(model_arch_data, colWidths=[2.5*inch, 3.5*inch])\n","    model_arch_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#34495e')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","        ('ALIGN', (1,1), (-1,-1), 'CENTER'),\n","    ]))\n","    story.append(model_arch_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Overall Performance Metrics\n","    story.append(Paragraph('Overall Performance Metrics on Test Set', styles['Heading3']))\n","    test_perf_data = [\n","        ['Metric', 'Confusion Matrix', 'YOLO Validation'],\n","        ['Precision', f\"{test_overall['precision']:.4f}\", f\"{test_yolo['precision']:.4f}\"],\n","        ['Recall', f\"{test_overall['recall']:.4f}\", f\"{test_yolo['recall']:.4f}\"],\n","        ['F1-Score', f\"{test_overall['f1']:.4f}\", 'N/A'],\n","        ['mAP@0.5 (Overall)', 'N/A', f\"{test_yolo['map50']:.4f}\"],\n","        ['mAP@0.5:0.95 (Overall)', 'N/A', f\"{test_yolo['map50_95']:.4f}\"],\n","    ]\n","    test_perf_table = Table(test_perf_data, colWidths=[2*inch, 2*inch, 2*inch])\n","    test_perf_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#e74c3c')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","        ('ALIGN', (1,0), (-1,-1), 'CENTER'),\n","    ]))\n","    story.append(test_perf_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Per-Class mAP@0.5 and Performance\n","    if 'df_metrics' in result and not result['df_metrics'].empty:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Per-Class Performance Metrics', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        df_metrics = result['df_metrics']\n","\n","        # Per-class table with all metrics\n","        per_class_data = [['Class', 'Precision', 'Recall', 'F1-Score', 'mAP@0.5', 'TP', 'FP', 'FN']]\n","        for _, row in df_metrics.iterrows():\n","            per_class_data.append([\n","                str(row['Class']),\n","                f\"{row['Precision']:.4f}\",\n","                f\"{row['Recall']:.4f}\",\n","                f\"{row['F1-Score']:.4f}\",\n","                f\"{row['mAP@0.5']:.4f}\",\n","                str(int(row['TP'])),\n","                str(int(row['FP'])),\n","                str(int(row['FN']))\n","            ])\n","\n","        per_class_table = Table(per_class_data, colWidths=[1.2*inch, 0.8*inch, 0.7*inch, 0.8*inch, 0.8*inch, 0.5*inch, 0.5*inch, 0.5*inch])\n","        per_class_table.setStyle(TableStyle([\n","            ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#9b59b6')),\n","            ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","            ('FONTSIZE', (0,0), (-1,-1), 8),\n","            ('GRID', (0,0), (-1,-1), 0.5, rl_colors.black),\n","            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","            ('ALIGN', (1,0), (-1,-1), 'CENTER'),\n","        ]))\n","        story.append(per_class_table)\n","        story.append(Spacer(1, 12))\n","\n","        # mAP@0.5 by Class visualization\n","        map50_by_class_img = result['figures'].get('map50_by_class')\n","        if map50_by_class_img and Path(map50_by_class_img).exists():\n","            try:\n","                story.append(Paragraph('mAP@0.5 Distribution by Class', styles['Heading4']))\n","                story.append(Spacer(1, 4))\n","                story.append(Image(str(map50_by_class_img), width=6.5*inch, height=4.5*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load mAP by class chart: {str(img_error)}', normal_style))\n","\n","    # IoU Information\n","    story.append(PageBreak())\n","    story.append(Paragraph('Intersection over Union (IoU) Analysis', styles['Heading3']))\n","    story.append(Spacer(1, 6))\n","\n","    iou_info_text = f\"\"\"\n","    <b>IoU Threshold Used:</b> {IOU_THRESHOLDS:.2f}<br/>\n","    <br/>\n","    IoU (Intersection over Union) measures the overlap between predicted and ground truth bounding boxes.\n","    A prediction is considered correct (True Positive) when IoU ‚â• {IOU_THRESHOLDS:.2f}.<br/>\n","    <br/>\n","    <b>Per-Class IoU Performance:</b><br/>\n","    The confusion matrix and per-class metrics above show detection accuracy at IoU={IOU_THRESHOLDS:.2f} threshold.\n","    Each class's True Positives (TP) represent detections with IoU ‚â• {IOU_THRESHOLDS:.2f}.\n","    \"\"\"\n","    story.append(Paragraph(iou_info_text, normal_style))\n","    story.append(Spacer(1, 12))\n","\n","    # Confusion Matrix\n","    story.append(PageBreak())\n","    story.append(Paragraph('Confusion Matrix (Test Set)', styles['Heading3']))\n","    story.append(Spacer(1, 6))\n","\n","    confusion_matrix_img = result['figures'].get('confusion_matrix')\n","    if confusion_matrix_img and Path(confusion_matrix_img).exists():\n","        try:\n","            with PILImage.open(confusion_matrix_img) as img:\n","                img_width, img_height = img.size\n","                aspect_ratio = img_height / img_width\n","                pdf_width = 6*inch\n","                pdf_height = pdf_width * aspect_ratio\n","                if pdf_height > 6*inch:\n","                    pdf_height = 6*inch\n","                    pdf_width = pdf_height / aspect_ratio\n","                story.append(Image(str(confusion_matrix_img), width=pdf_width, height=pdf_height))\n","                story.append(Spacer(1, 12))\n","        except Exception as img_error:\n","            story.append(Paragraph(f'Could not load confusion matrix: {str(img_error)}', normal_style))\n","    else:\n","        story.append(Paragraph('Confusion matrix image not available.', normal_style))\n","    story.append(Spacer(1, 12))\n","\n","    # Test Performance Curves - Only add section if curves exist\n","    pr_curve_img = result['figures'].get('pr_curve')\n","    f1_curve_img = result['figures'].get('f1_curve')\n","    overall_metrics_img = result['figures'].get('overall_metrics')\n","\n","    has_curves = (\n","        (pr_curve_img and Path(pr_curve_img).exists()) or\n","        (f1_curve_img and Path(f1_curve_img).exists()) or\n","        (overall_metrics_img and Path(overall_metrics_img).exists())\n","    )\n","\n","    if has_curves:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Test Set Performance Curves', styles['Heading3']))\n","        story.append(Spacer(1, 6))\n","\n","        # PR Curve\n","        if pr_curve_img and Path(pr_curve_img).exists():\n","            try:\n","                story.append(Paragraph('Precision-Recall Curve', styles['Heading4']))\n","                story.append(Image(str(pr_curve_img), width=6*inch, height=4*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load PR curve: {str(img_error)}', normal_style))\n","\n","        # F1 Curve\n","        if f1_curve_img and Path(f1_curve_img).exists():\n","            try:\n","                story.append(Paragraph('F1-Score Curve', styles['Heading4']))\n","                story.append(Image(str(f1_curve_img), width=6*inch, height=4*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load F1 curve: {str(img_error)}', normal_style))\n","\n","        # Overall Metrics\n","        if overall_metrics_img and Path(overall_metrics_img).exists():\n","            try:\n","                story.append(Paragraph('Overall Metrics Visualization', styles['Heading4']))\n","                story.append(Image(str(overall_metrics_img), width=6.5*inch, height=5*inch))\n","                story.append(Spacer(1, 12))\n","            except Exception as img_error:\n","                story.append(Paragraph(f'Could not load overall metrics: {str(img_error)}', normal_style))\n","\n","    # Sample Comparison Images\n","    if 'comparison_data' in result and result['comparison_data']:\n","        story.append(PageBreak())\n","        story.append(Paragraph('Sample Predictions: Ground Truth vs Model Output', heading_style))\n","        story.append(Spacer(1, 6))\n","\n","        # Add up to 6 comparison images\n","        for idx, comp in enumerate(result['comparison_data'][:6], 1):\n","            comp_img_path = comp.get('comparison_image_path')\n","            if comp_img_path and Path(comp_img_path).exists():\n","                try:\n","                    # Add attributes info\n","                    attributes = comp.get('attributes', {})\n","                    attr_text = f\"Sample {idx} - Weather: {attributes.get('weather', 'unknown')}, Scene: {attributes.get('scene', 'unknown')}, Time: {attributes.get('timeofday', 'unknown')}\"\n","                    story.append(Paragraph(attr_text, normal_style))\n","                    story.append(Spacer(1, 4))\n","\n","                    # Add comparison image\n","                    with PILImage.open(comp_img_path) as img:\n","                        img_width, img_height = img.size\n","                        aspect_ratio = img_height / img_width\n","                        pdf_width = 6.5*inch\n","                        pdf_height = pdf_width * aspect_ratio\n","                        if pdf_height > 4*inch:\n","                            pdf_height = 4*inch\n","                            pdf_width = pdf_height / aspect_ratio\n","                        story.append(Image(str(comp_img_path), width=pdf_width, height=pdf_height))\n","\n","                    # Add object count info\n","                    gt_count = comp.get('gt_count', 0)\n","                    pred_count = comp.get('pred_count', 0)\n","                    count_text = f\"Ground Truth: {gt_count} objects | Predictions: {pred_count} objects\"\n","                    story.append(Paragraph(count_text, ParagraphStyle('Small', parent=normal_style, fontSize=8, textColor=rl_colors.grey)))\n","                    story.append(Spacer(1, 15))\n","\n","                    # Page break after every 2 comparisons\n","                    if idx % 2 == 0 and idx < len(result['comparison_data'][:6]):\n","                        story.append(PageBreak())\n","\n","                except Exception as img_error:\n","                    story.append(Paragraph(f'Could not load comparison {idx}: {str(img_error)}', normal_style))\n","                    story.append(Spacer(1, 12))\n","\n","elif 'results_summary' in globals() and len(results_summary) > 0 and results_summary[0].get('status') == 'ok':\n","    # Fallback: Show basic info from results_summary\n","    story.append(PageBreak())\n","    story.append(Paragraph('Test Set Validation Results', heading_style))\n","\n","    res = results_summary[0]\n","    fallback_data = [\n","        ['Metric', 'Value'],\n","        ['Model', res.get('model_name', 'N/A')],\n","        ['Precision (YOLO)', f\"{res.get('precision_yolo', 0):.4f}\"],\n","        ['Recall (YOLO)', f\"{res.get('recall_yolo', 0):.4f}\"],\n","        ['mAP@0.5', f\"{res.get('map50', 0):.4f}\"],\n","        ['mAP@0.5:0.95', f\"{res.get('map50_95', 0):.4f}\"],\n","        ['Parameters (M)', f\"{res.get('params_m', 0):.2f}\"],\n","        ['Size (MB)', f\"{res.get('size_mb', 0):.2f}\"],\n","        ['FPS', f\"{res.get('fps', 0):.2f}\"],\n","    ]\n","    fallback_table = Table(fallback_data, colWidths=[3*inch, 3*inch])\n","    fallback_table.setStyle(TableStyle([\n","        ('BACKGROUND', (0,0), (-1,0), rl_colors.HexColor('#95a5a6')),\n","        ('TEXTCOLOR', (0,0), (-1,0), rl_colors.whitesmoke),\n","        ('GRID', (0,0), (-1,-1), 1, rl_colors.black),\n","    ]))\n","    story.append(fallback_table)\n","    story.append(Spacer(1, 12))\n","\n","    # Try to load images from run_dir if available\n","    if 'run_dir' in res:\n","        run_dir = Path(res['run_dir'])\n","\n","        # Try confusion matrix\n","        confusion_img = run_dir / 'confusion_matrix.png'\n","        if confusion_img.exists():\n","            try:\n","                story.append(PageBreak())\n","                story.append(Paragraph('Confusion Matrix', styles['Heading3']))\n","                story.append(Image(str(confusion_img), width=6*inch, height=5*inch))\n","                story.append(Spacer(1, 12))\n","            except:\n","                pass\n","\n","        # Try comparison images\n","        comparisons_dir = run_dir / 'sample_comparisons'\n","        if comparisons_dir.exists():\n","            comparison_imgs = sorted(comparisons_dir.glob('comparison_*.png'))[:4]\n","            if comparison_imgs:\n","                story.append(PageBreak())\n","                story.append(Paragraph('Sample Predictions', styles['Heading3']))\n","                for comp_img in comparison_imgs:\n","                    try:\n","                        story.append(Image(str(comp_img), width=6.5*inch, height=3.5*inch))\n","                        story.append(Spacer(1, 10))\n","                    except:\n","                        pass\n","\n","# --- Footer ---\n","story.append(Spacer(1, 20))\n","story.append(Paragraph('Generated by YOLO Training Notebook', ParagraphStyle('Footer', parent=styles['Normal'], alignment=TA_CENTER, textColor=rl_colors.grey)))\n","story.append(Paragraph('BDD100K Dataset - Computer Vision Project', ParagraphStyle('Footer2', parent=styles['Normal'], alignment=TA_CENTER, textColor=rl_colors.grey)))\n","\n","# Build PDF\n","try:\n","    doc.build(story)\n","    print(f'\\n‚úì Comprehensive Training PDF generated: {pdf_training_report_path}')\n","except Exception as e:\n","    print(f'\\n‚ùå Error generating PDF: {e}')\n","    import traceback\n","    traceback.print_exc()\n"]},{"cell_type":"markdown","id":"7d09278e","metadata":{"id":"7d09278e"},"source":["## 11. Final Summary"]},{"cell_type":"code","execution_count":14,"id":"9e9576f4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e9576f4","executionInfo":{"status":"ok","timestamp":1764372368465,"user_tz":-180,"elapsed":23,"user":{"displayName":"Mohammed Mahdy","userId":"14753646347419625264"}},"outputId":"35dd39c3-a68c-4b37-d6a5-8f33afe02e43"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","================================================================================\n","FINAL TRAINING COMPLETE!\n","================================================================================\n","\n","üìä Project: yolov8m_finetuned_1 on bdd100k_yolo_limited\n","üìÖ Date: 2025-11-28 23:26:08\n","\n","üî¨ Tuning Run Used:\n","  Run Name: yolov8m_finetuned_1_tune_20251127_230340\n","  Run Path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/tune/yolov8m_finetuned_1_tune_20251127_230340\n","\n","üéØ Training Run:\n","  Run Name: yolov8m_finetuned_1_train_20251128_221046\n","  Run Path: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\n","  Epochs: 100\n","  Batch Size: 64\n","\n","üéØ Final Model Performance:\n","  mAP@0.5: 0.5769\n","  mAP@0.5:0.95: 0.3347\n","  Precision: 0.6530\n","  Recall: 0.5310\n","\n","üìÅ Generated Files:\n","\n","  üìä Tuning Results (in yolov8m_finetuned_1_tune_20251127_230340):\n","    - best_hyperparameters.json\n","    - best_hparams.yaml\n","    - checkpoint_log.json\n","    - optuna_study.pkl\n","\n","  üéØ Training Results (in yolov8m_finetuned_1_train_20251128_221046):\n","    - training_log.json\n","    - weights/best.pt\n","    - weights/last.pt\n","    - results.csv\n","  üìÑ Training PDF Report:\n","    - yolov8m_finetuned_1_training_report.pdf\n","\n","  üéØ Final Model Package:\n","    - yolov8m_finetuned_1_finetuned_20251128.pt\n","    - yolov8m_finetuned_1_finetuned_20251128_metadata.json\n","    Location: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128\n","\n","üìÇ All results saved to:\n","  Tuning: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/tune/yolov8m_finetuned_1_tune_20251127_230340\n","  Training: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\n","  Final Model: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128\n","\n","üöÄ Next Steps:\n","  1. Review training PDF report: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046/yolov8m_finetuned_1_training_report.pdf\n","  2. Review training plots and metrics in: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/tune_train/training/yolov8m_finetuned_1_train_20251128_221046\n","  3. Use final model for inference: /content/Drive/MyDrive/ksu_yolo_tuning_2025/computer_vision_yolo/models/yolov8m_finetuned_1_finetuned_20251128/yolov8m_finetuned_1_finetuned_20251128.pt\n","  4. Evaluate on test set using yolo_test scripts\n","  5. Consider fine-tuning with different datasets or model sizes\n","\n","üìù To Resume Training:\n","  Set RESUME_TRAINING_RUN_NAME = \"yolov8m_finetuned_1_train_20251128_221046\"\n","  Then re-run this notebook\n","\n","================================================================================\n","SUCCESS! ‚úì\n","================================================================================\n"]}],"source":["# FINAL SUMMARY\n","# ============================================================================\n","\n","print('\\n\\n')\n","print('=' * 80)\n","print('FINAL TRAINING COMPLETE!')\n","print('=' * 80)\n","\n","print(f'\\nüìä Project: {MODEL_NAME} on {YOLO_DATASET_ROOT.name}')\n","print(f'üìÖ Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","\n","# Tuning Summary\n","print(f'\\nüî¨ Tuning Run Used:')\n","print(f'  Run Name: {TUNING_RUN_NAME}')\n","print(f'  Run Path: {TUNE_DIR}')\n","\n","if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","    print(f'  Total Trials: {tuning_metadata.get(\"total_trials\", \"N/A\")}')\n","    print(f'  Completed Trials: {tuning_metadata.get(\"completed_trials\", \"N/A\")}')\n","    print(f'  Best Trial: {tuning_metadata.get(\"best_trial\", \"N/A\")}')\n","    print(f'  Best Trial mAP@0.5: {tuning_metadata.get(\"best_map50\", 0):.4f}')\n","    if 'optimization_duration' in tuning_metadata:\n","        print(f'  Tuning Duration: {tuning_metadata[\"optimization_duration\"]}')\n","\n","# Training Summary\n","print(f'\\nüéØ Training Run:')\n","print(f'  Run Name: {RUN_NAME_TRAINING}')\n","print(f'  Run Path: {TRAIN_DIR}')\n","print(f'  Epochs: {EPOCHS_FINAL_TRAINING}')\n","print(f'  Batch Size: {BATCH_SIZE}')\n","\n","if 'final_metrics' in globals():\n","    print(f'\\nüéØ Final Model Performance:')\n","    print(f'  mAP@0.5: {final_metrics[\"map50\"]:.4f}')\n","    print(f'  mAP@0.5:0.95: {final_metrics[\"map50_95\"]:.4f}')\n","    print(f'  Precision: {final_metrics[\"precision\"]:.4f}')\n","    print(f'  Recall: {final_metrics[\"recall\"]:.4f}')\n","\n","    # Show improvement if available\n","    if not USE_DEFAULT_CONFIG and tuning_metadata_path.exists():\n","        tuning_best_map = tuning_metadata.get('best_map50', 0)\n","        if tuning_best_map > 0:\n","            improvement = final_metrics['map50'] - tuning_best_map\n","            print(f'\\nüìà Improvement vs Tuning:')\n","            print(f'  Tuning Best: {tuning_best_map:.4f}')\n","            print(f'  Training Final: {final_metrics[\"map50\"]:.4f}')\n","            print(f'  Improvement: {improvement:+.4f} ({improvement/tuning_best_map*100:+.2f}%)')\n","\n","print(f'\\nüìÅ Generated Files:')\n","if not USE_DEFAULT_CONFIG:\n","    print(f'\\n  üìä Tuning Results (in {TUNE_DIR.name}):')\n","    print(f'    - best_hyperparameters.json')\n","    print(f'    - best_hparams.yaml')\n","    print(f'    - checkpoint_log.json')\n","    print(f'    - optuna_study.pkl')\n","\n","print(f'\\n  üéØ Training Results (in {TRAIN_DIR.name}):')\n","print(f'    - training_log.json')\n","print(f'    - weights/best.pt')\n","print(f'    - weights/last.pt')\n","print(f'    - results.csv')\n","print(f'  üìÑ Training PDF Report:')\n","print(f'    - {MODEL_NAME}_training_report.pdf')\n","\n","if 'final_model_path' in globals():\n","    print(f'\\n  üéØ Final Model Package:')\n","    print(f'    - {final_model_path.name}')\n","    print(f'    - {metadata_path.name}')\n","    print(f'    Location: {model_save_dir}')\n","\n","print(f'\\nüìÇ All results saved to:')\n","print(f'  Tuning: {TUNE_DIR}')\n","print(f'  Training: {TRAIN_DIR}')\n","if 'model_save_dir' in globals():\n","    print(f'  Final Model: {model_save_dir}')\n","\n","print(f'\\nüöÄ Next Steps:')\n","print(f'  1. Review training PDF report: {TRAIN_DIR / f\"{MODEL_NAME}_training_report.pdf\"}')\n","print(f'  2. Review training plots and metrics in: {TRAIN_DIR}')\n","if 'final_model_path' in globals():\n","    print(f'  3. Use final model for inference: {final_model_path}')\n","    print(f'  4. Evaluate on test set using yolo_test scripts')\n","else:\n","    print(f'  3. Complete training to generate final model')\n","print(f'  5. Consider fine-tuning with different datasets or model sizes')\n","\n","print('\\nüìù To Resume Training:')\n","print(f'  Set RESUME_TRAINING_RUN_NAME = \"{RUN_NAME_TRAINING}\"')\n","print(f'  Then re-run this notebook')\n","\n","print('\\n' + '=' * 80)\n","print('SUCCESS! ‚úì')\n","print('=' * 80)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}